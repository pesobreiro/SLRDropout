@Article{alboukaey_dynamic_2020,
  title = {Dynamic behavior based churn prediction in mobile telecom},
  volume = {162},
  issn = {09574174},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417420306035},
  doi = {10.1016/j.eswa.2020.113779},
  language = {en},
  urldate = {2020-09-07},
  journal = {Expert Systems with Applications},
  author = {Nadia Alboukaey and Ammar Joukhadar and Nada Ghneim},
  month = {dec},
  year = {2020},
  abstract = {Customer churn is one of the most challenging problems that affects revenue and customer base in mobile telecom operators. The success of retention campaigns depends not only on the accuracy of predicting potential churners, but with equal importance, it depends on the timing when the prediction is done. Previous works related to churn prediction presented models to predict churn monthly with a focus on the static behavior of customers, and even the studies that considered the dynamic behavior of the customer, looked mainly at the monthly level behavior. However, customer behavior is susceptible to changes over days of month, and during the time leading up to a customer decision to churn, he/she starts behaving differently. Therefore, considering monthly behavioral features negatively affects the predictive performance, because it ignores changes in behavior over days of month. Moreover, predicting churners on monthly basis will be late for customers who decided to leave at the beginning of the month because they will not be detected as churners until the next month. To address these issues, in this paper, we propose daily churn prediction instead of monthly based on the daily dynamic behavior of customer instead of his monthly one. More precisely, we represent customer’s daily behavior as multivariate time series and propose four models to predict churn daily based on this representation. Two models depend on features extracted from the multivariate time series, namely RFM-based model and statistics-based model. While the other models, exploit deep learning techniques for automatic feature extraction, namely LSTM-based model and CNN-based model. The predictive performance of the proposed models were investigated by evaluating them using a 150-day-long dataset collected from MTN operator in the country. The results showed that the daily models significantly outperform the monthly models in terms of predicting churners earlier and more accurately. Furthermore, the LSTM-based model significantly outperforms the CNN-based model. However, the prediction performances of the LSTM-based and the CNN-based models are equal to the prediction performance of the RFM-based model. Moreover, all of these three models significantly outperform the Statistics-based model.},
  pages = {113779},
}

@Article{gonzalez_practical_2020,
  title = {A practical tutorial on bagging and boosting based ensembles for machine learning: {Algorithms}, software tools, performance study, practical perspectives and opportunities},
  volume = {64},
  issn = {15662535},
  shorttitle = {A practical tutorial on bagging and boosting based ensembles for machine learning},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253520303195},
  doi = {10.1016/j.inffus.2020.07.007},
  language = {en},
  urldate = {2020-09-07},
  journal = {Information Fusion},
  author = {Sergio González and Salvador García and Javier {Del Ser} and Lior Rokach and Francisco Herrera},
  month = {dec},
  year = {2020},
  pages = {205--237},
  abstract = {Ensembles, especially ensembles of decision trees, are one of the most popular and successful techniques in machine learning. Recently, the number of ensemble-based proposals has grown steadily. Therefore, it is necessary to identify which are the appropriate algorithms for a certain problem. In this paper, we aim to help practitioners to choose the best ensemble technique according to their problem characteristics and their workflow. To do so, we revise the most renowned bagging and boosting algorithms and their software tools. These ensembles are described in detail within their variants and improvements available in the literature. Their online-available software tools are reviewed attending to the implemented versions and features. They are categorized according to their supported programming languages and computing paradigms. The performance of 14 different bagging and boosting based ensembles, including XGBoost, LightGBM and Random Forest, is empirically analyzed in terms of predictive capability and efficiency. This comparison is done under the same software environment with 76 different classification tasks. Their predictive capabilities are evaluated with a wide variety of scenarios, such as standard multi-class problems, scenarios with categorical features and big size data. The efficiency of these methods is analyzed with considerably large data-sets. Several practical perspectives and opportunities are also exposed for ensemble learning.},
}

@Article{choi_social_2020,
  title = {Social media analytics and business intelligence research: {A} systematic review},
  volume = {57},
  issn = {03064573},
  shorttitle = {Social media analytics and business intelligence research},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S030645731931057X},
  doi = {10.1016/j.ipm.2020.102279},
  language = {en},
  number = {6},
  urldate = {2020-09-07},
  journal = {Information Processing \& Management},
  author = {Jaewoong Choi and Janghyeok Yoon and Jaemin Chung and Byoung-Youl Coh and Jae-Min Lee},
  month = {nov},
  year = {2020},
  pages = {102279},
  abstract = {Evidently, online voice of customers (VoC) expressed in social media has emerged as quality data for researchers who are willing to conduct customer-driven business intelligence (BI) research. Nevertheless, to the best of authors’ knowledge, there is still a dearth of studies that deal with such remarkable research stream and address various open data (e.g., social media, intellectual property) from a BI research perspective. Therefore, this study has attempted to evaluate the applicability of social media data in BI research and provide a systematic review on the primary research articles in the domain. This study compared social media data with the other open data (e.g., gray literature, public government data) in terms of data content, collection, updatability and structure, which are determined through a thorough discussion with experts. Next, this study selected 57 social media-based BI research articles from the Web of Science (WoS) database and analyzed them with three research questions about the data, methodologies, and results to understand this research domain. Our findings are expected to inform the existing researchers in the research domain about the future research directions, enable newcomers to understand the overall process of analyzing social media data, and provide the practitioners with social media analysis approaches suitable for their environment.},
}

@Article{cho_instance-based_2020,
  title = {Instance-based entropy fuzzy support vector machine for imbalanced data},
  volume = {23},
  issn = {1433-7541, 1433-755X},
  url = {http://link.springer.com/10.1007/s10044-019-00851-x},
  doi = {10.1007/s10044-019-00851-x},
  language = {en},
  number = {3},
  urldate = {2020-09-07},
  journal = {Pattern Analysis and Applications},
  author = {Poongjin Cho and Minhyuk Lee and Woojin Chang},
  month = {aug},
  year = {2020},
  pages = {1183--1202},
  file = {Cho et al_2020_Instance-based entropy fuzzy support vector machine for imbalanced data.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\EIBKVTRH\\Cho et al_2020_Instance-based entropy fuzzy support vector machine for imbalanced data.pdf:application/pdf},
  abstract = {Imbalanced classification has been a major challenge for machine learning because many standard classifiers mainly focus on balanced datasets and tend to have biased results toward the majority class. We modify entropy fuzzy support vector machine (EFSVM) and introduce instance-based entropy fuzzy support vector machine (IEFSVM). Both EFSVM and IEFSVM use the entropy information of k-nearest neighbors to determine the fuzzy membership value for each sample which prioritizes the importance of each sample. IEFSVM considers the diversity of entropy patterns for each sample when increasing the size of neighbors, k, while EFSVM uses single entropy information of the fixed size of neighbors for all samples. By varying k, we can reflect the component change of sample’s neighbors from near to far distance in the determination of fuzzy value membership. Numerical experiments on 35 public and 12 real-world imbalanced datasets are performed to validate IEFSVM, and area under the receiver operating characteristic curve (AUC) is used to compare its performance with other SVMs and machine learning methods. IEFSVM shows a much higher AUC value for datasets with high imbalance ratio, implying that IEFSVM is effective in dealing with the class imbalance problem. },
}

@Article{guan_portrait_2020,
  title = {The {Portrait} {Depiction} of the {Market} {Members} {Based} on {Data} {Mining}},
  volume = {34},
  issn = {0218-0014, 1793-6381},
  url = {https://www.worldscientific.com/doi/abs/10.1142/S0218001420590247},
  doi = {10.1142/S0218001420590247},
  language = {en},
  number = {07},
  urldate = {2020-09-07},
  journal = {International Journal of Pattern Recognition and Artificial Intelligence},
  author = {Jinlan Guan and Cuifang Tang and Jiequan Ou},
  month = {jun},
  year = {2020},
  pages = {2059024},
  abstract = {Aiming at the problem of portrait of members in shopping malls, this paper analyzes the similarities and differences of consumption behaviors between member groups and nonmember groups, and constructs the LRFMC model with k-means algorithm to analyze the value of membership. Second, active states of members are divided according to the consumption time interval, and KNN algorithm model is established to predict member states and used to predict the membership status. Finally, it discusses which types of goods are more suitable for promotional activities and can bring more profits to the shopping mall.},
}

@Article{shukla_bibliometric_2020,
  title = {A bibliometric analysis and cutting-edge overview on fuzzy techniques in {Big} {Data}},
  volume = {92},
  issn = {09521976},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0952197620300877},
  doi = {10.1016/j.engappai.2020.103625},
  language = {en},
  urldate = {2020-09-07},
  journal = {Engineering Applications of Artificial Intelligence},
  author = {Amit K. Shukla and Pranab K. Muhuri and Ajith Abraham},
  month = {jun},
  year = {2020},
  abstract = {Over the last few years, Big Data has gained a tremendous attention from the research community. The data being generated in huge quantity from almost every field is unstructured and unprocessed. Extracting knowledge base and useful information from the big raw data is one of the major challenges, present today. Various computational intelligence and soft computing techniques have been proposed for efficient big data analytics. Fuzzy techniques are one of the soft computing approaches which can play a very crucial role in current big data challenges by pre-processing and reconstructing data. There is a wide spread application domains where traditional fuzzy sets (type-1 fuzzy sets) and higher order fuzzy sets (type-2 fuzzy sets) have shown remarkable outcomes. Although, this research domain of “fuzzy techniques in Big Data” is gaining some attention, there is a strong need for a motivation to encourage researchers to explore more in this area. In this paper, we have conducted bibliometric study on recent development in the field of “fuzzy techniques in big data”. In bibliometric study, various performance metrics including total papers, total citations, and citation per paper are calculated. Further, top 10 of most productive and highly cited authors, discipline, source journals, countries, institutions, and highly influential papers are also evaluated. Later, a comparative analysis is performed on the fuzzy techniques in big data after analysing the most influential works in this field.},
  pages = {103625},
}

@Article{beggs_cusum_2020,
  title = {A {CUSUM} tool for retrospectively evaluating team performance: the case of the {English} {Premier} {League}},
  volume = {10},
  issn = {2042-678X},
  shorttitle = {A {CUSUM} tool for retrospectively evaluating team performance},
  url = {https://www.emerald.com/insight/content/doi/10.1108/SBM-03-2019-0025/full/html},
  doi = {10.1108/SBM-03-2019-0025},
  abstract = {Purpose:Despite being a widely used management technique, cumulative sum (CUSUM) analysis remains almost unheard of in professional sport. To address this, CUSUM analysis of soccer match data from the English Premier League (EPL) was performed. The primary objective of the study was to evaluate CUSUM as a tool for assessing “on-field” team performance. As a secondary objective, the association between managerial change and team performance was evaluated. Design/methodology/approach: CUSUM was applied retrospectively to goal difference data for six EPL teams (Arsenal, Chelsea, Everton, Liverpool, Manchester United and Tottenham) over 23 consecutive seasons from 1995 to 2018. This was supplemented with change point analysis to identify structural changes in mean goal difference. Succession was evaluated by mapping historical managerial changes onto the CUSUM plots for the respective clubs. Findings: CUSUM analysis revealed the presence of structural changes in four clubs. Two structural change points were identified for both Chelsea and Everton, one for Manchester United and Tottenham and none for Arsenal and Liverpool. Relatively few managerial changes coincided temporally with structural changes in “on-field” performance, with most appointments having minimal impact on long-term team performance. Other factors (e.g. changes in ownership) appear to have been influential. Research limitations/implications:The study was limited by the fact that only successful teams were investigated. Practical implications: CUSUM analysis appears to have potential as a tool for executive decision-makers to evaluate performance outcomes in professional soccer. Originality/value: The study is the first of its kind to use CUSUM analysis to evaluate team performance in professional soccer.},
  language = {en},
  number = {3},
  urldate = {2020-09-07},
  journal = {Sport, Business and Management: An International Journal},
  author = {Clive Beggs and Alexander John Bond},
  month = {apr},
  year = {2020},
  pages = {263--289},
}

@InProceedings{amornvetchayakul_customer_2020,
  address = {Bangkok, Thailand},
  title = {Customer {Churn} {Prediction} for a {Software}-as-a-{Service} {Inventory} {Management} {Software} {Company}: {A} {Case} {Study} in {Thailand}},
  isbn = {978-1-72816-785-5},
  shorttitle = {Customer {Churn} {Prediction} for a {Software}-as-a-{Service} {Inventory} {Management} {Software} {Company}},
  url = {https://ieeexplore.ieee.org/document/9102099/},
  doi = {10.1109/ICIEA49774.2020.9102099},
  urldate = {2020-09-07},
  booktitle = {2020 {IEEE} 7th {International} {Conference} on {Industrial} {Engineering} and {Applications} ({ICIEA})},
  publisher = {IEEE},
  author = {Phongsatorn Amornvetchayakul and Naragain Phumchusri},
  month = {apr},
  year = {2020},
  abstract = {Software-as-a-Service is the fast growth and high market values as a new emerging online business. Customer churn is a critical measure for this business. Thus, this paper focuses on seeking a customer churn prediction model for a Software-as-a-Service inventory management software company in Thailand which is facing a high churn rate. This paper executes the prediction models with four machine learning algorithms: logistic regression, support vector machine, decision tree and random forest. The random forest model is capable to provide lowest error with 10-fold cross validation average scores of 91.6% recall and 92.6% F1-score. Moreover, feature importance scores can highlight useful insights of case-study that business metrics are significantly related to churn behavior. As a result, this paper is beneficial to the case-study company to help indicate real churn customer and enhance the effectiveness in executive decision and marketing campaign.},
  pages = {514--518},
}

@Article{liu_micro-_2020,
  title = {Micro- and macro-level churn analysis of large-scale mobile games},
  volume = {62},
  issn = {0219-1377, 0219-3116},
  url = {http://link.springer.com/10.1007/s10115-019-01394-7},
  doi = {10.1007/s10115-019-01394-7},
  language = {en},
  number = {4},
  urldate = {2020-09-07},
  journal = {Knowledge and Information Systems},
  author = {Xi Liu and Muhe Xie and Xidao Wen and Rui Chen and Yong Ge and Nick Duffield and Na Wang},
  month = {apr},
  year = {2020},
  pages = {1465--1496},
  abstract = {As mobile devices become more and more popular, mobile gaming has emerged as a promising market with billion-dollar revenue. A variety of mobile game platforms and services have been developed around the world. A critical challenge for these platforms and services is to understand the churn behavior in mobile games, which usually involves churn at micro-level (between an app and a specific user) and macro-level (between an app and all its users). Accurate micro-level churn prediction and macro-level churn ranking will benefit many stakeholders such as game developers, advertisers, and platform operators. In this paper, we present the first large-scale churn analysis for mobile games that supports both micro-level churn prediction and macro-level churn ranking. For micro-level churn prediction, in view of the common limitations of the state-of-the-art methods built upon traditional machine learning models, we devise a novel semi-supervised and inductive embedding model that jointly learns the prediction function and the embedding function for user–app relationships. We model these two functions by deep neural networks with a unique edge embedding technique that is able to capture both contextual information and relationship dynamics. We also design a novel attributed random walk technique that takes into consideration both topological adjacency and attribute similarities. To address macro-level churn ranking, we propose to construct a relationship graph with estimated micro-level churn probabilities as edge weights and adapt link analysis algorithms on the graph. We devise a simple algorithm SimSum and adapt two more advanced algorithms PageRank and HITS. The performance of our solutions to the two-level churn analysis problem is evaluated on real-world data collected from the Samsung Game Launcher platform. The data includes tens of thousands of mobile games and hundreds of millions of user–app interactions. The experimental results with this data demonstrate the superiority of our proposed models against existing state-of-the-art methods.},
  file = {Liu et al_2020_Micro- and macro-level churn analysis of large-scale mobile games.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\JUUSNUS7\\Liu et al_2020_Micro- and macro-level churn analysis of large-scale mobile games.pdf:application/pdf},
}

@Article{martinez_machine_2020,
  title = {A machine learning framework for customer purchase prediction in the non-contractual setting},
  volume = {281},
  issn = {03772217},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0377221718303370},
  doi = {10.1016/j.ejor.2018.04.034},
  language = {en},
  number = {3},
  urldate = {2020-09-07},
  journal = {European Journal of Operational Research},
  author = {Andrés Martínez and Claudia Schmuck and Sergiy Pereverzyev and Clemens Pirker and Markus Haltmeier},
  month = {mar},
  year = {2020},
  abstract = {Predicting future customer behavior provides key information for efficiently directing resources at sales and marketing departments. Such information supports planning the inventory at the warehouse and point of sales, as well strategic decisions during manufacturing processes. In this paper, we develop advanced analytics tools that predict future customer behavior in the non-contractual setting. We establish a dynamic and data driven framework for predicting whether a customer is going to make purchase at the company within a certain time frame in the near future. For that purpose, we propose a new set of customer relevant features that derives from times and values of previous purchases. These customer features are updated every month, and state of the art machine learning algorithms are applied for purchase prediction. In our studies, the gradient tree boosting method turns out to be the best performing method. Using a data set containing more than 10 000 customers and a total number of 200 000 purchases we obtain an accuracy score of 89% and an AUC value of 0.95 for predicting next moth purchases on the test data set.},
  pages = {588--596},
}

@Article{de_caigny_leveraging_2020,
  title = {Leveraging fine-grained transaction data for customer life event predictions},
  volume = {130},
  issn = {01679236},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167923619302611},
  doi = {10.1016/j.dss.2019.113232},
  language = {en},
  urldate = {2020-09-07},
  journal = {Decision Support Systems},
  author = {Arno {De Caigny} and Kristof Coussement and Koen W. {De Bock}},
  month = {mar},
  year = {2020},
  abstract = {This real-world study with a large European financial services provider combines aggregated customer data including customer demographics, behavior and contact with the firm, with fine-grained transaction data to predict four different customer life events: moving, birth of a child, new relationship, and end of a relationship. The fine-grained transaction data—approximately 60 million debit transactions involving around 132,000 customers to >1.5 million different counterparties over a one-year period—reveal a pseudo-social network that supports the derivation of behavioral similarity measures. To advance decision support systems literature, this study validates the proposed customer life event prediction model in a real-world setting in the financial services industry; compares models that rely on aggregated data, fine-grained transaction data, and their combination; and extends existing methods to incorporate fine-grained data that preserve recency, frequency, and monetary value information of the transactions. The results show that the proposed model predicts life events significantly better than random guessing, especially with the combination of fine-grained transaction and aggregated data. Incorporating recency, frequency, and monetary value information of fine-grained transaction data also significantly improves performance compared with models based on binary logs. Fine-grained transaction data accounts for the largest part of the total variable importance, for all but one of the life events.},
  pages = {113232},
}

@Article{seal_fuzzy_2020,
  title = {Fuzzy c-means clustering using {Jeffreys}-divergence based similarity measure},
  volume = {88},
  issn = {15684946},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1568494619307987},
  doi = {10.1016/j.asoc.2019.106016},
  language = {en},
  urldate = {2020-09-07},
  journal = {Applied Soft Computing},
  author = {Ayan Seal and Aditya Karlekar and Ondrej Krejcar and Consuelo Gonzalo-Martin},
  month = {mar},
  year = {2020},
  abstract = {In clustering, similarity measure has been one of the major factors for discovering the natural grouping of a given dataset by identifying hidden patterns. To determine a suitable similarity measure is an open problem in clustering analysis for several years. The purpose of this study is to make known a divergence based similarity measure. The notion of the proposed similarity measure is derived from Jeffrey-divergence. Various features of the proposed similarity measure are explained. Afterwards we develop fuzzy c-means (FCM) by making use of the proposed similarity measure, which guarantees to converge to local minima. The various characteristics of the modified FCM algorithm are also addressed. Some well known real-world and synthetic datasets are considered for the experiments. In addition to that two remote sensing image datasets are also adopted in this work to illustrate the effectiveness of the proposed FCM over some existing methods. All the obtained results demonstrate that FCM with divergence based proposed similarity measure outperforms three latest FCM algorithms.},
  pages = {106016},
}

@Article{sun_novel_2020,
  title = {A novel approach to generate a large scale of supervised data for short text sentiment analysis},
  volume = {79},
  issn = {1380-7501, 1573-7721},
  url = {http://link.springer.com/10.1007/s11042-018-5748-4},
  doi = {10.1007/s11042-018-5748-4},
  language = {en},
  number = {9-10},
  urldate = {2020-09-07},
  journal = {Multimedia Tools and Applications},
  author = {Xiao Sun and Jiajin He},
  month = {mar},
  year = {2020},
  abstract = {As for the complexity of language structure, the semantic structure, and the relative scarcity of labeled data and context information, sentiment analysis has been regarded as a challenging task in Natural Language Processing especially in the field of short-text processing. Deep learning model need a large scale of training data to overcome data sparseness and the over-fitting problem, we propose multi-granularity text-oriented data augmentation technologies to generate large-scale artificial data for training model, which is compared with Generative adversarial network(GAN). In this paper, a novel hybrid neural network model architecture(LSCNN) was proposed with our data augmentation technology, which is can outperforms many single neural network models. The proposed data augmentation method enhances the generalization ability of the proposed model. Experiment results show that the proposed data augmentation method in combination with the neural networks model can achieve astonishing performance without any handcrafted features on sentiment analysis or short text classification. It was validated on a Chinese on-line comment dataset and Chinese news headline corpus, and outperforms many state-of-the-art models. Evidence shows that the proposed data argumentation technology can obtain more accurate distribution representation from data for deep learning, which improves the generalization characteristics of the extracted features. The combination of the data argumentation technology and LSCNN fusion model is well suited to short text sentiment analysis, especially on small scale corpus.},
  pages = {5439--5459},
}

@Article{alweshah_water_2019,
  title = {Water {Evaporation} {Algorithm} with {Probabilistic} {Neural} {Network} for {Solving} {Classification} {Problems}},
  issn = {2413-9351},
  url = {https://www.ejmanager.com/fulltextpdf.php?mno=62278},
  doi = {10.5455/jjcit.71-1566466063},
  number = {0},
  urldate = {2020-09-07},
  journal = {Jordanian Journal of Computers and Information Technology},
  author = {Mohammed Alweshah and Enas Ramadan and Mohammed Reyalat and Muder ani and Abdelaziz Hammouri},
  year = {2019},
  abstract = {Classification is a crucial step in data mining as it facilitates decision-making in many areas of human activity, such  as  scientific  endeavors, marketing  campaigns,  biomedical  research  and  industrial  applications.The probabilistic neural network (PNN)is widely utilized to solve classification and pattern recognition problems and is considered an effective method for solving such problems. In this paper, we propose an improved PNNmodel that employs the water evaporation algorithm (WEA)in order to solve classification problems more efficiently. The  proposed  method isable  to  obtain  classification  accuracies  that  are  close  to  each  other  across  all  11 benchmark  tested  datasets  from  the  UCI  machine-learning  repository,  which  demonstratesthe  validity  of  this method (with respect to classification accuracy). The results show that the WEA is better than the firefly algorithm (FA)and biogeography-based optimization (BBO)in terms of both classification accuracy and convergence speed.},
  pages = {1},
}

@Article{li_discriminative_2020,
  title = {A {Discriminative} {Approach} to {Sentiment} {Classification}},
  volume = {51},
  issn = {1370-4621, 1573-773X},
  url = {http://link.springer.com/10.1007/s11063-019-10108-7},
  doi = {10.1007/s11063-019-10108-7},
  language = {en},
  number = {1},
  urldate = {2020-09-07},
  journal = {Neural Processing Letters},
  author = {Guangmin Li and Zhiwei Lin and Hui Wang and Xin Wei},
  month = {feb},
  year = {2020},
  abstract = {Due to the explosive growth of user-generated contents, understanding opinions (such as reviews on products) generated by Internet users is important for optimizing business decision. To achieve such understanding, this paper investigates a discriminative approach to classifying opinions according to sentiments. The discriminative approach builds a model with the prior knowledge of the categorization information in order to extract meaningful features from the unstructured texts. The prior knowledge includes ratio factors to reinforce terms’ sentiment polarity by using TF-IDF, short for term frequency-inverse document frequency. Experimental results with four datasets show the proposed approach is very competitive, compared with some of the previous works.},
  pages = {749--758},
}

@Article{schaeffer_forecasting_2020,
  title = {Forecasting client retention — {A} machine-learning approach},
  volume = {52},
  issn = {09696989},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0969698919302668},
  doi = {10.1016/j.jretconser.2019.101918},
  language = {en},
  urldate = {2020-09-07},
  journal = {Journal of Retailing and Consumer Services},
  author = {Satu Elisa Schaeffer and Sara Veronica {Rodriguez Sanchez}},
  month = {jan},
  year = {2020},
  abstract = {In the age of big data, companies store practically all data on any client transaction. Making use of this data is commonly done with machine-learning techniques so as to turn it into information that can be used to drive business decisions. Our interest lies in using data on prepaid unitary services in a business-to-business setting to forecast client retention: whether a particular client is at risk of being lost before they cease being clients. The purpose of such a forecast is to provide the company with an opportunity to reach out to such clients as an effort to ensure their retention. We work with monthly records of client transactions: each client is represented as a series of purchases and consumptions. We vary (1) the length of the time period used to make the forecast, (2) the length of a period of inactivity after which a client is assumed to be lost, and (3) how far in advance the forecast is made. Our experimental work finds that current machine-learning techniques able to adequately predict, well in advance, which clients will be lost. This knowledge permits a company to focus marketing efforts on such clients as early as three months in advance.},
  pages = {101918},
}

@Article{devriendt_why_2019,
  title = {Why you should stop predicting customer churn and start using uplift models},
  issn = {00200255},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0020025519312022},
  doi = {10.1016/j.ins.2019.12.075},
  language = {en},
  urldate = {2020-09-07},
  journal = {Information Sciences},
  author = {Floris Devriendt and Jeroen Berrevoets and Wouter Verbeke},
  month = {dec},
  year = {2019},
  abstract = {Uplift modeling has received increasing interest in both the business analytics research community and the industry as an improved paradigm for predictive analytics for data-driven operational decision-making. The literature, however, does not provide conclusive empirical evidence that uplift modeling outperforms predictive modeling. Case studies that directly compare both approaches are lacking, and the performance of predictive models and uplift models as reported in various experimental studies cannot be compared indirectly since different evaluation measures are used to assess their performance. Therefore, in this paper, we introduce a novel evaluation metric called the maximum profit uplift (MPU) measure that allows assessing the performance in terms of the maximum potential profit that can be achieved by adopting an uplift model. This measure, developed for evaluating customer churn uplift models, extends the maximum profit measure for evaluating customer churn prediction models. While introducing the MPU measure, we describe the generally applicable liftup curve and liftup measure for evaluating uplift models as counterparts of the lift curve and lift measure that are broadly used to evaluate predictive models. These measures are subsequently applied to assess and compare the performance of customer churn prediction and uplift models in a case study that applies uplift modeling to customer retention in the financial industry. We observe that uplift models outperform predictive models and lead to improved profitability of retention campaigns.},
  pages = {S0020025519312022},
}

@Article{xie_systematic_2020,
  title = {Systematic comparisons of customer base prediction accuracy: {Pareto}/{NBD} versus neural network},
  volume = {ahead-of-print},
  issn = {1355-5855},
  shorttitle = {Systematic comparisons of customer base prediction accuracy},
  url = {https://www.emerald.com/insight/content/doi/10.1108/APJML-09-2019-0520/full/html},
  doi = {10.1108/APJML-09-2019-0520},
  abstract = {Purpose: Predicting the inactivity and the repeat transaction frequency of a firm's customer base is critical for customer relationship management. The literature offers two main approaches to such predictions: stochastic modeling efforts represented by Pareto/NBD and machine learning represented by neural network analysis. As these two approaches have been developed and applied in parallel, this study systematically compares the two approaches in their prediction accuracy and defines the relatively appropriate implementation scenarios of each model. Design/methodology/approach: By designing a rolling exploration scheme with moving calibration/holdout combinations of customer data, this research explores the two approaches' relative performance by first utilizing three real world datasets and then a wide range of simulated datasets. Findings: The empirical result indicates that neither approach is dominant and identifies patterns of relative applicability between the two. Such patterns are consistent across the empirical and the simulated datasets. Originality/value: This study contributes to the literature by bridging two previously parallel analytical approaches applicable to customer base predictions. No prior research has rendered a comprehensive comparison on the two approaches' relative performance in customer base predictions as this study has done. The patterns identified in the two approaches' relative prediction performance provide practitioners with a clear-cut menu upon selecting approaches for customer base predictions. The findings further urge marketing scientists to reevaluate prior modeling efforts during the past half century by assessing what can be replaced by black boxes such as NNA and what cannot.},
  language = {en},
  number = {ahead-of-print},
  urldate = {2020-09-07},
  journal = {Asia Pacific Journal of Marketing and Logistics},
  author = {Shao-Ming Xie and Chun-Yao Huang},
  month = {may},
  year = {2020},
}

@Article{routh_estimating_2020,
  title = {Estimating customer churn under competing risks},
  issn = {0160-5682, 1476-9360},
  url = {https://www.tandfonline.com/doi/full/10.1080/01605682.2020.1776166},
  doi = {10.1080/01605682.2020.1776166},
  language = {en},
  urldate = {2020-09-07},
  journal = {Journal of the Operational Research Society},
  author = {Pallav Routh and Arkajyoti Roy and Jeff Meyer},
  month = {aug},
  year = {2020},
  abstract = {Customer churn management focuses on identifying potential churners and implementing incentives that can cure churn. The success of a churn management program depends on accurately identifying potential churners and understanding what conditions contribute to churn. However, in the presence of uncertainties in the process of churn, such as competing risks and unpredictable customer behaviour, the accuracy of the prediction models can be limited. To overcome this, we employ a competing risk methodology within a random survival forest framework that accurately computes the risks of churn and identifies relationships between the risks and customer behaviour. In contrast to existing methods, the proposed model does not rely on a specific functional form to model the relationships between risk and behaviour, and does not have underlying distributional assumptions, both of which are limitations faced in practice. The performance of the method is evaluated using data from a membership-based firm in the hospitality industry, where customers face two competing churning events. The proposed model improves prediction accuracy by up to 20%, compared to conventional models. The findings from this work can allow marketers to identify and understand churners, and develop strategies on how to design and implement incentives.},
  pages = {1--18},
}

@Article{wang_comparative_2020,
  title = {A {Comparative} {Study} on {Contract} {Recommendation} {Model}: {Using} {Macao} {Mobile} {Phone} {Datasets}},
  volume = {8},
  issn = {2169-3536},
  shorttitle = {A {Comparative} {Study} on {Contract} {Recommendation} {Model}},
  url = {https://ieeexplore.ieee.org/document/9004530/},
  doi = {10.1109/ACCESS.2020.2975029},
  urldate = {2020-09-07},
  journal = {IEEE Access},
  author = {Gang Wang and Naiqi Wu},
  year = {2020},
  pages = {39747--39757},
  abstract = {Bordering on the Mainland of China, Macao is a city with diverse population and is also a world-famous gambling city. The residents living in Macao are not only localresidents, but a large-scale of residents are from the Mainland of China. There are diversified living habits among different resident groups. Some of them live and work in Macao most of the time, while others cross the border between Macao and the Mainland of China frequently. These diversities result in different demands for mobile phone contract services. Machine learning and data mining are powerful tools used by telecom companies to monitor the behavior of their customers. This paper aims to build a contract service recommendation model suitable for Macao telecom companies, which can accurately recommend the best fit contract services according to call data records (CDR). Based on data mining algorithm and large number of comparative experiments, this study conducts a study on variety of factors that have effect on the contract recommendation for obtaining a composition structure of factors that have the greatest impact on contract recommendation accuracy so as to improve the operation efficiency of the model. In addition, this study makes comparisons among five classification algorithms, including Bayesian, logistic, Random trees, Decision tree (C5.0), and KNN (k-nearest neighbor). The results are analyzed by different metrics such as Gain, Lift, ROI (region of interest), Response, and Profile. Experimental results demonstrate that the best classifier is decision trees (C5.0), and the contract service obtained by this method can achieve a recommendation accuracy close to 90%, which successfully reaches the expected goal.},
  file = {Wang_Wu_2020_A Comparative Study on Contract Recommendation Model.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\HETARXRA\\Wang_Wu_2020_A Comparative Study on Contract Recommendation Model.pdf:application/pdf},
}

@Article{libai_brave_2020,
  title = {Brave {New} {World}? {On} {AI} and the {Management} of {Customer} {Relationships}},
  volume = {51},
  issn = {10949968},
  shorttitle = {Brave {New} {World}?},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1094996820300839},
  doi = {10.1016/j.intmar.2020.04.002},
  language = {en},
  urldate = {2020-09-07},
  journal = {Journal of Interactive Marketing},
  author = {Barak Libai and Yakov Bart and Sonja Gensler and Charles F. Hofacker and Andreas Kaplan and Kim Kötterheinrich and Eike Benjamin Kroll},
  month = {aug},
  year = {2020},
  abstract = {In light of the emerging discourse on AI systems' effect on society, whose perception swings widely between utopian and dystopian, we conduct herein a critical analysis of how artificial intelligence (AI) affects the essential nature of customer relationship management (CRM). To do so, we survey the AI capabilities that will transform CRM into AI-CRM and examine how the transformation will influence customer acquisition, development, and retention. We highlight in particular how AI-CRM's improving ability to predict customer lifetime value will generate an inexorable rise in implementing adapted treatment of customers, leading to greater customer prioritization and service discrimination in markets. We further consider the consequences for firms and the challenges to regulators.},
  pages = {44--56},
}

@Article{singh_multi-objective_2020,
  title = {Multi-objective {Evolutionary} {Approach} for the {Performance} {Improvement} of {Learners} using {Ensembling} {Feature} {Selection} and {Discretization} {Technique} on {Medical} {Data}},
  volume = {16},
  issn = {15734056},
  url = {https://www.eurekaselect.com/165063/article},
  doi = {10.2174/1573405614666180903114534},
  abstract = {Background: Biomedical data is filled with continuous real values; these values in the
feature set tend to create problems like underfitting, the curse of dimensionality and increase in
misclassification rate because of higher variance. In response, pre-processing techniques on dataset
minimizes the side effects and have shown success in maintaining the adequate accuracy. Aims: Feature selection and discretization are the two necessary preprocessing steps that were effectively employed to handle the data redundancies in the biomedical data. However, in the previous works, the absence of unified effort by integrating feature selection and discretization together in solving the data redundancy problem leads to the disjoint and fragmented field. This paper proposes a novel multi-objective based dimensionality reduction framework, which incorporates both discretization and feature reduction as an ensemble model for performing feature selection and discretization. Selection of optimal features and the categorization of discretized and non-discretized features from the feature subset is governed by the multi-objective genetic algorithm (NSGA-II). The two objectives, minimizing the error rate during the feature selection and maximizing the information gain, while discretization is considered as fitness criteria. Methods: The proposed model used wrapper-based feature selection algorithm to select the optimal features and categorized these selected features into two blocks namely discretized and nondiscretized blocks. The feature belongs to the discretized block will participate in the binary discretization while the second block features will not be discretized and used in its original form. Results: For the establishment and acceptability of the proposed ensemble model, the experiment is conducted on the fifteen medical datasets, and the metric such as accuracy, mean and standard deviation are computed for the performance evaluation of the classifiers. Conclusion: After an extensive experiment conducted on the dataset, it can be said that the proposed model improves the classification rate and outperform the base learner.},
  language = {en},
  number = {4},
  urldate = {2020-09-07},
  journal = {Current Medical Imaging Formerly Current Medical Imaging Reviews},
  author = {Deepak Singh and Dilip Singh Sisodia and Pradeep Singh},
  month = {may},
  year = {2020},
  pages = {355--370},
  file = {Singh et al_2020_Multi-objective Evolutionary Approach for the Performance Improvement of.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\A2UT4M34\\Singh et al_2020_Multi-objective Evolutionary Approach for the Performance Improvement of.pdf:application/pdf},
}

@Article{jegierski_outside_2020,
  title = {An “{Outside} the {Box}” {Solution} for {Imbalanced} {Data} {Classification}},
  volume = {8},
  issn = {2169-3536},
  url = {https://ieeexplore.ieee.org/document/9136875/},
  doi = {10.1109/ACCESS.2020.3007801},
  urldate = {2020-09-07},
  journal = {IEEE Access},
  author = {Hubert Jegierski and Stanislaw Saganowski},
  year = {2020},
  pages = {125191--125209},
  abstract = {A common problem of the real-world data sets is the class imbalance, which can significantly affect the classification abilities of classifiers. Numerous methods have been proposed to cope with this problem; however, even state-of-the-art methods offer a limited improvement (if any) for data sets with critically under-represented minority classes. For such problematic cases, an “outside the box” solution is required. Therefore, we propose a novel technique, called enrichment, which uses the information (observations) from the external data set(s). We present three approaches to implement the enrichment technique: (1) selecting observations randomly, (2) iteratively choosing observations that improve the classification result, (3) adding observations that help the classifier to determine the border between classes better. We then thoroughly analyze developed solutions on ten real-world data sets to experimentally validate their usefulness. On average, our best approach improves the classification quality by 27%, and in the best case, by outstanding 66%. We also compare our technique with the state-of-the-art methods. We find that our technique surpasses the existing methods performing, on average, 21% better. The advantage is especially noticeable for the smallest data sets, for which existing methods failed, while our solutions achieved the best results. Additionally, the enrichment technique applies to both the multi-class and binary classification tasks. It can also be combined with other techniques dealing with the class imbalance problem.},
  file = {Jegierski_Saganowski_2020_An “Outside the Box” Solution for Imbalanced Data Classification.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\ZNPIINJK\\Jegierski_Saganowski_2020_An “Outside the Box” Solution for Imbalanced Data Classification.pdf:application/pdf},
}

@InProceedings{liu_user_2019,
  address = {Macao, Macao},
  title = {User {Classification} in {Electronic} {Devices} {Using} {Machine} {Learning} {Methods}},
  isbn = {978-1-72813-804-6},
  url = {https://ieeexplore.ieee.org/document/8978567/},
  doi = {10.1109/IEEM44572.2019.8978567},
  urldate = {2020-09-07},
  booktitle = {2019 {IEEE} {International} {Conference} on {Industrial} {Engineering} and {Engineering} {Management} ({IEEM})},
  publisher = {IEEE},
  author = {Xinglu Liu and Wan Wang and Wai Kin {Victor Chan} and Chiung Ying Kuan and Junyoung Lee},
  month = {dec},
  year = {2019},
  abstract = {User classification is a major concern for electronic device providers, because accurate and efficient classification can cut operating cost of company significantly. To deal with this problem, manufacturers try to classify customer into several categories and recognize the characteristic of users, then adopt different promotion strategies to improve sales revenue. This study aims to build models to divide users into several categories, and identify critical and controllable features which dramatically affects classification results. We test proposed models with real data coming from an electronic device producer. Results shows that random forest model performs best. Our main contributions are: 1) we focus on user classification of electronic devices; although many existing studies have discussed similar problem, few of them focus on applications in electronic devices; 2) we consider imbalance sample, and datasets are from real company. This work will be helpful for electronic device producers to improve operation and enhance marketing competitiveness.},
  pages = {1553--1556},
}

@Article{madichetty_disaster_2019,
  title = {Disaster damage assessment from the tweets using the combination of statistical features and informative words},
  volume = {9},
  issn = {1869-5450, 1869-5469},
  url = {http://link.springer.com/10.1007/s13278-019-0579-5},
  doi = {10.1007/s13278-019-0579-5},
  language = {en},
  number = {1},
  urldate = {2020-09-07},
  journal = {Social Network Analysis and Mining},
  author = {Sreenivasulu Madichetty and M. Sridevi},
  month = {dec},
  year = {2019},
  abstract = {Nowadays, Twitter has become more popular among the users for communicating the information, especially during disaster. Identifying tweets related to the target event during disaster is a challenging task. Many prior studies discussed situational and non-situational information related to disaster. The detection of tweets related to damage assessment is a very difficult task in social media because it is a subset of situational information. One of the following drawbacks has been present in the existing damage assessment works: (1) focused only on infrastructure damage but does not include human damage in the assessment, (2) focused only on social media image data for damage assessment and (3) focused only on regional language tweets. To overcome these issues, Stacking-based Ensemble using Statistical features and Informative Words (SESIW) is proposed for detecting the tweets related to damage assessment. It uses proposed features, namely frequency of hashtags, user mentions, wh-words, URLs, count of numerals and informative words. Informative words are mined using term frequency and inverse document frequency technique. The SESIW method is tested on different Twitter disaster datasets, and it outperforms the baseline SVM with Bag-of-Words model.},
  pages = {42},
}

@Article{khodabandelou_fuzzy_2019,
  title = {Fuzzy neural network with support vector-based learning for classification and regression},
  volume = {23},
  issn = {1432-7643, 1433-7479},
  url = {http://link.springer.com/10.1007/s00500-019-04116-x},
  doi = {10.1007/s00500-019-04116-x},
  language = {en},
  number = {23},
  urldate = {2020-09-07},
  journal = {Soft Computing},
  author = {Ghazaleh Khodabandelou and Mohammad Mehdi Ebadzadeh},
  month = {dec},
  year = {2019},
  abstract = {Fuzzy neural network (FNN) and support vector machine (SVM) are two prominent and powerful learning models broadly used for classification and regression. FNN has a significant local representation and human reasoning advantage. However, the drawback of such a network is that the focal point of the learning algorithms is minimizing empirical risk. In contrary to FNN, SVM emphasizes simultaneously on minimizing empirical and expected risks, which theoretically leads to an excellent generalization performance power. In this paper, we show that a Takagi–Sugeno–Kang (TSK)-type-based fuzzy neural network is, in fact, equivalent to an SVM with an adaptive kernel based on fuzzy rules generated in this FNN. Consequently, it is possible to learn the last layer of the FNN using the concepts of SVM and thus taking the advantage of SVM in generalization. In fact, on the one side, the proposed method is an SVM with an adaptive kernel based on fuzzy rules and, on the other side, it is a TSK–FNN with SVM-based learning. As a matter of fact, the defined kernel in FNN is an adaptive kernel based on data characteristics, which is derived from the fuzzy rules generated by the FNN itself. The results obtained from the proposed method for classification and regression outperform the results of SVM with the conventional kernels and fuzzy neural network trained in the traditional way. },
  pages = {12153--12168},
}

@Article{khan_machine_2019,
  title = {Machine learning facilitated business intelligence ({Part} {II}): {Neural} networks optimization techniques and applications},
  volume = {120},
  issn = {0263-5577},
  shorttitle = {Machine learning facilitated business intelligence ({Part} {II})},
  url = {https://www.emerald.com/insight/content/doi/10.1108/IMDS-06-2019-0351/full/html},
  doi = {10.1108/IMDS-06-2019-0351},
  abstract = {Purpose: The purpose of this paper is three-fold: to review the categories explaining mainly optimization algorithms (techniques) in that needed to improve the generalization performance and learning speed of the Feedforward Neural Network (FNN); to discover the change in research trends by analyzing all six categories (i.e. gradient learning algorithms for network training, gradient free learning algorithms, optimization algorithms for learning rate, bias and variance (underfitting and overfitting) minimization algorithms, constructive topology neural networks, metaheuristic search algorithms) collectively; and recommend new research directions for researchers and facilitate users to understand algorithms real-world applications in solving complex management, engineering and health sciences problems. Design/methodology/approach: The FNN has gained much attention from researchers to make a more informed decision in the last few decades. The literature survey is focused on the learning algorithms and the optimization techniques proposed in the last three decades. This paper (Part II) is an extension of Part I. For the sake of simplicity, the paper entitled “Machine learning facilitated business intelligence (Part I): Neural networks learning algorithms and applications” is referred to as Part I. To make the study consistent with Part I, the approach and survey methodology in this paper are kept similar to those in Part I. Findings: Combining the work performed in Part I, the authors studied a total of 80 articles through popular keywords searching. The FNN learning algorithms and optimization techniques identified in the selected literature are classified into six categories based on their problem identification, mathematical model, technical reasoning and proposed solution. Previously, in Part I, the two categories focusing on the learning algorithms (i.e. gradient learning algorithms for network training, gradient free learning algorithms) are reviewed with their real-world applications in management, engineering, and health sciences. Therefore, in the current paper, Part II, the remaining four categories, exploring optimization techniques (i.e. optimization algorithms for learning rate, bias and variance (underfitting and overfitting) minimization algorithms, constructive topology neural networks, metaheuristic search algorithms) are studied in detail. The algorithm explanation is made enriched by discussing their technical merits, limitations, and applications in their respective categories. Finally, the authors recommend future new research directions which can contribute to strengthening the literature. Research limitations/implications: The FNN contributions are rapidly increasing because of its ability to make reliably informed decisions. Like learning algorithms, reviewed in Part I, the focus is to enrich the comprehensive study by reviewing remaining categories focusing on the optimization techniques. However, future efforts may be needed to incorporate other algorithms into identified six categories or suggest new category to continuously monitor the shift in the research trends. Practical implications: The authors studied the shift in research trend for three decades by collectively analyzing the learning algorithms and optimization techniques with their applications. This may help researchers to identify future research gaps to improve the generalization performance and learning speed, and user to understand the applications areas of the FNN. For instance, research contribution in FNN in the last three decades has changed from complex gradient-based algorithms to gradient free algorithms, trial and error hidden units fixed topology approach to cascade topology, hyperparameters initial guess to analytically calculation and converging algorithms at a global minimum rather than the local minimum. Originality/value: The existing literature surveys include comparative study of the algorithms, identifying algorithms application areas and focusing on specific techniques in that it may not be able to identify algorithms categories, a shift in research trends over time, application area frequently analyzed, common research gaps and collective future directions. Part I and II attempts to overcome the existing literature surveys limitations by classifying articles into six categories covering a wide range of algorithm proposed to improve the FNN generalization performance and convergence rate. The classification of algorithms into six categories helps to analyze the shift in research trend which makes the classification scheme significant and innovative.},
  language = {en},
  number = {1},
  urldate = {2020-09-07},
  journal = {Industrial Management \& Data Systems},
  author = {Waqar Ahmed Khan and S.H. Chung and Muhammad Usman Awan and Xin Wen},
  month = {nov},
  year = {2019},
  pages = {128--163},
}

@Article{wilson_can_2019,
  title = {Can artificial neural network models be used to improve the analysis of {B2B} marketing research data?},
  volume = {35},
  issn = {0885-8624, 0885-8624},
  url = {https://www.emerald.com/insight/content/doi/10.1108/JBIM-01-2019-0060/full/html},
  doi = {10.1108/JBIM-01-2019-0060},
  abstract = {Purpose: Artificial neural network (ANN) models, part of the discipline of machine learning and artificial intelligence, are becoming more popular in the marketing literature and in marketing practice. This paper aims to provide a series of tests between ANN models and competing predictive models. Design/methodology/approach: A total of 46 pairs of models were evaluated in an objective model-building environment. Either logistic regression or multiple regression models were developed and then were compared to ANN models using the same set of input variables. Three sets of B2B data were used to test the models. Emphasis also was placed on evaluating small samples. Findings: ANN models tend to generate model predictions that are more accurate or the same as logistic regression models. However, when ANN models are compared to multiple regression models, the results are mixed. For small sample sizes, the modeling results are the same as for larger samples. Research limitations/implications: Like all marketing research, this application is limited by the methods and the data used to conduct the research. The findings strongly suggest that, because of their predictive accuracy, ANN models will have an important role in the future of B2B marketing research and model-building applications. Practical implications: ANN models should be carefully considered for potential use in marketing research and model-building applications by B2B academics and practitioners alike. Originality/value: The research contributes to the B2B marketing literature by providing a more rigorous test on ANN models using B2B data than has been conducted before.},
  language = {en},
  number = {3},
  urldate = {2020-09-07},
  journal = {Journal of Business \& Industrial Marketing},
  author = {R. Dale Wilson and Harriette Bettis-Outland},
  month = {nov},
  year = {2019},
  pages = {495--507},
}

@Article{guajardo_how_2019,
  title = {How {Do} {Usage} and {Payment} {Behavior} {Interact} in {Rent}‐to‐{Own} {Business} {Models}? {Evidence} from {Developing} {Economies}},
  volume = {28},
  issn = {1059-1478, 1937-5956},
  shorttitle = {How {Do} {Usage} and {Payment} {Behavior} {Interact} in {Rent}‐to‐{Own} {Business} {Models}?},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/poms.13067},
  doi = {10.1111/poms.13067},
  language = {en},
  number = {11},
  urldate = {2020-09-07},
  journal = {Production and Operations Management},
  author = {Jose A. Guajardo},
  month = {nov},
  year = {2019},
  pages = {2808--2822},
  abstract = {The diffusion of technological innovations in developing economies has been facilitated by the use of rent‐to‐own business models, which give flexibility to customers by allowing them to make incremental payments over time. Understanding the implications of this flexibility is a fundamental problem for an increasing number of firms operating in these markets. In this study, we empirically analyze how consumer usage and payment behaviors interact in an application of rent‐to‐own to the distribution of solar lamps in developing countries. By exploiting the longitudinal variation in the data—and hence accounting for intrinsic differences between customers—the analysis led to three main insights. First, higher usage rates lowered the probability of late payments by customers. Our characterization of this engagement effect enhances existing knowledge of the drivers of payment behavior in these environments. Second, customers often “bundled” payments, making advance payments for future product access. We showed that bundling the initial payment led to lower usage rates (bundling effect), suggesting that firms may not benefit from advance payments upfront, and that they should closely track usage patterns from these customers. Finally, we showed that first‐period usage information can improve the accuracy of predictive models of default and that observing usage rates in subsequent periods does not lead to further improvements. Overall, the analysis highlights the importance for firms of jointly tracking and analyzing payment and usage behavior by customers, particularly in initial stages of the adoption process.},
  file = {Guajardo_2019_How Do Usage and Payment Behavior Interact in Rent‐to‐Own Business Models.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\KW3BXA9X\\Guajardo_2019_How Do Usage and Payment Behavior Interact in Rent‐to‐Own Business Models.pdf:application/pdf},
}

@Article{han_prediction_2019,
  title = {Prediction of cooling efficiency of forced-air precooling systems based on optimized differential evolution and improved {BP} neural network},
  volume = {84},
  issn = {15684946},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1568494619305149},
  doi = {10.1016/j.asoc.2019.105733},
  language = {en},
  urldate = {2020-09-07},
  journal = {Applied Soft Computing},
  author = {Jia-Wei Han and Qing-Xue Li and Hua-Rui Wu and Hua-Ji Zhu and Yu-Ling Song},
  month = {nov},
  year = {2019},
  abstract = {This paper proposes an approach to predict the efficiency of forced-air cooling of fresh apples that combines the optimized differential evolution (DE) algorithm and the back-propagation (BP) neural network algorithm. First, to balance population diversity and fast convergence, the individual mutation operation of the basic DE algorithm was optimized by dividing the entire population into two equal parts according to the fitness value of individuals, and DE-best-1 and DE-current-to-rand-1 are used as individual mutation operations for the superior- and inferior-part individuals, respectively. Moreover, the selection operation of basic DE was also changed by using a crowding scheme, which helps maintain population diversity and discover more regions containing the global optima. Second, an optimized DE-BP neural network model was established by using the optimized DE to determine the initial weights and thresholds of the BP neural network to avoid being trapped in local minima, following which the effect of input parameters on the network output was subjected to a comprehensive sensitivity analysis based on the trained neural network. The results show that the optimized DE-BP model accurately predicts the efficiency with which apples are cooled. Furthermore, the airflow velocity and total opening area have a significant negative correlation with the average apple temperature and a positive correlation with the cooling rate of the apples. Finally, the most important factor influencing the cooling efficiency of the pre-cooling system is the total opening area of the ventilated packaging.},
  pages = {105733},
}

@InProceedings{rivera-castro_topology-based_2019,
  address = {Washington, DC, USA},
  title = {Topology-{Based} {Clusterwise} {Regression} for {User} {Segmentation} and {Demand} {Forecasting}},
  isbn = {978-1-72814-493-1},
  url = {https://ieeexplore.ieee.org/document/8964133/},
  doi = {10.1109/DSAA.2019.00048},
  urldate = {2020-09-07},
  booktitle = {2019 {IEEE} {International} {Conference} on {Data} {Science} and {Advanced} {Analytics} ({DSAA})},
  publisher = {IEEE},
  author = {Rodrigo Rivera-Castro and Aleksandr Pletnev and Polina Pilyugina and Grecia Diaz and Ivan Nazarov and Wanyi Zhu and Evgeny Burnaev},
  month = {oct},
  year = {2019},
  abstract = {Topological Data Analysis (TDA) is a recent approach to analyze data sets from the perspective of their topological structure. Its use for time series data has been limited. In this work, a system developed for a leading provider of cloud computing combining both user segmentation and demand forecasting is presented. It consists of a TDA-based clustering method for time series inspired by a popular managerial framework for customer segmentation and extended to the case of clusterwise regression using matrix factorization methods to forecast demand. Increasing customer loyalty and producing accurate forecasts remain active topics of discussion both for researchers and managers. Using a public and a novel proprietary data set of commercial data, this research shows that the proposed system enables analysts to both cluster their user base and plan demand at a granular level with significantly higher accuracy than a state of the art baseline. This work thus seeks to introduce TDA-based clustering of time series and clusterwise regression with matrix factorization methods as viable tools for the practitioner.},
  pages = {326--336},
}

@Article{vijaya_efficient_2019,
  title = {An efficient system for customer churn prediction through particle swarm optimization based feature selection model with simulated annealing},
  volume = {22},
  issn = {1386-7857, 1573-7543},
  url = {http://link.springer.com/10.1007/s10586-017-1172-1},
  doi = {10.1007/s10586-017-1172-1},
  language = {en},
  number = {S5},
  urldate = {2020-09-07},
  journal = {Cluster Computing},
  author = {J. Vijaya and E. Sivasankar},
  month = {sep},
  year = {2019},
  abstract = {Churn prediction in telecom has gained a huge prominence in the recent times due to the extensive interests exhibited by the stakeholders, large number of competitors and huge revenue losses incurred due to churn. Predicting telecom churn is challenging due to the voluminous and sparse nature of the data. This paper presents a technique for the telecom churn prediction that employs particle swarm optimization (PSO) and proposes three variants of PSO for churn prediction namely, PSO incorporated with feature selection as its pre-processing mechanism, PSO embedded with simulated annealing and finally PSO with a combination of both feature selection and simulated annealing. The proposed classifiers were compared with decision tree, naive bayes, K-nearest neighbor, support vector machine, random forest and three hybrid models to analyze their predictability levels and performance aspects. Accuracy, true positive rate, true negative rate, false positive rate, Precision, F-Measures, receiver operating characteristic and precision-recall plots were used as performance metrics. Experiments reveal that the performance of metaheuristics was more efficient and they also exhibited better predictability levels.},
  pages = {10757--10768},
}

@InProceedings{guitart_non-paying_2019,
  address = {San Luis Obispo California USA},
  title = {From non-paying to premium: predicting user conversion in video games with ensemble learning},
  isbn = {978-1-4503-7217-6},
  shorttitle = {From non-paying to premium},
  url = {https://dl.acm.org/doi/10.1145/3337722.3341855},
  doi = {10.1145/3337722.3341855},
  language = {en},
  urldate = {2020-09-07},
  booktitle = {Proceedings of the 14th {International} {Conference} on the {Foundations} of {Digital} {Games}},
  publisher = {ACM},
  author = {Anna Guitart and Shi Hui Tan and Ana Fernández del Río and Pei Pei Chen and África Periáñez},
  month = {aug},
  year = {2019},
  pages = {1--9},
  abstract = {Retaining premium players is key to the success of free-to-play games, but most of them do not start purchasing right after joining the game. By exploiting the exceptionally rich datasets recorded by modern video games---which provide information on the individual behavior of each and every player---survival analysis techniques can be used to predict what players are more likely to become paying (or even premium) users and when, both in terms of time and game level, the conversion will take place. Here we show that a traditional semi-parametric model (Cox regression), a random survival forest (RSF) technique and a method based on conditional inference survival ensembles all yield very promising results. However, the last approach has the advantage of being able to correct the inherent bias in RSF models by dividing the procedure into two steps: first selecting the best predictor to perform the splitting and then the best split point for that covariate. The proposed conditional inference survival ensembles method could be readily used in operational environments for early identification of premium players and the parts of the game that may prompt them to become paying users. Such knowledge would allow developers to induce their conversion and, more generally, to better understand the needs of their players and provide them with a personalized experience, thereby increasing their engagement and paving the way to higher monetization.},
  file = {Guitart et al_2019_From non-paying to premium.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\ZRRD7SCQ\\Guitart et al_2019_From non-paying to premium.pdf:application/pdf},
}

@Article{yu_fuzzy_2019,
  title = {Fuzzy {One}-{Class} {Extreme} {Auto}-encoder},
  volume = {50},
  issn = {1370-4621, 1573-773X},
  url = {http://link.springer.com/10.1007/s11063-018-9952-z},
  doi = {10.1007/s11063-018-9952-z},
  language = {en},
  number = {1},
  urldate = {2020-09-07},
  journal = {Neural Processing Letters},
  author = {Hualong Yu and Dan Sun and Xiaoyan Xi and Xibei Yang and Shang Zheng and Qi Wang},
  month = {aug},
  year = {2019},
  abstract = {A novel one class classification (OCC) algorithm called fuzzy one class extreme auto-encoder (FOCEAE) is presented in this article. The algorithm combines the precision of probability density estimation and the generalization of neural networks to accurately generate the compact bound for the target class cases. Firstly, a K-nearest-neighbors non-parametric probability density estimation-alike strategy is used to estimate the relative densities of all target class training objects, then the relative densities are transformed to be the fuzzy coefficients for further training fuzzy extreme learning machine (FELM) model. Specifically, considering there are only one-class instances, FELM is trained in the form of auto-encoder, i.e., each input equals to be the expected output of the network. Finally, the bound (i.e., the threshold) of the target class cases is determined by calculating and ranking the reconstructed errors of all training instances. We show the effectiveness and superiority of the proposed FOCEAE algorithm by comparing it with some benchmark OCC algorithms on a mass of data sets in terms of both F-measure and G-mean metrics. The statistical results also indicate that the proposed algorithm performs significantly better than some conventional ones.},
  pages = {701--727},
}

@Article{lopez-diaz_criterion_2019,
  title = {A criterion for the comparison of binary classifiers based on a stochastic dominance with an application to the sale of home insurances},
  volume = {2019},
  issn = {0346-1238, 1651-2030},
  url = {https://www.tandfonline.com/doi/full/10.1080/03461238.2019.1574237},
  doi = {10.1080/03461238.2019.1574237},
  language = {en},
  number = {6},
  urldate = {2020-09-07},
  journal = {Scandinavian Actuarial Journal},
  author = {María Concepción López-Díaz and Miguel López-Díaz and Sergio Martínez-Fernández},
  month = {jul},
  year = {2019},
  pages = {453--477},
  abstract = {Binary classification is an essential matter in multiple real-life problems, and so, the comparison of the performance of classifiers is a key issue. A criterion for that purpose is introduced in this manuscript. That criterion is based on a stochastic dominance, and permits to compare classifiers in subgroups of the population with the same size. By means of the new criterion, the alteration of the size of the subgroups where classifiers are compared, does not entail the modification of the suitable classifier. Characterization results of the criterion are proved. For that purpose, connections of the criterion with the theory of copulas, and with a tool introduced in the manuscript, the so-called continuity modelling vector, are essential. An application to the comparison of some classifiers for the detection of purchasers of home insurances is developed.},
  file = {López-Díaz et al_2019_A criterion for the comparison of binary classifiers based on a stochastic.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\X5BJ5HEY\\López-Díaz et al_2019_A criterion for the comparison of binary classifiers based on a stochastic.pdf:application/pdf},
}

@InProceedings{ullah_churn_2019,
  address = {Swat, Pakistan},
  title = {Churn {Prediction} in {Banking} {System} using {K}-{Means}, {LOF}, and {CBLOF}},
  isbn = {978-1-72813-825-1},
  url = {https://ieeexplore.ieee.org/document/8940667/},
  doi = {10.1109/ICECCE47252.2019.8940667},
  urldate = {2020-09-07},
  booktitle = {2019 {International} {Conference} on {Electrical}, {Communication}, and {Computer} {Engineering} ({ICECCE})},
  publisher = {IEEE},
  author = {Irfan Ullah and Hameed Hussain and Iftikhar Ali and Anum Liaquat},
  month = {jul},
  year = {2019},
  abstract = {Customer churn prediction helps in identifying those customers who are probable to stop a subscriptions, products or services, and is therefore very essential for any business. Churn predictions can be very valuable for customers retentions, as it helps in predicting customer that are at risks of sendoff. It is more challenging to put forth churn prediction in banking sector, as there are no contractual agreements between a customer and a bank regarding the duration of services. Loss of customers can be very costly as it is very expensive to obtain new customers in this age of competition. There are many churn prediction techniques however; K-Means, Local Outlier Factors (LOF) and Cluster-Based Local Outlier Factors (CBLOF) have not been used so far for this purpose. In this research, we apply these techniques for customer churn prediction. The results are evaluated and analyzed using Precision (Pr), Recall (Re) and F1-Measures to justify the efficiency and effectiveness of this research.},
  pages = {1--6},
}

@InProceedings{thuseethan_emotion_2019,
  address = {Budapest, Hungary},
  title = {Emotion {Intensity} {Estimation} from {Video} {Frames} using {Deep} {Hybrid} {Convolutional} {Neural} {Networks}},
  isbn = {978-1-72811-985-4},
  url = {https://ieeexplore.ieee.org/document/8852365/},
  doi = {10.1109/IJCNN.2019.8852365},
  urldate = {2020-09-07},
  booktitle = {2019 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
  publisher = {IEEE},
  author = {Selvarajah Thuseethan and Sutharshan Rajasegarar and John Yearwood},
  month = {jul},
  year = {2019},
  abstract = {Detecting emotional states of human from videos is essential in order to automate the process of profiling human behaviour, which has applications in a variety of domains, such as social, medical and behavioural science. Considerable research has been carried out for binary classification of emotions using facial expressions. However, a challenge exists to automate the feature extraction process to recognise the various intensities or levels of emotions. The intensity information of emotions is essential for tasks such as sentiment analysis. In this work, we propose a metric-based intensity estimation mechanism for primary emotions, and a deep hybrid convolutional neural network-based approach to recognise the defined intensities of the primary emotions from spontaneous and posed sequences. Further, we extend the intensity estimation approach to detect the basic emotions. The frame level facial action coding system annotations and the intensities of action units associated with each primary emotion are considered for deriving the various intensity levels of emotions. The evaluation on benchmark datasets demonstrates that our proposed approach is capable of correctly classifying the various intensity levels of emotions as well as detecting them.},
  pages = {1--10},
}

@Article{aaldering_analysis_2019,
  title = {Analysis of technological knowledge stock and prediction of its future development potential: {The} case of lithium-ion batteries},
  volume = {223},
  issn = {09596526},
  shorttitle = {Analysis of technological knowledge stock and prediction of its future development potential},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0959652619308650},
  doi = {10.1016/j.jclepro.2019.03.174},
  language = {en},
  urldate = {2020-09-07},
  journal = {Journal of Cleaner Production},
  author = {Lukas Jan Aaldering and Jens Leker and Chie Hoon Song},
  month = {jun},
  year = {2019},
  abstract = {Amidst growing environmental challenges and increasing penetration of sustainable energy production, the development of efficient energy storage system (EES) is indispensable to fully benefit from the use of intermittent renewable energy sources. Although lithium-ion batteries (LIBs) have been the dominant energy storage technology for applications in consumer electronics, electrified transportation and stationary electricity storage, further technological progress is required for achieving the climate protection objectives. In this context, continuous exploration of new technological opportunities is favorable to advance the global energy transition towards sustainability to success. To facilitate such progress and to avoid potential technological lock-in situation, understanding the underlying latent knowledge construct revealed in patents may prove beneficial. However, previous studies have either focused on analyzing the technological improvements along a particular development path or emphasizing the necessity for improvements in environmental-economic dimensions. Accordingly, a novel patent-based analysis framework is proposed to obtain an inclusive view on the continuing path of technology development concerning the research of LIBs as well as to predict its future development potential based on two machine learning algorithms: Principal component analysis (PCA) and random forest classifier (RFC). In this study, PCA is firstly applied to group various knowledge areas into key technological knowledge stocks. Building upon these results, RFC is adopted to forecast the future developmental potential of technological knowledge areas. The findings show that the research landscape of LIBs is a dynamic environment where new knowledge stocks emerged overtime and the identified knowledge stocks within a defined time interval play an orchestrating role in the advancement of lithium-ion battery technology. In particular, knowledge stocks related to battery management system and hybrid capacitors have gained relevance. The proposed analysis framework can be used to track the past chronology of knowledge accumulation trajectory and can proactively enhance the knowledge management capabilities of experts, who wish to create a path towards a low-carbon economy.},
  pages = {301--311},
}

@Article{batistic_history_2019,
  title = {History, {Evolution} and {Future} of {Big} {Data} and {Analytics}: {A} {Bibliometric} {Analysis} of {Its} {Relationship} to {Performance} in {Organizations}},
  volume = {30},
  issn = {1045-3172, 1467-8551},
  shorttitle = {History, {Evolution} and {Future} of {Big} {Data} and {Analytics}},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-8551.12340},
  doi = {10.1111/1467-8551.12340},
  language = {en},
  number = {2},
  urldate = {2020-09-07},
  journal = {British Journal of Management},
  author = {Saša Batistič and Paul {der Laken}},
  month = {apr},
  year = {2019},
  pages = {229--251},
  abstract = {Big data and analytics (BDA) are gaining momentum, particularly in the practitioner world. Research linking BDA to improved organizational performance seems scarce and widely dispersed though, with the majority focused on specific domains and/or macro‐level relationships. In order to synthesize past research and advance knowledge of the potential organizational value of BDA, the authors obtained a data set of 327 primary studies and 1252 secondary cited papers. This paper reviews this body of research, using three bibliometric methods. First, it elucidates its intellectual foundations via co‐citation analysis. Second, it visualizes the historical evolution of BDA and performance research and its substreams through algorithmic historiography. Third, it provides insights into the field's potential evolution via bibliographic coupling. The results reveal that the academic attention for the BDA–performance link has been increasing rapidly. The study uncovered ten research clusters that form the field's foundation. While research seems to have evolved following two main, isolated streams, the past decade has witnessed more cross‐disciplinary collaborations. Moreover, the study identified several research topics undergoing focused development, including financial and customer risk management, text mining and evolutionary algorithms. The review concludes with a discussion of the implications for different functional management domains and the gaps for both research and practice.},
  file = {Batistič_der Laken_2019_History, Evolution and Future of Big Data and Analytics.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\P3HRVXJL\\Batistič_der Laken_2019_History, Evolution and Future of Big Data and Analytics.pdf:application/pdf},
}

@Article{wang_customer_2019,
  title = {Customer {Churn} {Prediction} with {Feature} {Embedded} {Convolutional} {Neural} {Network}: {An} {Empirical} {Study} in the {Internet} {Funds} {Industry}},
  volume = {18},
  issn = {1469-0268, 1757-5885},
  shorttitle = {Customer {Churn} {Prediction} with {Feature} {Embedded} {Convolutional} {Neural} {Network}},
  url = {https://www.worldscientific.com/doi/abs/10.1142/S1469026819500032},
  doi = {10.1142/S1469026819500032},
  language = {en},
  number = {01},
  urldate = {2020-09-07},
  journal = {International Journal of Computational Intelligence and Applications},
  author = {Chongren Wang and Dongmei Han and Weiguo Fan and Qigang Liu},
  month = {mar},
  year = {2019},
  abstract = {In this paper, we investigated the customer churn prediction problem in the Internet funds industry. We designed a novel feature embedded convolutional neural networks (FE-CNN) method that can automatically learn features from both the dynamic customer behavioral data and static customer demographic data and can utilize the advantage of convolutional neural networks to automatically learn features that capture the structured information. Our results show that our FE-CNN model outperforms the other traditional machine learning models with hand-crafted features, such as logistic regression (LR), support vector machines (SVM), random forests (RF) and neural networks (NN) in terms of accuracy, area under the receiver operating characteristics curve (AUC) and top-decile lift. Furthermore, we found that after adding the demographic data feature to the basic CNN model, the performance of the FE-CNN model improved. Overall, we found that the FE-CNN is the most powerful way to solve the problem of customer churn prediction in the Internet funds industry. Our FE-CNN method can also be applied to other fields that have both dynamic data and static data.},
  pages = {1950003},
}

@InProceedings{rahman_aspect_2018,
  address = {Kitakyushu, Japan},
  title = {Aspect {Extraction} from {Bangla} {Reviews} using {Convolutional} {Neural} {Network}},
  isbn = {978-1-5386-5163-6},
  url = {https://ieeexplore.ieee.org/document/8641050/},
  doi = {10.1109/ICIEV.2018.8641050},
  urldate = {2020-09-07},
  booktitle = {2018 {Joint} 7th {International} {Conference} on {Informatics}, {Electronics} \& {Vision} ({ICIEV}) and 2018 2nd {International} {Conference} on {Imaging}, {Vision} \& {Pattern} {Recognition} ({icIVPR})},
  publisher = {IEEE},
  author = {Md. Atikur Rahman and Emon {Kumar Dey}},
  month = {jun},
  year = {2018},
  abstract = {The extensive customer reviews in web assist customers for purchase-decision-making as well as providers for business planning. Summarization of reviews is desirable as reading all reviews is not feasible to evaluate properly. To find aspect categories from reviews is a sub-task of summarization known as Aspect Based Sentiment Analysis (ABSA). In this paper, we present two Bangla datasets to perform ABSA task. We collected user comments on cricket game and annotated manually. The other dataset consists of consumer reviews of restaurants. A model to extract aspect category based on Convo-lutional Neural Network (CNN) is presented. The model shows the convincing performance of the proposed datasets compared to the conventional classifiers.},
  pages = {262--267},
}

@InProceedings{chen_customer_2018,
  address = {Seattle, WA, USA},
  title = {Customer {Lifetime} {Value} in {Video} {Games} {Using} {Deep} {Learning} and {Parametric} {Models}},
  isbn = {978-1-5386-5035-6},
  url = {https://ieeexplore.ieee.org/document/8622151/},
  doi = {10.1109/BigData.2018.8622151},
  urldate = {2020-09-07},
  booktitle = {2018 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
  publisher = {IEEE},
  author = {Pei Pei Chen and Anna Guitart and Ana Fernandez {del Rio} and Africa Perianez},
  month = {dec},
  year = {2018},
  pages = {2134--2140},
  abstract = {Nowadays, video game developers record every virtual action performed by their players. As each player can remain in the game for years, this results in an exceptionally rich dataset that can be used to understand and predict player behavior. In particular, this information may serve to identify the most valuable players and foresee the amount of money they will spend in in-app purchases during their lifetime. This is crucial in free-to-play games, where up to 50% of the revenue is generated by just around 2% of the players, the so-called whales.To address this challenge, we explore how deep neural networks can be used to predict customer lifetime value in video games, and compare their performance to parametric models such as Pareto/NBD. Our results suggest that convolutional neural network structures are the most efficient in predicting the economic value of individual players. They not only perform better in terms of accuracy, but also scale to big data and significantly reduce computational time, as they can work directly with raw sequential data and thus do not require any feature engineering process. This becomes important when datasets are very large, as is often the case with video game logs.Moreover, convolutional neural networks are particularly well suited to identify potential whales. Such an early identification is of paramount importance for business purposes, as it would allow developers to implement in-game actions aimed at retaining big spenders and maximizing their lifetime, which would ultimately translate into increased revenue.},
  file = {Chen et al_2018_Customer Lifetime Value in Video Games Using Deep Learning and Parametric Models.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\JISKVVWW\\Chen et al_2018_Customer Lifetime Value in Video Games Using Deep Learning and Parametric Models.pdf:application/pdf},
}

@InProceedings{di_adaptive_2018,
  address = {Miyazaki, Japan},
  title = {An {Adaptive} {Pre}-clustering {Support} {Vector} {Machine} for {Binary} {Imbalanced} {Classification}},
  isbn = {978-1-5386-6650-0},
  url = {https://ieeexplore.ieee.org/document/8616120/},
  doi = {10.1109/SMC.2018.00124},
  urldate = {2020-09-07},
  booktitle = {2018 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({SMC})},
  publisher = {IEEE},
  author = {Zonglin Di and Siya Yao and Qi Kang and Mengchu Zhou},
  month = {oct},
  year = {2018},
  abstract = {Imbalance classification is a common but critical problem in machine learning and artificial intelligence. Derived from structural risk minimization, a Support Vector Machine (SVM) enjoys great reputation in classification. However, the original SVM is not suitable for the imbalance classification and the existing modifications of SVM for this kind of problems fail to take the distribution of datasets into full consideration, thereby leading to some remarkable loss in their classification performance. Recently, an Adaptive Clustering by Fast Search and Find of Density Peaks (ADPclust) is proposed and performs well in finding cluster centroids in a sample space automatically and more reliably by using adaptive density peak detection and silhouette theory. Motivated by this, this work proposes an adaptive pre-clustering SVM (AP-SVM) such that the information of the original dataset distribution is well utilized to yield balanced sub-datasets for accurate and efficient classification. Specifically, AP-SVM clusters the majority into several groups given a dataset and then applies undersampling on every cluster to re-balance the dataset to be used in the SVM classification step. After experiments on 10 binary public datasets and evaluation using Area Under Curve (AUC), F-Measure, G-Mean, we well show the superiority of the proposed method over SVM, Synthetic Minority Over-sampling Technique algorithm (SMOTE), Undersampling-SVM (U-SVM), K-Means, Fuzzy C Means and EasyEnsemble.},
  pages = {681--686},
}

@Article{deng_sampling_2019,
  title = {Sampling method based on improved {C4}.5 decision tree and its application in prediction of telecom customer churn},
  volume = {18},
  issn = {1461-4111, 1741-5179},
  url = {http://www.inderscience.com/link.php?id=97887},
  doi = {10.1504/IJITM.2019.097887},
  language = {en},
  number = {1},
  urldate = {2020-09-07},
  journal = {International Journal of Information Technology and Management},
  author = {Weibin Deng and Linsen Deng and Jin Liu and Jie Qi},
  year = {2019},
  abstract = {Nowadays, customer churn prediction is quite important for telecom operators to reduce churn rate and remain competitive. However, the imbalance between the retained   customers and the churners affects the prediction accuracy. For solving this problem, a new sampling method based on improved C4.5 decision tree is proposed. Firstly, an initial weight is set for each sample according to the data scale of each class. Then, the samples’ weight is adjusted through several rounds of alternative training by the improved C4.5 decision tree algorithm. Both the gain ratio and the misclassification cost are considered for splitting criterion. Besides, the boundary minority examples and the centre majority examples are found according to their weights. Furthermore, over-sampling is conducted for the boundary minority examples by synthetic minority over-sampling technique (SMOTE) and under-sampling is executed for the majority examples. Experiments on UCI public data and telecom operator data show the efficiency of the new method.},
  pages = {93},
}

@Article{li_efficient_2019,
  title = {An efficient noise-filtered ensemble model for customer churn analysis in aviation industry},
  volume = {37},
  issn = {10641246, 18758967},
  url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/JIFS-182807},
  doi = {10.3233/JIFS-182807},
  number = {2},
  urldate = {2020-09-07},
  journal = {Journal of Intelligent \& Fuzzy Systems},
  author = {Yongjun Li and Jianshuang Wei and Kai Kang and Zhouyang Wu},
  month = {sep},
  year = {2019},
  abstract = {Aviation customer churn analysis is a difficult point, which has puzzled over airlines. The difficulties lie in the imbalance of customer churn data distribution and noisy data interference. Although some existing sampling techniques and ensemble models are good at dealing with class imbalance problem, noisy examples in dataset seriously affects the sampling quality and predictive accuracy of classifiers. Therefore, the purpose of our work is to effectively solve the problem of noise interference in imbalanced data classification and improve the effect of the ensemble classifier. In this paper, we propose a novel noise filtering algorithm that combined Tomek-link with distance weighted KNN (TWK), which can effectively filter the noise from both minority and majority class in the imbalanced dataset and prevent relative value samples from being rejected by mistake. We integrate TWK and feature sampling into EasyEnsemble to get a new ensemble model, named FSEE-TWK for short, for customer churn analysis. The introduction of feature sampling to FSEE-TWK accelerate the process of training and avoid model over-fitting. We obtained imbalanced customer data from a major Chinese airline to predict potential churn customers. We use F-Measure and G-Mean to evaluate the performance of the new ensemble model. The experimental results show that the proposed model can effectively improve the classification of datasets and significantly reduce the training time of the model.},
  pages = {2575--2585},
}

@Article{qian_multi-class_2019,
  title = {Multi-{Class} {Learning} from {Label} {Proportions} for {Bank} {Customer} {Classification}},
  volume = {162},
  issn = {18770509},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050919320162},
  doi = {10.1016/j.procs.2019.12.006},
  language = {en},
  urldate = {2020-09-07},
  journal = {Procedia Computer Science},
  author = {Yaxing Qian and Qiang Tong and Bo Wang},
  year = {2019},
  abstract = {In this paper, we study multi-class learning from label proportions, and apply it to bank customer classification, in order to provide advice for banks to better manage customer relationships. We attempt to apply the multi-class extreme learning machine (ELM) to learning from label proportions (LLP). With a structure similar to neural network, ELM possesses higher computational speed and better generalization ability. As a result, it is suitable to deal with large-scale and multi-class problems. LLP is a learning problem, which classifies training data into bags where only the label proportions of each class in each bag is available. Furthermore, in order to maintain the stable model accuracy when the bag sizes increasing, we manage to add small number of labeled samples in our model, called LLP-ELM with in a semi-supervised learning framework. The experiments prove that our improvement has advantages in the case of large bag sizes. In practical, it is worth to consider to apply the proposed algorithm to multi-class learning from label proportions for bank customer classification and other multi-class scenarios.},
  pages = {421--428},
}

@InCollection{naldi_making_2019,
  address = {Cham},
  title = {Making {Machine} {Learning} {Forget}},
  volume = {11498},
  isbn = {978-3-030-21751-8 978-3-030-21752-5},
  url = {http://link.springer.com/10.1007/978-3-030-21752-5_6},
  language = {en},
  urldate = {2020-09-07},
  booktitle = {Privacy {Technologies} and {Policy}},
  publisher = {Springer International Publishing},
  author = {Saurabh Shintre and Kevin A. Roundy and Jasjeet Dhaliwal},
  editor = {Maurizio Naldi and Giuseppe F. Italiano and Kai Rannenberg and Manel Medina and Athena Bourka},
  year = {2019},
  doi = {10.1007/978-3-030-21752-5_6},
  note = {Series Title: Lecture Notes in Computer Science},
  abstract = {Machine learning models often overfit to the training data and do not learn general patterns like humans do. This allows an attacker to learn private membership or attributes about the training data, simply by having access to the machine learning model. We argue that this vulnerability of current machine learning models makes them indirect stores of the personal data used for training and therefore, corresponding data protection regulations must apply to machine learning models as well. In this position paper, we specifically analyze how the “right-to-be-forgotten” provided by the European Union General Data Protection Regulation can be implemented on current machine learning models and which techniques can be used to build future models that can forget. This document also serves as a call-to-action for researchers and policy-makers to identify other technologies that can be used for this purpose.},
  pages = {72--83},
}

@Article{tang_spammer_2019,
  title = {A {Spammer} {Identification} {Method} for {Class} {Imbalanced} {Weibo} {Datasets}},
  volume = {7},
  issn = {2169-3536},
  url = {https://ieeexplore.ieee.org/document/8662765/},
  doi = {10.1109/ACCESS.2019.2901756},
  urldate = {2020-09-07},
  journal = {IEEE Access},
  author = {Wenbing Tang and Zuohua Ding and Mengchu Zhou},
  year = {2019},
  pages = {29193--29201},
  abstract = {Nowadays, Weibo has become a significant and popular information sharing platform in China. Meanwhile, spammer identification has been a big challenge for it. To mitigate the damage caused by spammers, classification algorithms from machine learning have been applied to distinguish spammers and non-spammers. However, most of the previous studies overlook the class imbalance problem of real-world data. In this paper, by analyzing the characteristics of spammers in Weibo, we select microblog content similarity, the average number of links, and the other 12 features to construct a comprehensive feature vector never seen before. Considering the existence of imbalance problems in spammer identification, an ensemble learning method is used to combine multiple base classifiers for improving the learning performance. During the training stage of base learners, fuzzy-logic-based oversampling and cost-sensitive support vector machine are considered to tackle imbalanced data at both data and algorithmic levels. The experimental results demonstrate that compared with the existing state-of-the-art methods, the recall rate of our proposed approach increases by 6.5% and reaches the precision value of 87.53% when used to deal with real-world Weibo datasets we collected.},
  file = {Tang et al_2019_A Spammer Identification Method for Class Imbalanced Weibo Datasets.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\JQR65ZK6\\Tang et al_2019_A Spammer Identification Method for Class Imbalanced Weibo Datasets.pdf:application/pdf},
}

@InCollection{berlingerio_variational_2019,
  address = {Cham},
  title = {Variational {Bayes} for {Mixture} {Models} with {Censored} {Data}},
  volume = {11052},
  isbn = {978-3-030-10927-1 978-3-030-10928-8},
  url = {http://link.springer.com/10.1007/978-3-030-10928-8_36},
  language = {en},
  urldate = {2020-09-07},
  booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
  publisher = {Springer International Publishing},
  author = {Masahiro Kohjima and Tatsushi Matsubayashi and Hiroyuki Toda},
  editor = {Michele Berlingerio and Francesco Bonchi and Thomas Gärtner and Neil Hurley and Georgiana Ifrim},
  year = {2019},
  doi = {10.1007/978-3-030-10928-8_36},
  note = {Series Title: Lecture Notes in Computer Science},
  abstract = {In this paper, we propose a variational Bayesian algorithm for mixture models that can deal with censored data, which is the data under the situation that the exact value is known only when the value is within a certain range and otherwise only partial information is available. The proposed algorithm can be applied to any mixture model whose component distribution belongs to exponential family; it is a natural generalization of the variational Bayes that deals with “standard” samples whose values are known. We confirm the effectiveness of the proposed algorithm by experiments on synthetic and real world data.},
  pages = {605--620},
}

@Article{lai_robust_2019,
  title = {A robust correlation analysis framework for imbalanced and dichotomous data with uncertainty},
  volume = {470},
  issn = {00200255},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0020025518306224},
  doi = {10.1016/j.ins.2018.08.017},
  language = {en},
  urldate = {2020-09-07},
  journal = {Information Sciences},
  author = {Chun Sing Lai and Yingshan Tao and Fangyuan Xu and Wing W.Y. Ng and Youwei Jia and Haoliang Yuan and Chao Huang and Loi Lei Lai and Zhao Xu and Giorgio Locatelli},
  month = {jan},
  year = {2019},
  pages = {58--77},
  abstract = {Correlation analysis is one of the fundamental mathematical tools for identifying dependence between classes. However, the accuracy of the analysis could be jeopardized due to variance error in the data set. This paper provides a mathematical analysis of the impact of imbalanced data concerning Pearson Product Moment Correlation (PPMC) analysis. To alleviate this issue, the novel framework Robust Correlation Analysis Framework (RCAF) is proposed to improve the correlation analysis accuracy. A review of the issues due to imbalanced data and data uncertainty in machine learning is given. The proposed framework is tested with in-depth analysis of real-life solar irradiance and weather condition data from Johannesburg, South Africa. Additionally, comparisons of correlation analysis with prominent sampling techniques, i.e., Synthetic Minority Over-Sampling Technique (SMOTE) and Adaptive Synthetic (ADASYN) sampling techniques are conducted. Finally, K-Means and Wards Agglomerative hierarchical clustering are performed to study the correlation results. Compared to the traditional PPMC, RCAF can reduce the standard deviation of the correlation coefficient under imbalanced data in the range of 32.5%–93.02%.},
  file = {Lai et al_2019_A robust correlation analysis framework for imbalanced and dichotomous data.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\GGFVVRNU\\Lai et al_2019_A robust correlation analysis framework for imbalanced and dichotomous data.pdf:application/pdf},
}

@Article{vinayakumar_deep_2019,
  title = {Deep {Learning} {Approach} for {Intelligent} {Intrusion} {Detection} {System}},
  volume = {7},
  issn = {2169-3536},
  url = {https://ieeexplore.ieee.org/document/8681044/},
  doi = {10.1109/ACCESS.2019.2895334},
  urldate = {2020-09-07},
  journal = {IEEE Access},
  author = {R. Vinayakumar and Mamoun Alazab and K. P. Soman and Prabaharan Poornachandran and Ameer Al-Nemrat and Sitalakshmi Venkatraman},
  year = {2019},
  pages = {41525--41550},
  abstract = {Machine learning techniques are being widely used to develop an intrusion detection system (IDS) for detecting and classifying cyberattacks at the network-level and the host-level in a timely and automatic manner. However, many challenges arise since malicious attacks are continually changing and are occurring in very large volumes requiring a scalable solution. There are different malware datasets available publicly for further research by cyber security community. However, no existing study has shown the detailed analysis of the performance of various machine learning algorithms on various publicly available datasets. Due to the dynamic nature of malware with continuously changing attacking methods, the malware datasets available publicly are to be updated systematically and benchmarked. In this paper, a deep neural network (DNN), a type of deep learning model, is explored to develop a flexible and effective IDS to detect and classify unforeseen and unpredictable cyberattacks. The continuous change in network behavior and rapid evolution of attacks makes it necessary to evaluate various datasets which are generated over the years through static and dynamic approaches. This type of study facilitates to identify the best algorithm which can effectively work in detecting future cyberattacks. A comprehensive evaluation of experiments of DNNs and other classical machine learning classifiers are shown on various publicly available benchmark malware datasets. The optimal network parameters and network topologies for DNNs are chosen through the following hyperparameter selection methods with KDDCup 99 dataset. All the experiments of DNNs are run till 1,000 epochs with the learning rate varying in the range [0.01-0.5]. The DNN model which performed well on KDDCup 99 is applied on other datasets, such as NSL-KDD, UNSW-NB15, Kyoto, WSN-DS, and CICIDS 2017, to conduct the benchmark. Our DNN model learns the abstract and high-dimensional feature representation of the IDS data ...},
  file = {Vinayakumar et al_2019_Deep Learning Approach for Intelligent Intrusion Detection System.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\6QAG34JS\\Vinayakumar et al_2019_Deep Learning Approach for Intelligent Intrusion Detection System.pdf:application/pdf},
}

@Article{sohrabi_efficient_2019,
  title = {An efficient preprocessing method for supervised sentiment analysis by converting sentences to numerical vectors: a twitter case study},
  volume = {78},
  issn = {1380-7501, 1573-7721},
  shorttitle = {An efficient preprocessing method for supervised sentiment analysis by converting sentences to numerical vectors},
  url = {http://link.springer.com/10.1007/s11042-019-7586-4},
  doi = {10.1007/s11042-019-7586-4},
  language = {en},
  number = {17},
  urldate = {2020-09-07},
  journal = {Multimedia Tools and Applications},
  author = {Mohammad Karim Sohrabi and Fatemeh Hemmatian},
  month = {sep},
  year = {2019},
  abstract = {Along with the significant growth of social media, individuals and companies are increasingly receiving public opinions which direct their decisions. Opinion mining, which is considered as a sub-field of natural language processing, information retrieval, and text mining, is the process of understanding the users’ views from their comment, which have been represented as unstructured texts. Emergence of online social media has led to the production of a huge amount of user comments on websites, and thus, has raised opinion mining as a very useful and challenging problem. In this paper, an efficient preprocessing method for opinion mining is presented and will be used for analyzing users’ comments on Twitter social network. For this purpose, different text preprocessing techniques have been used on the dataset to achieve an acceptable standard text. Word2vec method which is a fast and accurate method have been also exploited to convert the words’ arrays to numerical vectors. Machine learning methods, with supervised learning approach, have been applied on the obtained data after this fast and accurate preprocessing phase. Python and RapidMiner have been used to implement different opinion mining methods and the results of these implementations have been compared and evaluated. The experimental results show that the combined use of the preprocessing method of this paper and support vector machine and artificial neural network have the highest accuracy compared to other methods.},
  pages = {24863--24882},
}

@Article{yang_vibration_2019,
  title = {Vibration {Test} of {Single} {Coal} {Gangue} {Particle} {Directly} {Impacting} the {Metal} {Plate} and the {Study} of {Coal} {Gangue} {Recognition} {Based} on {Vibration} {Signal} and {Stacking} {Integration}},
  volume = {7},
  issn = {2169-3536},
  url = {https://ieeexplore.ieee.org/document/8782521/},
  doi = {10.1109/ACCESS.2019.2932118},
  urldate = {2020-09-07},
  journal = {IEEE Access},
  author = {Yang Yang and Qingliang Zeng and Guangjun Yin and Lirong Wan},
  year = {2019},
  pages = {106784--106805},
  abstract = {n order to realize the recognition of coal gangue in the top coal caving process, a scheme of the coal gangue recognition based on the collision vibration signal between coal gangue and the metal plate is proposed in this paper, a systematic and standardized impacting test between coal gangue particles and the metal plate is designed for the first time, the vibration signal standardized processing method by the signal intercepting and the coal gangue impact vibration signal recognition algorithm by stacking integration are innovatively proposed. First, a single particle impact on the metal plate test-bed was designed and constructed. Then 1,000 groups coal and 1,000 groups gangue impact on the metal plate tests were carried out respectively, and the vibration acceleration signals of the metal plate were collected. After that, through the signal intercepting, calculating the time-domain characteristics and HHT processing of the vibration signal, 10 time-frequency characteristics, such as the variance of the intercepted signal and the Hilbert marginal spectrum energy value, are determined to form the feature vector. Finally, based on the two different type of the signal samples, the intercepted signal feature vector, and the original intercepted signal, coal gangue recognition by the seven machine learning algorithms, including the decision tree (DT), random forest (RF), XGBoost, long short-term memory (LSTM), support vector machine (SVM), factorization machine (FM), and stacking integration is carried out respectively, and the basis for selecting recognition schemes is discussed. The results show that the coal gangue recognition rate with the same recognition algorithm by using the intercepted signal samples is higher than that of the feature vector samples, the Staking integration algorithm based on the same sample has the highest recognition rate, and the Staking integration algorithm based on the feature vector has the most significant comprehensive advantage in ...},
  file = {Yang et al_2019_Vibration Test of Single Coal Gangue Particle Directly Impacting the Metal.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\U3JGJ9PZ\\Yang et al_2019_Vibration Test of Single Coal Gangue Particle Directly Impacting the Metal.pdf:application/pdf},
}

@Article{kaya_behavioral_2018,
  title = {Behavioral attributes and financial churn prediction},
  volume = {7},
  issn = {2193-1127},
  url = {https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-018-0165-5},
  doi = {10.1140/epjds/s13688-018-0165-5},
  language = {en},
  number = {1},
  urldate = {2020-09-07},
  journal = {EPJ Data Science},
  author = {Erdem Kaya and Xiaowen Dong and Yoshihiko Suhara and Selim Balcisoy and Burcin Bozkaya and Alex “Sandy” Pentland},
  month = {dec},
  year = {2018},
  pages = {41},
  abstract = {Customer retention is crucial in a variety of businesses as acquiring new customers is often more costly than keeping the current ones. As a consequence, churn prediction has attracted great attention from both the business and academic worlds. Traditional efforts in the financial domain mainly focus on domain specific variables such as product ownership or service usage aggregation, however, without considering dynamic behavioral patterns of customers’ financial transactions. In this paper, we attempt to fill in this gap by investigating the spatio-temporal patterns and entropy of choices underlying the customers’ financial decisions, and their relations to customer churning activities. Inspired by previous works in the emerging field of computational social science, we built a prediction model based on spatio-temporal and choice behavioral traits using individual transaction records. Our results show that proposed dynamic behavioral models could predict churn decisions significantly better than traditionally considered factors such as demographic-based features, and that this effect remains consistent across multiple data sets and various churn definitions. We further study the relative importance of the various behavioral features in churn prediction, and how the predictive power varies across different demographic groups. More generally, the proposed features can also be applied to churn prediction in other domains where spatio-temporal behavioral data are available.},
  file = {Kaya et al_2018_Behavioral attributes and financial churn prediction.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\DC95HA4T\\Kaya et al_2018_Behavioral attributes and financial churn prediction.pdf:application/pdf},
}

@Article{karumur_personality_2018,
  title = {Personality, {User} {Preferences} and {Behavior} in {Recommender} systems},
  volume = {20},
  issn = {1387-3326, 1572-9419},
  url = {http://link.springer.com/10.1007/s10796-017-9800-0},
  doi = {10.1007/s10796-017-9800-0},
  language = {en},
  number = {6},
  urldate = {2020-09-07},
  journal = {Information Systems Frontiers},
  author = {Raghav Pavan Karumur and Tien T. Nguyen and Joseph A. Konstan},
  month = {dec},
  year = {2018},
  abstract = {This paper reports on a study of 1840 users of the MovieLens recommender system with identified Big-5 personality types. Based on prior literature that suggests that personality type is a stable predictor of user preferences and behavior, we examine factors of user retention and engagement, content preferences, and rating patterns to identify recommender-system related behaviors and preferences that correlate with user personality. We find that personality traits correlate significantly with behaviors and preferences such as newcomer retention, intensity of engagement, activity types, item categories, consumption versus contribution, and rating patterns.},
  pages = {1241--1265},
}

@InProceedings{agrawal_customer_2018,
  address = {Shah Alam},
  title = {Customer {Churn} {Prediction} {Modelling} {Based} on {Behavioural} {Patterns} {Analysis} using {Deep} {Learning}},
  isbn = {978-1-5386-4836-0 978-1-5386-4838-4},
  url = {https://ieeexplore.ieee.org/document/8538420/},
  doi = {10.1109/ICSCEE.2018.8538420},
  urldate = {2020-09-07},
  booktitle = {2018 {International} {Conference} on {Smart} {Computing} and {Electronic} {Enterprise} ({ICSCEE})},
  publisher = {IEEE},
  author = {Sanket Agrawal and Aditya Das and Amit Gaikwad and Sudhir Dhage},
  month = {jul},
  year = {2018},
  pages = {1--6},
}

@InProceedings{kuznietsova_forecasting_2018,
  address = {Kyiv},
  title = {Forecasting of {Financial} {Risk} {Users}' {Outflow}},
  isbn = {978-1-5386-7196-2},
  url = {https://ieeexplore.ieee.org/document/8516782/},
  doi = {10.1109/SAIC.2018.8516782},
  urldate = {2020-09-07},
  booktitle = {2018 {IEEE} {First} {International} {Conference} on {System} {Analysis} \& {Intelligent} {Computing} ({SAIC})},
  publisher = {IEEE},
  author = {Nataliia Kuznietsova and Petro Bidyuk},
  month = {oct},
  year = {2018},
  abstract = {The study is devoted to building new models for forecasting the users' outflow on online platform games which are based on their behavior and strategy in previous games that is used as history statistics. Such models as logistic regression, decision trees, gradient boosting, etc. for online gambling platform were used for calculating the probability of the client's outflow. High quality performance of the models was received for model based on gradient boosting. For evaluating the models the following parameters were used: common accuracy, first and second-types errors, and area under the receiver operation curve. The most effective criterion which combines the possibility to receive high accuracy, sensitivity to real situation and minimize the specificity of the model was the statistic: area under the curve (AUC). It turned out that the Xgboost method makes it much easier to model nonlinear relationships between regressors and the target field, which leads to better and more accurate classification and evaluation of the probability of a client outflow.},
  pages = {1--6},
}

@Article{duan_afsnn_2018,
  title = {{AFSNN}: {A} {Classification} {Algorithm} {Using} {Axiomatic} {Fuzzy} {Sets} and {Neural} {Networks}},
  volume = {26},
  issn = {1063-6706, 1941-0034},
  shorttitle = {{AFSNN}},
  url = {https://ieeexplore.ieee.org/document/8244313/},
  doi = {10.1109/TFUZZ.2017.2788875},
  number = {5},
  urldate = {2020-09-07},
  journal = {IEEE Transactions on Fuzzy Systems},
  author = {Xiaodong Duan and Yuangang Wang and Witold Pedrycz and Xiaodong Liu and Cunrui Wang and Zedong Li},
  month = {oct},
  year = {2018},
  abstract = {In this study, we present a comprehensible classifier AFSNN that embeds a new type of coherence membership function, which builds upon the theoretical findings of the axiomatic fuzzy set (AFS) theory into the hidden layer of neural network with random weights (NNRWs). Borrowing from the idea of NNRWs that employs the random initialization technique, the relation among attributes, simple concepts, and complex concepts are randomly determined. Complex concepts are generated through the combination of randomly selected simple concepts by AFS logic operation. The output weights of NNRWs are utilized to evaluate the confidence of each complex concept for every target class, which means that the feasibility of complex concepts for every class is determined analytically rather than through the tuning parameters of constraint conditions such as in conventional AFS-based classifiers. For the proposed method, compared to other neural-network-based classification methods, the fuzzy descriptions generated from complex concepts in hidden layer make classification result human understandable. We have experimented with several benchmark datasets and compared the results with other neural network-based classifiers. We show that our method outperforms Ensemble, EvRBFN, NNEP, LVQ, and iRProp+ in the seven out of ten datasets. The results show that the performance of AFSNN is competitive in terms of classification accuracy and the network shows a distinctive capability of providing explicit knowledge in the form of linguistic description.},
  pages = {3151--3163},
}

@Article{kappe_random_2018,
  title = {A random coefficients mixture hidden {Markov} model for marketing research},
  volume = {35},
  issn = {01678116},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167811618300314},
  doi = {10.1016/j.ijresmar.2018.07.002},
  language = {en},
  number = {3},
  urldate = {2020-09-07},
  journal = {International Journal of Research in Marketing},
  author = {Eelco Kappe and Ashley {Stadler Blank} and Wayne S. DeSarbo},
  month = {sep},
  year = {2018},
  abstract = {The hidden Markov model (HMM) provides a framework to model the time-varying effects of marketing mix variables. When employed in a panel data context, it is important to properly account for unobserved heterogeneity across individuals. We propose a new random coefficients mixture HMM (RCMHMM) that allows for flexible patterns of unobserved heterogeneity in both the state-dependent and transition parameters. The RCMHMM nests all HMMs found in the marketing literature. Results of two simulation studies demonstrate that 1) averaging across a large number of different data generating processes, the RCMHMM outperforms all its nested versions using both in-sample and out-of-sample performance and 2) the RCMHMM is more robust than its nested versions when underlying model assumptions are violated. In addition, we apply the RCMHMM to an empirical application where we examine the effectiveness of in-game promotions in increasing the short-term demand for Major League Baseball (MLB) attendance. We find that the effectiveness of four promotional categories varies over the course of the season and across teams and that the RCMHMM performs best.},
  pages = {415--431},
}

@Article{lismont_predicting_2018,
  title = {Predicting interpurchase time in a retail environment using customer-product networks: {An} empirical study and evaluation},
  volume = {104},
  issn = {09574174},
  shorttitle = {Predicting interpurchase time in a retail environment using customer-product networks},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417418301568},
  doi = {10.1016/j.eswa.2018.03.016},
  language = {en},
  urldate = {2020-09-07},
  journal = {Expert Systems with Applications},
  author = {Jasmien Lismont and Sudha Ram and Jan Vanthienen and Wilfried Lemahieu and Bart Baesens},
  month = {aug},
  year = {2018},
  pages = {22--32},
  abstract = {In predictive analytics and statistics, entities are frequently treated as individual actors. However, in reality this assumption is not valid. In the context of retail, similar customers will behave and thus also purchase similarly to each other. By combining their behavior in an intelligent way, based on transaction history, we can leverage these connections and improve our ability to predict purchase outcomes. As such, we can create customer-product networks from which we can deduce information on customers expressing similar purchasing behavior. This allows us to exploit their preferences and predict which products are going to be sold significantly less often. We want to use this information mainly for gaining novel marketing insights on products. For example, if customers refrain from buying products this might be due to contextual reasons such as new complements or supplements, or new nearby shops. By using these networks on data from an offline European retail corporation, we are able to boost performance of the predictive models by 6% and the identification of these specific products by 20%. This indicates that the development of customer-product graphs in retail can lead to improved marketing intelligence. To our knowledge, this is one of the first studies to use customer-product networks for predictive modeling in an offline retail setting. Furthermore, we suggest an extensive set of product and network features which can guide future researchers and practitioners in their model development.},
  file = {Lismont et al_2018_Predicting interpurchase time in a retail environment using customer-product.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\H3ZGH6CQ\\Lismont et al_2018_Predicting interpurchase time in a retail environment using customer-product.pdf:application/pdf},
}

@Article{rachid_clustering_2018,
  title = {Clustering {Prediction} {Techniques} in {Defining} and {Predicting} {Customers} {Defection}: {The} {Case} of {E}-{Commerce} {Context}},
  volume = {8},
  issn = {2088-8708, 2088-8708},
  shorttitle = {Clustering {Prediction} {Techniques} in {Defining} and {Predicting} {Customers} {Defection}},
  url = {http://ijece.iaescore.com/index.php/IJECE/article/view/8531},
  doi = {10.11591/ijece.v8i4.pp2367-2383},
  abstract = {With the growth of the e-commerce sector, customers have more choices, a fact which encourages them to divide their purchases amongst several e-commerce sites and compare their competitors’ products, yet this increases high risks of churning. A review of the literature on customer churning models reveals that no prior research had considered both partial and total defection in non-contractual online environments. Instead, they focused either on a total or partial defect. This study proposes a customer churn prediction model in an e-commerce context, wherein a clustering phase is based on the integration of the k-means method and the Length-Recency-Frequency-Monetary (LRFM) model. This phase is employed to define churn followed by a multi-class prediction phase based on three classification techniques: Simple decision tree, Artificial neural networks and Decision tree ensemble, in which the dependent variable classifies a particular customer into a customer continuing loyal buying patterns (Non-churned), a partial defector (Partially-churned), and a total defector (Totally-churned). Macro-averaging measures including average accuracy, macro-average of Precision, Recall, and F-1 are used to evaluate classifiers’ performance on 10-fold cross validation. Using real data from an online store, the results show the efficiency of decision tree ensemble model over the other models in identifying both future partial and total defection.},
  number = {4},
  urldate = {2020-09-07},
  journal = {International Journal of Electrical and Computer Engineering (IJECE)},
  author = {Ait Daoud Rachid and Amine Abdellah and Bouikhalene Belaid and Lbibb Rachid},
  month = {aug},
  year = {2018},
  pages = {2367},
}

@Article{srinivasan_multi_2018,
  title = {Multi {Criteria} {Decision} {Making} in {Financial} {Risk} {Management} with a {Multi}-objective {Genetic} {Algorithm}},
  volume = {52},
  issn = {0927-7099, 1572-9974},
  url = {http://link.springer.com/10.1007/s10614-017-9683-7},
  doi = {10.1007/s10614-017-9683-7},
  language = {en},
  number = {2},
  urldate = {2020-09-07},
  journal = {Computational Economics},
  author = {Sujatha Srinivasan and T. Kamalakannan},
  month = {aug},
  year = {2018},
  abstract = {A huge amount of data is being collected and stored by financial institutions like banks during their operations. These data contain the most important facts about the institutions and its customers. A good and efficient data analytics system can find patterns in this huge data source that can be used in actionable knowledge creation. Actionable knowledge is the knowledge that can be put to decision making and take some positive action towards better performance of organizations. This actionable knowledge is termed Business Intelligence by data scientists. Business Intelligence and Analytics is the process of applying data mining techniques to organizational or corporate data to discover patterns. Business Intelligence and Business Analytics are emerging as important and essential fields both for data scientists and organizations. Risk analysis, fraud detection, customer retention, customer satisfaction analysis and actuarial analysis are some of the areas of application of business intelligence and analytics. Credit risk analysis is an important part of a successful financial institution particularly in the banking sector. The current study takes this risk analysis in financial institutions and reviews the state of the art in using data analytics or data mining techniques for financial risk analysis. The analysis of risk from financial data depends on several factors that are both objective and subjective. Hence it is a multi-criteria decision problem. The study also proposes a multi-objective genetic algorithm (MOGA) for analyzing financial data for risk analysis and prediction. The proposed MOGA is different from other evolutionary systems in that a memory component to hold the rules is added to the system while other systems in the literature are memory less. The algorithm is applied to bench mark data sets for predicting the decision on credit card and credit applications. The preliminary results are encouraging and show light towards better decision making in reducing risks.},
  pages = {443--457},
}

@Article{li_novel_2018,
  title = {A novel knowledge-leverage-based transfer learning algorithm},
  volume = {48},
  issn = {0924-669X, 1573-7497},
  url = {http://link.springer.com/10.1007/s10489-017-1084-z},
  doi = {10.1007/s10489-017-1084-z},
  language = {en},
  number = {8},
  urldate = {2020-09-07},
  journal = {Applied Intelligence},
  author = {Meiling Li and Qun Dai},
  month = {aug},
  year = {2018},
  abstract = {A major assumption in traditional machine leaning is that the training and testing data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. In recent years, transfer learning has emerged as a new learning paradigm to cope with this considerable challenge. It focuses on exploiting previously learnt knowledge by leveraging information from an old source domain to help learning in a new target domain. In this work, we integrate the knowledge-leverage-based Transfer Learning mechanism with a Rank-based Reduce Error ensemble selection approach to fulfill the transfer learning task, called RankRE-TL. Ensemble selection is important for improving both efficiency and predictive accuracy of an ensemble system. It aims to select a proper subset of the whole ensemble, which usually outperforms the whole one. Therefore, we appropriately modify the Reduce Error (RE) pruning technique and design a new Rank-based Reduce Error ensemble selection method (RankRE) to deal with the transfer learning task. The design idea of RankRE is to find the candidate classifier which is expected to improve the classification performance of the extended subensemble the most. In the RankRE-TL algorithm, the initial Support Vector Machine (SVM) ensemble is learnt based upon dynamic training dataset regrouping. And simultaneously, a new construction method of validation set is designed for RankRE-TL, which differs from the method used in conventional ensemble selection paradigm.},
  pages = {2355--2372},
}

@InProceedings{yuan_multiple_2018,
  address = {Shenyang},
  title = {Multiple feature inputs support vector regression},
  isbn = {978-1-5386-1244-6},
  url = {https://ieeexplore.ieee.org/document/8407338/},
  doi = {10.1109/CCDC.2018.8407338},
  urldate = {2020-09-07},
  booktitle = {2018 {Chinese} {Control} {And} {Decision} {Conference} ({CCDC})},
  publisher = {IEEE},
  author = {Conggui Yuan and Shuqiong Xu},
  month = {jun},
  year = {2018},
  abstract = {While traditional support vector regression algorithm is based on a single kernel, in practice it is often desirable to use multiple kernels. Learning linear combinations of multiple kernels is an appealing strategy when the right choice of feature is unknown. A new support vector regression framework for multiple feature inputs learning problem is proposed here, which mapped the different feature inputs with different nonlinear functions into high dimensional feature space, and resolved as a semi-infinite linear program. Experimental results show that the proposed algorithm works well, and helps to improve the accuracy and the generalization for regression problems with multiple feature inputs.},
  pages = {1352--1357},
}

@InProceedings{naik_innovative_2017,
  address = {Tumkur},
  title = {An innovative optimized model to anticipate clients about immigration in telecom industry},
  isbn = {978-1-5386-1144-9},
  url = {https://ieeexplore.ieee.org/document/8389139/},
  doi = {10.1109/ICATCCT.2017.8389139},
  urldate = {2020-09-07},
  booktitle = {2017 3rd {International} {Conference} on {Applied} and {Theoretical} {Computing} and {Communication} {Technology} ({iCATccT})},
  publisher = {IEEE},
  author = {Midde Venkateswarlu Naik and Sareddy Shiva Reddy},
  month = {dec},
  year = {2017},
  abstract = {This article proposed an innovative model to predict churning and non churning of clients in the telecom industry. Now-a-days, telecom customers are frequently migrating from one network to the other due to various constraints, policies and standards in public and private sectors. Usually in current industry the cost to retain the existing clients is smaller than getting a pioneering customer. To survive in the current competitive world, there is a need to design an optimal prediction model for churning and non churning of telecom clients. The proposed model has been outperformed with an accuracy level of 99.61% than existing models and techniques. Earlier authors have achieved 94.03 % of accuracy using machine learning techniques to predict churning of customers.},
  pages = {232--236},
}

@Article{mitrovic_operational_2018,
  title = {On the operational efficiency of different feature types for telco {Churn} prediction},
  volume = {267},
  issn = {03772217},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0377221717311177},
  doi = {10.1016/j.ejor.2017.12.015},
  language = {en},
  number = {3},
  urldate = {2020-09-07},
  journal = {European Journal of Operational Research},
  author = {Sandra Mitrović and Bart Baesens and Wilfried Lemahieu and Jochen {De Weerdt}},
  month = {jun},
  year = {2018},
  pages = {1141--1155},
  abstract = {Churn prediction in telco remains a very active research topic. Due to the uptake of social network analytics and the results of previous benchmarking studies showing a rather flat maximum performance effect of predictive modeling techniques, the focus has mainly shifted to expanding and exploring the relevant feature space. While previous studies generally agree that adding features typically increases predictive performance, they rarely discuss the accompanying issues such as data availability and computational cost. In this work, we bridge the gap between predictive performance and operational efficiency by devising a new feature type classification and a novel reusable method to determine optimal feature type combinations based on Pareto multi-criteria optimization. Our results provide several insights that can serve as a guideline for industry practitioners.},
  file = {Mitrović et al_2018_On the operational efficiency of different feature types for telco Churn.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\LIUFFNIB\\Mitrović et al_2018_On the operational efficiency of different feature types for telco Churn.pdf:application/pdf},
}

@Article{obajemu_new_2018,
  title = {A {New} {Fuzzy} {Modeling} {Framework} for {Integrated} {Risk} {Prognosis} and {Therapy} of {Bladder} {Cancer} {Patients}},
  volume = {26},
  issn = {1063-6706, 1941-0034},
  url = {https://ieeexplore.ieee.org/document/8000603/},
  doi = {10.1109/TFUZZ.2017.2735939},
  number = {3},
  urldate = {2020-09-07},
  journal = {IEEE Transactions on Fuzzy Systems},
  author = {Olusayo Obajemu and Mahdi Mahfouf and James W. F. Catto},
  month = {jun},
  year = {2018},
  pages = {1565--1577},
  abstract = {This paper presents a new fuzzy modeling approach for analyzing censored survival data and finding risk groups of patients diagnosed with bladder cancer. The proposed framework involves a new procedure for integrating the frameworks of interval type-2 fuzzy logic and Cox modeling intrinsically. The output of this synergistic framework is a score/prognostics index which is indicative of the patient's level of mortality risk. A threshold value is selected whereby patients with risk scores that are greater than this threshold are classed as high-risk patients and vice versa. Unlike in the case of black-box type modeling approaches, the paper shows that interpretability and transparency are maintained using the proposed fuzzy modeling framework. Two datasets are used to test the modeling accuracy of the elicited models. The first is an artificial dataset which has similar characteristics as in a typical survival data. The second relates to real-life bladder cancer data from which one requires a model that identifies the low-risk and high-risk patients and then recommends risk management decisions based on, predicted risk level, patient history and characteristics, disease pathology, and event times. The performance of the proposed framework is compared with the traditional Cox model, logistic regression as well as a nonlinear survival data modeling technique based on neural networks. This is the first time an attempt has been made to exploit the transparency advantages of fuzzy models and the principled statistical framework of the Cox model in order to identify risk groups and recommend risk management decisions from complex survival datasets. In both the artificial data and real data, the proposed modeling framework, although minimalistic, shows better generalization performances than the previously reported models against which the results were compared.},
  file = {Obajemu et al_2018_A New Fuzzy Modeling Framework for Integrated Risk Prognosis and Therapy of.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\S8PSUL3W\\Obajemu et al_2018_A New Fuzzy Modeling Framework for Integrated Risk Prognosis and Therapy of.pdf:application/pdf},
}

@Article{ozmen_interactive_2018,
  title = {Interactive evolutionary approaches to multiobjective feature selection},
  volume = {25},
  issn = {09696016},
  url = {http://doi.wiley.com/10.1111/itor.12428},
  doi = {10.1111/itor.12428},
  language = {en},
  number = {3},
  urldate = {2020-09-07},
  journal = {International Transactions in Operational Research},
  author = {Müberra Özmen and Gülşah Karakaya and Murat Köksalan},
  month = {may},
  year = {2018},
  abstract = {In feature selection problems, the aim is to select a subset of features to characterize an output of interest. In characterizing an output, we may want to consider multiple objectives such as maximizing classification performance, minimizing number of selected features or cost, etc. We develop a preference‐based approach for multiobjective feature selection problems. Finding all Pareto‐optimal subsets may turn out to be a computationally demanding problem and we still would need to select a solution. Therefore, we develop interactive evolutionary approaches that aim to converge to a subset that is highly preferred by the decision maker (DM). We test our approaches on several instances simulating DM preferences by underlying preference functions and demonstrate that they work well.},
  pages = {1027--1052},
}

@Article{chiang_does_2018,
  title = {Does country-of-origin brand personality generate retail customer lifetime value? {A} {Big} {Data} analytics approach},
  volume = {130},
  issn = {00401625},
  shorttitle = {Does country-of-origin brand personality generate retail customer lifetime value?},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0040162517309150},
  doi = {10.1016/j.techfore.2017.06.034},
  language = {en},
  urldate = {2020-09-07},
  journal = {Technological Forecasting and Social Change},
  author = {Lan-Lung (Luke) Chiang and Chin-Sheng Yang},
  month = {may},
  year = {2018},
  abstract = {Many retail firms have witnessed the erosion of customer loyalty with the rise of e-commerce and its resulting benefits to consumers, including increased choices, lower prices, and ease of brand switching. Retailers have long collected data to learn about customer purchasing habits; however, many currently do not use data-mining analytics to increase marketing effectiveness by predicting future buying patterns and potential customer lifetime value, particularly to important segments such as loyal and potential repeat customers. Data mining can efficiently analyze large amounts of business data (“Big Data”) in an effort to forecast consumer needs and increase the lifetime value of customers (CLV). Previous studies on these topics primarily focus on conceptual assumptions and generally do not present empirically valid models. The present study sought to fill the research gap by using Big Data analytics to analyze approximately 44,000 point-of-sale transaction records for 26,000 customers of a Taiwanese retail store to understand how consumer personality traits relate to the country-of-origin (COO) traits (brand personality) of beer brands, and to predict potential customer lifetime value (CLV). The findings revealed that consumers tend to purchase and co-purchase brands with traits similar to their own personality traits (i.e., Japan—peacefulness, Belgium—openness, Ireland—excitement, etc.). Significantly, customers with the group of personality traits associated with “peacefulness” and “openness” were the most profitable customers among the five analyzed clusters (CLV value = 0.3149, 0.2635). The study provides valuable new insights into COO brand personality and consumer personality traits with co-purchase behaviors via data mining techniques, and highlights the value of extending CLV in developing useful marketing strategies.},
  pages = {177--187},
}

@Article{kitchens_advanced_2018,
  title = {Advanced {Customer} {Analytics}: {Strategic} {Value} {Through} {Integration} of {Relationship}-{Oriented} {Big} {Data}},
  volume = {35},
  issn = {0742-1222, 1557-928X},
  shorttitle = {Advanced {Customer} {Analytics}},
  url = {https://www.tandfonline.com/doi/full/10.1080/07421222.2018.1451957},
  doi = {10.1080/07421222.2018.1451957},
  language = {en},
  number = {2},
  urldate = {2020-09-07},
  journal = {Journal of Management Information Systems},
  author = {Brent Kitchens and David Dobolyi and Jingjing Li and Ahmed Abbasi},
  month = {apr},
  year = {2018},
  abstract = {As more firms adopt big data analytics to better understand their customers and differentiate their offerings from competitors, it becomes increasingly difficult to generate strategic value from isolated and unfocused ad hoc initiatives. To attain sustainable competitive advantage from big data, firms must achieve agility in combining rich data across the organization to deploy analytics that sense and respond to customers in a dynamic environment. A key challenge in achieving this agility lies in the identification, collection, and integration of data across functional silos both within and outside the organization. Because it is infeasible to systematically integrate all available data, managers need guidance in finding which data can provide valuable and actionable insights about customers. Leveraging relationship marketing theory, we develop a framework for identifying and evaluating various sources of big data in order to create a value-justified data infrastructure that enables focused and agile deployment of advanced customer analytics. Such analytics move beyond siloed transactional customer analytics approaches of the past and incorporate a variety of rich, relationship-oriented constructs to provide actionable and valuable insights. We develop a customized kernel-based learning method to take advantage of these rich constructs and instantiate the framework in a novel prototype system that accurately predicts a variety of customer behaviors in a challenging environment, demonstrating the framework’s ability to drive significant value.},
  pages = {540--574},
}

@Article{ye_novel_2017,
  title = {A {Novel} {Greedy} {Randomized} {Dynamic} {Ensemble} {Selection} {Algorithm}},
  issn = {1370-4621, 1573-773X},
  url = {http://link.springer.com/10.1007/s11063-017-9670-y},
  doi = {10.1007/s11063-017-9670-y},
  language = {en},
  urldate = {2020-09-07},
  journal = {Neural Processing Letters},
  author = {Rui Ye and Qun Dai},
  month = {jul},
  abstract = {This work proposes a novel greedy randomized dynamic ensemble selection (DES) algorithm combined with path-relinking, variable neighborhood search (VNS) and greedy randomized adaptive search procedure (GRASP) algorithms, abbreviated DyPReVNsGraspEnS. Instead of simply selecting the classifiers with the best competence under certain criterions, DyPReVNsGraspEnS makes greedy randomized selection of appropriate classifiers to form the ensemble. It realizes random multi-start search, is capable to easily escape from the local optimums, and possesses a high probability to find the optimal subensemble. It effectively strengthens the link between iterations and solves the problem of no memory by integrating the path-relinking technique. Moreover, the algorithm properly extends the neighborhood with the help of VNS, having a higher competence of global optimization. Most important of all, DyPReVNsGraspEnS is developed based upon the framework of DES paradigm, therefore, it inherits numerous advantages of DES. Empirical investigations are conducted based on twelve benchmark datasets, including six real-world credit/finance datasets and six datasets drawn from other different fields with more than two classes. To clearly articulate the performance of each strategy in the proposed algorithm, experiments for the incorporation of different strategies are implemented separately on all the datasets. The experimental results demonstrate that, in comparison with other classical ensemble learning techniques, DyPReVNsGraspEnS achieves significantly superior generalization performance. And it can not only be used for regions like credit risk assessment, but is applicable to much broader application fields as well.},
  year = {2017},
}

@Article{oskarsdottir_profit-based_2018,
  title = {Profit-{Based} {Model} {Selection} for {Customer} {Retention} {Using} {Individual} {Customer} {Lifetime} {Values}},
  volume = {6},
  issn = {2167-6461, 2167-647X},
  url = {http://www.liebertpub.com/doi/10.1089/big.2018.0015},
  doi = {10.1089/big.2018.0015},
  language = {en},
  number = {1},
  urldate = {2020-09-07},
  journal = {Big Data},
  author = {María Óskarsdóttir and Bart Baesens and Jan Vanthienen},
  month = {mar},
  year = {2018},
  pages = {53--65},
  abstract = {The goal of customer retention campaigns, by design, is to add value and enhance the operational efficiency of businesses. For organizations that strive to retain their customers in saturated, and sometimes fast moving, markets such as the telecommunication and banking industries, implementing customer churn prediction models that perform well and in accordance with the business goals is vital. The expected maximum profit (EMP) measure is tailored toward this problem by taking into account the costs and benefits of a retention campaign and estimating its worth for the organization. Unfortunately, the measure assumes fixed and equal customer lifetime value (CLV) for all customers, which has been shown to not correspond well with reality. In this article, we extend the EMP measure to take into account the variability in the lifetime values of customers, thereby basing it on individual characteristics. We demonstrate how to incorporate the heterogeneity of CLVs when CLVs are known, when their prior distribution is known, and when neither is known. By taking into account individual CLVs, our proposed approach of measuring model performance gives novel insights when deciding on a customer retention campaign. The method is dependent on the characteristics of the customer base as is compliant with modern business analytics and accommodates the data-driven culture that has manifested itself within organizations.},
  file = {Óskarsdóttir et al_2018_Profit-Based Model Selection for Customer Retention Using Individual Customer.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\LHDEV8VL\\Óskarsdóttir et al_2018_Profit-Based Model Selection for Customer Retention Using Individual Customer.pdf:application/pdf},
}

@Article{ascarza_retention_2018,
  title = {Retention {Futility}: {Targeting} {High}-{Risk} {Customers} {Might} be {Ineffective}},
  volume = {55},
  issn = {0022-2437, 1547-7193},
  shorttitle = {Retention {Futility}},
  url = {http://journals.sagepub.com/doi/10.1509/jmr.16.0163},
  doi = {10.1509/jmr.16.0163},
  abstract = {Companies in a variety of sectors are increasingly managing customer churn proactively, generally by detecting customers at the highest risk of churning and targeting retention efforts towards them. While there is a vast literature on developing churn prediction models that identify customers at the highest risk of churning, no research has investigated whether it is indeed optimal to target those individuals. Combining two field experiments with machine learning techniques, the author demonstrates that customers identified as having the highest risk of churning are not necessarily the best targets for proactive churn programs. This finding is not only contrary to common wisdom but also suggests that retention programs are sometimes futile not because firms offer the wrong incentives but because they do not apply the right targeting rules. Accordingly, firms should focus their modeling efforts on identifying the observed heterogeneity in response to the intervention and to target customers on the basis of their sensitivity to the intervention, regardless of their risk of churning. This approach is empirically demonstrated to be significantly more effective than the standard practice of targeting customers with the highest risk of churning. More broadly, the author encourages firms and researchers using randomized trials (or A/B tests) to look beyond the average effect of interventions and leverage the observed heterogeneity in customers' response to select customer targets.},
  language = {en},
  number = {1},
  urldate = {2020-09-07},
  journal = {Journal of Marketing Research},
  author = {Eva Ascarza},
  month = {feb},
  year = {2018},
  pages = {80--98},
}

@Article{garrido_robust_2018,
  title = {A {Robust} profit measure for binary classification model evaluation},
  volume = {92},
  issn = {09574174},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417417306498},
  doi = {10.1016/j.eswa.2017.09.045},
  language = {en},
  urldate = {2020-09-07},
  journal = {Expert Systems with Applications},
  author = {Franco Garrido and Wouter Verbeke and Cristián Bravo},
  month = {feb},
  year = {2018},
  pages = {154--160},
  abstract = {Using profit-based evaluation measures is a necessity in business-oriented contexts, as they aid companies in making cost-optimal decisions. Among the measures that effectively include the true nature of costs and benefits in binary classification, the expected maximum profit (EMP) has been used successfully for churn prediction and credit scoring, and defined in general for binary classification problems. However, despite its competitive results against the most frequently used measures, the EMP relies on a fixed probability distribution of costs and benefits, the range of which in real applications is not entirely known. In this paper, we propose to extend this measure by adding random shocks to these distributions. We call this new measure the R-EMP, following the convention of the analogous EMP measure. Our metric adds a stochastic component to each point of the cost-benefit distributions, assuming that costs and benefits have a fixed probability, but its distribution range is subject to an external shock, which can be different for each cost or benefit. The experimental set-up is focused on a credit scoring application using a dataset of a Chilean financial institution, with the attribute selection for a logistic regression being accomplished using the AUC, EMP, H-measure, and R-EMP as the selection criteria. The results indicate that the R-EMP measure is the most robust metric for achieving the greatest profit for the company under uncertain external conditions.},
  file = {Garrido et al_2018_A Robust profit measure for binary classification model evaluation.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\DR98RGES\\Garrido et al_2018_A Robust profit measure for binary classification model evaluation.pdf:application/pdf},
}

@InProceedings{drachen_be_2018,
  address = {Brisband, Queensland, Australia},
  title = {To be or not to be...social: incorporating simple social features in mobile game customer lifetime value predictions},
  isbn = {978-1-4503-5436-3},
  shorttitle = {To be or not to be...social},
  url = {http://dl.acm.org/citation.cfm?doid=3167918.3167925},
  doi = {10.1145/3167918.3167925},
  language = {en},
  urldate = {2020-09-07},
  booktitle = {Proceedings of the {Australasian} {Computer} {Science} {Week} {Multiconference} on - {ACSW} '18},
  publisher = {ACM Press},
  author = {Anders Drachen and Mari Pastor and Aron Liu and Dylan Jack Fontaine and Yuan Chang and Julian Runge and Rafet Sifa and Diego Klabjan},
  year = {2018},
  pages = {1--10},
  abstract = {Mobile games make up the largest segment of the games industry, in terms of revenue as well as players. Hundreds of thousands of games are available with most being free to download and play. In freemium games, revenue is predominantly generated by users making in-game purchases. As only a small fraction of users make purchases, predicting these users and their Customer Lifetime Value are key challenges in Game Analytics and currently barely explored in academic research. Furthermore, while social factors have been shown to be essential for retention in games in general, the impact on retention and monetization in mobile games is unexplored. In this paper, the problem of defining social features in freemium casual mobile games is addressed through a case study with over 200,000 players. The study evaluates the influence of specific types of social interactions typical of casual mobile games, on predictions of premium users and Customer Lifetime Value by applying classifiers and regression models respectively. Results indicate that social activity does not correlate with the tendency to become a premium user, but that social activity increases over time in a cohort.},
  file = {Drachen et al_2018_To be or not to be.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\HQ9VRZI6\\Drachen et al_2018_To be or not to be.pdf:application/pdf},
}

@Article{ding_multiagent-consensus-mapreduce-based_2018,
  title = {Multiagent-consensus-{MapReduce}-based attribute reduction using co-evolutionary quantum {PSO} for big data applications},
  volume = {272},
  issn = {09252312},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231217311876},
  doi = {10.1016/j.neucom.2017.06.059},
  language = {en},
  urldate = {2020-09-07},
  journal = {Neurocomputing},
  author = {Weiping Ding and Chin-Teng Lin and Senbo Chen and Xiaofeng Zhang and Bin Hu},
  month = {jan},
  year = {2018},
  abstract = {The attribute reduction for big data applications has become an urgent challenge in pattern recognition, machine learning and data mining. In this paper, we introduce the multi-agent consensus MapReduce optimization model and co-evolutionary quantum PSO with self-adaptive memeplexes for designing the attribute reduction method, and propose a multiagent-consensus-MapReduce-based attribute reduction algorithm (MCMAR). Firstly, the co-evolutionary quantum PSO with self-adaptive memeplexes is designed for grouping particles into different memeplexes, which aims to explore the search space and locate the global best region during the attribute reduction of big datasets. Secondly, the four layers neighborhood radius framework with compensatory scheme is constructed to partition big attribute sets by exploiting the interdependency among multiple-relevant-attribute sets. Thirdly, a novel multi-agent consensus MapReduce optimization model is adopted to perform the multiple-relevance-attribute reduction, in which five kinds of agents are used to conduct the ensemble co-evolutionary optimization. So the uniform reduction framework of different agents’ co-evolutionary game under the bounded rationality is further refined. Fourthly, the approximation MapReduce parallelism mechanism is permitted to formalize to the multi-agent co-evolutionary consensus structure, interaction and adaptation, which enhances different agents to share their solutions. Finally, extensive experimental studies substantiate the effectiveness and accuracy of MCMAR on some well-known benchmark datasets. Moreover, successful applications in big medical datasets are expected to dramatically scaling up MCMAR for complex infant brain MRI in terms of efficiency and feasibility.},
  pages = {136--153},
}

@Article{zhu_benchmarking_2018,
  title = {Benchmarking sampling techniques for imbalance learning in churn prediction},
  volume = {69},
  issn = {0160-5682, 1476-9360},
  url = {https://www.tandfonline.com/doi/full/10.1057/s41274-016-0176-1},
  doi = {10.1057/s41274-016-0176-1},
  language = {en},
  number = {1},
  urldate = {2020-09-07},
  journal = {Journal of the Operational Research Society},
  author = {Bing Zhu and Bart Baesens and Aimée Backiel and Seppe K. L. M. {vanden Broucke}},
  month = {jan},
  year = {2018},
  pages = {49--65},
  abstract = {Class imbalance presents significant challenges to customer churn prediction. Many data-level sampling solutions have been developed to deal with this issue. In this paper, we comprehensively compare the performance of several state-of-the-art sampling techniques in the context of churn prediction. A recently developed maximum profit criterion is used as one of the main performance measures to offer more insights from the perspective of cost–benefit. The experimental results show that the impact of sampling methods depends on the used evaluation metric and that the impact pattern is interrelated with the classifiers. An in-depth exploration of the reaction patterns is conducted, and suitable sampling strategies are recommended for each situation. Furthermore, we also discuss the setting of the sampling rate in the empirical comparison. Our findings will offer a useful guideline for the use of sampling methods in the context of churn prediction.},
  file = {Zhu et al_2018_Benchmarking sampling techniques for imbalance learning in churn prediction.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\3H469XIL\\Zhu et al_2018_Benchmarking sampling techniques for imbalance learning in churn prediction.pdf:application/pdf},
}

@Article{chattopadhyay_herding_2018,
  title = {Herding by {Foreign} {Institutional} {Investors}: {An} {Evidential} {Exploration} for {Persistence} and {Predictability}},
  volume = {19},
  issn = {1542-7560, 1542-7579},
  shorttitle = {Herding by {Foreign} {Institutional} {Investors}},
  url = {https://www.tandfonline.com/doi/full/10.1080/15427560.2017.1373282},
  doi = {10.1080/15427560.2017.1373282},
  language = {en},
  number = {1},
  urldate = {2020-09-07},
  journal = {Journal of Behavioral Finance},
  author = {Manojit Chattopadhyay and Ashish Kumar Garg and Subrata Kumar Mitra},
  month = {jan},
  year = {2018},
  abstract = {The primary objective of the study is to explore the predictability of herding patterns of foreign institutional investors in the Indian market using high frequency data over a period from January 2003 to June 2014. Herding of an individual stock was measured estimating a simple volume based ratio and persistence of trends was detected using the runs test (Wald and Wolfowitz [1940]) on that ratio. Predictability of herding behavior has been successfully modeled by applying 7 data mining models using various measures of performance. Market regulators may consider our findings to regulate the foreign institutional investors trading to make the financial system more transparent and robust.},
  pages = {73--88},
}

@InCollection{bhateja_churn_2018,
  address = {Singapore},
  title = {Churn and {Non}-churn of {Customers} in {Banking} {Sector} {Using} {Extreme} {Learning} {Machine}},
  volume = {712},
  isbn = {978-981-10-8227-6 978-981-10-8228-3},
  url = {http://link.springer.com/10.1007/978-981-10-8228-3_6},
  urldate = {2020-09-07},
  booktitle = {Proceedings of the {Second} {International} {Conference} on {Computational} {Intelligence} and {Informatics}},
  publisher = {Springer Singapore},
  author = {Ramakanta Mohanty and C. {Naga Ratna Sree}},
  editor = {Vikrant Bhateja and João Manuel R.S. Tavares and B. Padmaja Rani and V. Kamakshi Prasad and K. Srujan Raju},
  year = {2018},
  doi = {10.1007/978-981-10-8228-3_6},
  note = {Series Title: Advances in Intelligent Systems and Computing},
  abstract = {With an invention been made in computational techniques, there has been increasing interest in the field of neural networks as major potential machine learning techniques have come to the front. In the current past, gradient descent-based algorithm is used in feed forward neural network and the parameter utilized, which devours extensively more opportunity for learning and tuned iteratively. The single hidden layer feed forward neural network have the input weights, the hidden layer and biases randomly assigned. The straightforward backward operation is utilized to discover output weight which thus relies on upon handling from hidden layer to output layer. In this paper, we propose to utilize Extreme Learning Machine (ELM) to foresee client churn. The goal of this paper is to propose a novel approach that enhances the precision of churn and non-churn of clients using banking data. },
  pages = {51--58},
}

@Article{sun_stability_2018,
  title = {A stability constrained adaptive alpha for gravitational search algorithm},
  volume = {139},
  issn = {09507051},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705117304884},
  doi = {10.1016/j.knosys.2017.10.018},
  language = {en},
  urldate = {2020-09-07},
  journal = {Knowledge-Based Systems},
  author = {Genyun Sun and Ping Ma and Jinchang Ren and Aizhu Zhang and Xiuping Jia},
  month = {jan},
  year = {2018},
  pages = {200--213},
  abstract = {Gravitational search algorithm (GSA), a recent meta-heuristic algorithm inspired by Newton's law of gravity and mass interactions, shows good performance in various optimization problems. In GSA, the gravitational constant attenuation factor alpha (α) plays a vital role in convergence and the balance between exploration and exploitation. However, in GSA and most of its variants, all agents share the same α value without considering their evolutionary states, which has inevitably caused the premature convergence and imbalance of exploration and exploitation. In order to alleviate these drawbacks, in this paper, we propose a new variant of GSA, namely stability constrained adaptive alpha for GSA (SCAA). In SCAA, each agent's evolutionary state is estimated, which is then combined with the variation of the agent's position and fitness feedback to adaptively adjust the value of α. Moreover, to preserve agents’ stable trajectories and improve convergence precision, a boundary constraint is derived from the stability conditions of GSA to restrict the value of α in each iteration. The performance of SCAA has been evaluated by comparing with the original GSA and four alpha adjusting algorithms on 13 conventional functions and 15 complex CEC2015 functions. The experimental results have demonstrated that SCAA has significantly better searching performance than its peers do.},
  file = {Sun et al_2018_A stability constrained adaptive alpha for gravitational search algorithm.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\B62XYRI7\\Sun et al_2018_A stability constrained adaptive alpha for gravitational search algorithm.pdf:application/pdf},
}

@Article{bogaert_identifying_2017,
  title = {Identifying {Soccer} {Players} on {Facebook} {Through} {Predictive} {Analytics}},
  volume = {14},
  issn = {1545-8490, 1545-8504},
  url = {http://pubsonline.informs.org/doi/10.1287/deca.2017.0354},
  doi = {10.1287/deca.2017.0354},
  language = {en},
  number = {4},
  urldate = {2020-09-07},
  journal = {Decision Analysis},
  author = {Matthias Bogaert and Michel Ballings and Martijn Hosten and Dirk {Van den Poel}},
  month = {dec},
  year = {2017},
  pages = {274--297},
  abstract = {This study assesses the feasibility of identifying self-reported sports practitioners (soccer players) on Facebook. The main goal is to develop a system to support marketers with the decision as to which prospects to target for advertising purposes. To do so, we benchmark several algorithms (i.e., random forest, logistic regression, adaboost, rotation forest, neural networks, and kernel factory) using five times twofold cross-validation. To evaluate performance and variable importances, we build a fusion model, which combines the results of the other algorithms using the weighted average. This technique is also referred to as information-fusion sensitivity analysis. The results reveal that Facebook data provide a viable basis to come up with sports predictions as the predictive performance ranges from 72.01% to 80.43% for area under the receiver operating characteristic curve (AUC), from 81.96% to 83.95% for accuracy, and from 2.41 to 3.06 for top-decile lift. Our benchmark study indicates that stochastic adaboost, the fusion model, random forest, rotation forest, and regularized logistic regression are the best-performing algorithms. Furthermore, the results show that the most important variables are the average number of friends that play soccer, membership of a soccer group, and the number of favorite teams. We also assess the impact of our results on profitability by conducting a thorough sensitivity analysis. Our analysis reveals that our approach can be beneficial for a wide range of companies. The analysis and results in this study will assist sports brands with decisions regarding their implementation of targeted marketing approaches.},
  file = {Bogaert et al_2017_Identifying Soccer Players on Facebook Through Predictive Analytics.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\WWJP7PKU\\Bogaert et al_2017_Identifying Soccer Players on Facebook Through Predictive Analytics.pdf:application/pdf},
}

@Article{liu_personalized_2018,
  title = {Personalized {Air} {Travel} {Prediction}: {A} {Multi}-factor {Perspective}},
  volume = {9},
  issn = {2157-6904, 2157-6912},
  shorttitle = {Personalized {Air} {Travel} {Prediction}},
  url = {https://dl.acm.org/doi/10.1145/3078845},
  doi = {10.1145/3078845},
  language = {en},
  number = {3},
  urldate = {2020-09-07},
  journal = {ACM Transactions on Intelligent Systems and Technology},
  author = {Jie Liu and Bin Liu and Yanchi Liu and Huipeng Chen and Lina Feng and Hui Xiong and Yalou Huang},
  month = {feb},
  year = {2018},
  abstract = {Human mobility analysis is one of the most important research problems in the field of urban computing. Existing research mainly focuses on the intra-city ground travel behavior modeling, while the inter-city air travel behavior modeling has been largely ignored. Actually, the inter-city travel analysis can be of equivalent importance and complementary to the intra-city travel analysis. Understanding massive passenger-air-travel behavior delivers intelligence for airlines’ precision marketing and related socioeconomic activities, such as airport planning, emergency management, local transportation planning, and tourism-related businesses. Moreover, it provides opportunities to study the characteristics of cities and the mutual relationships between them. However, modeling and predicting air traveler behavior is challenging due to the complex factors of the market situation and individual characteristics of customers (e.g., airlines’ market share, customer membership, and travelers’ intrinsic interests on destinations). To this end, in this article, we present a systematic study on the personalized air travel prediction problem, namely where a customer will fly to and which airline carrier to fly with, by leveraging real-world anonymized Passenger Name Record (PNR) data. Specifically, we first propose a relational travel topic model, which combines the merits of latent factor model with a neighborhood-based method, to uncover the personal travel preferences of aviation customers and the latent travel topics of air routes and airline carriers simultaneously. Then we present a multi-factor travel prediction framework, which fuses complex factors of the market situation and individual characteristics of customers, to predict airline customers’ personalized travel demands. Experimental results on two real-world PNR datasets demonstrate the effectiveness of our approach on both travel topic discovery and customer travel prediction.},
  pages = {1--26},
}

@Article{goswami_challenges_2017,
  title = {Challenges in the {Analysis} of {Online} {Social} {Networks}: {A} {Data} {Collection} {Tool} {Perspective}},
  volume = {97},
  issn = {0929-6212, 1572-834X},
  shorttitle = {Challenges in the {Analysis} of {Online} {Social} {Networks}},
  url = {http://link.springer.com/10.1007/s11277-017-4712-3},
  doi = {10.1007/s11277-017-4712-3},
  language = {en},
  number = {3},
  urldate = {2020-09-07},
  journal = {Wireless Personal Communications},
  author = {Anuradha Goswami and Ajey Kumar},
  month = {dec},
  year = {2017},
  abstract = {The present era of internet has radically changed the way people communicate with each other. Online Social Network platforms have enhanced this to real-time communication where interactions vary from casual relationships to formal bonding. This real-time communication between the users over Online Social Network platforms generates data which directly or indirectly gives lot of information. But extracting this data and mining information out of it is a profound challenge. Researchers need appropriate tools to churn out this data and get valuable information by analyzing and visualizing it. This paper does a comprehensive survey of types of Online Social Network Analysis resulting in segregation of research challenges associated with each of the types. A detailed study of the existing data collection tools and analysis techniques was further carried out to understand the challenges a researcher faces while using it. Finally, mapping analysis was done using research challenges, data collection tools and the types of Online Social Network Analysis, to understand to what extent the existing data collection tools and analysis techniques can meet the research challenges. The mapping analysis shows an absolute requirement of new data collection tools and algorithms by the researchers/developers.},
  pages = {4015--4061},
}

@Article{department_of_applied_mobile_technology_yuanpei_university_of_medical_technology_hsinchu_30015_taiwan_comparison_2017,
  title = {Comparison of {Deep} {Learning} {Algorithms} to {Predict} {Customer} {Churn} within a {Local} {Retail} {Industry}},
  volume = {7},
  issn = {20103700},
  url = {http://www.ijmlc.org/vol7/634-P16.pdf},
  doi = {10.18178/ijmlc.2017.7.5.634},
  number = {5},
  urldate = {2020-09-07},
  journal = {International Journal of Machine Learning and Computing},
  author = {{Department of Applied Mobile Technology, Yuanpei University of Medical Technology, HsinChu, 30015, Taiwan} and Alexiei Dingli and Vincent Marmara and Nicole Sant Fournier},
  month = {oct},
  year = {2017},
  pages = {128--132},
  abstract = {A top priority in any business is a constant need to increase  revenue  and  profitability. One  of  the  causes  for  a decrease in profits is when current customers stop transacting. When   a   customer   leaves   or   churns   from   a   business,   the opportunity  for  potential  sales  or  cross  selling  is  lost.  If  a customer  leaves  the  business  without  any  form  of  advice,  the company may find it hard to respond and take corrective action. Ideally   companies   should   adopt   a   proactive   and   identify potential  churners  prior  to  them  leaving.  Customer  retention strategies have been noted to be less costly than attracting new customers.  Through  data  available  within  the  Point  of  Sales (POS)  systems,  customer  transactions  may  be  extracted  and their buying patterns may be analysed. This paper demonstrates how through transactional data features are created and may be identified  as  significant  to  predict  churn  within  the  retail industry. The data provided within this paper pertains to a local supermarket.  Therefore, the  churners  identified  and  results attained are based on real scenarios. The novelty of this paper is the    concept    of    implementing    deep    learning    algorithms. Convolution   Neural   Networks   and   Restricted   Boltzmann Machine   are   the   selected   deep   learning   techniques.   The Restricted Boltzmann Machine attained the best results that of 83% in predicting customer churn},
  file = {Department of Applied Mobile Technology, Yuanpei University of Medical Technology, HsinChu, 30015, Taiwan et al_2017_Comparison of Deep Learning Algorithms to Predict Customer Churn within a Local.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\UMRW3HDR\\Department of Applied Mobile Technology, Yuanpei University of Medical Technology, HsinChu, 30015, Taiwan et al. - 2017 - Comparison of Deep Learning Algorithms to Predict .pdf:application/pdf},
}

@Article{xiao_gmdh-based_2017,
  title = {{GMDH}-based semi-supervised feature selection for customer classification},
  volume = {132},
  issn = {09507051},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705117302927},
  doi = {10.1016/j.knosys.2017.06.018},
  language = {en},
  urldate = {2020-09-07},
  journal = {Knowledge-Based Systems},
  author = {Jin Xiao and Hanwen Cao and Xiaoyi Jiang and Xin Gu and Ling Xie},
  month = {sep},
  year = {2017},
  abstract = {Data dimension reduction is an important step for customer classification modeling, and feature selection has been a research focus of the data dimension reduction field. This study introduces the group method of data handling (GMDH), puts forward a GMDH-based semi-supervised feature selection (GMDH-SSFS) algorithm, and applies it to customer feature selection. The algorithm can utilize a few samples with class labels L, and a large number of samples without class labels U simultaneously. What is more, it considers the relationship between features and class labels, and that between features during feature selection. The GMDH-SSFS model mainly consists of three stages: 1) Train N basic classification models based on the dataset L with class labels; 2) Label samples selectively in the dataset U without class labels, and add them to L; 3) Train the GMDH neural network based on the new training set L, and select the optimal feature subset FS. Based on an empirical analysis of four customer classification datasets, results suggest that the features selected by the GMDH-SSFS model have a good explainability. Meanwhile, the customer classification performance of the classification model trained by the selected feature subset is superior to that of the models trained by the commonly used Laplacian score (an unsupervised feature selection algorithm), Fisher score (a supervised feature selection algorithm), and the FW-SemiFS and S3VM-FS (two semi-supervised feature selection algorithms).},
  pages = {236--248},
}

@InProceedings{wang_partition_2017,
  address = {Dalian, China},
  title = {Partition cost-sensitive {CART} based on customer value for {Telecom} customer churn prediction},
  isbn = {978-988-15639-3-4},
  url = {http://ieeexplore.ieee.org/document/8028259/},
  doi = {10.23919/ChiCC.2017.8028259},
  urldate = {2020-09-07},
  booktitle = {2017 36th {Chinese} {Control} {Conference} ({CCC})},
  publisher = {IEEE},
  author = {Chuanqi Wang and Ruiqi Li and Peng Wang and Zonghai Chen},
  month = {jul},
  year = {2017},
  abstract = {Telecom Customer churn prediction is a cost sensitive classification problem. Most of studies regard it as a general classification problem use traditional methods, that the two types of misclassification cost are equal. And, in aspect of cost sensitive classification, there are some researches focused on static cost sensitive situation. In fact, customer value of each customer is different, so misclassification cost of each sample is different. For this problem, we propose the partition cost-sensitive CART model in this paper. According to the experiment based on the real data, it is showed that the method not only obtains a good classification performance, but also reduces the total misclassification costs effectively.},
  pages = {5680--5684},
}

@Article{barfar_applying_2017,
  title = {Applying behavioral economics in predictive analytics for {B2B} churn: {Findings} from service quality data},
  volume = {101},
  issn = {01679236},
  shorttitle = {Applying behavioral economics in predictive analytics for {B2B} churn},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167923617301185},
  doi = {10.1016/j.dss.2017.06.006},
  language = {en},
  urldate = {2020-09-07},
  journal = {Decision Support Systems},
  author = {Arash Barfar and Balaji Padmanabhan and Alan Hevner},
  month = {sep},
  year = {2017},
  abstract = {Motivated by the long-standing debate on rationality in behavioral economics and the potential of theory-driven predictive analytics, this paper examines the link between service quality and B2B churn. Using longitudinal B2B transactional data with service quality indicators provided by a large company, we present evidence that both rationality and bounded-rationality assumptions play significant roles in predicting organizational decisions on churn. Specifically, variables that relate to the assumed rationality of organizations appear to provide accurate predictions while, at the same time, variables that capture boundedly rational decision rules appear to play a role through “somatic states” that make organizations more sensitive to the rational variables. In addition to presenting a novel approach for predicting organizational decisions on churn, this paper offers theoretical and managerial insights as well as opportunities for future research at the intersection of behavioral economics and predictive analytics for decision-making.},
  pages = {115--127},
}

@Article{lopez-diaz_stochastic_2017,
  title = {A stochastic comparison of customer classifiers with an application to customer attrition in commercial banking},
  volume = {2017},
  issn = {0346-1238, 1651-2030},
  url = {https://www.tandfonline.com/doi/full/10.1080/03461238.2016.1209549},
  doi = {10.1080/03461238.2016.1209549},
  language = {en},
  number = {7},
  urldate = {2020-09-07},
  journal = {Scandinavian Actuarial Journal},
  author = {M. C. López-Díaz and M. López-Díaz and S. Martínez-Fernández},
  month = {aug},
  year = {2017},
  abstract = {The classification of clients is an essential matter in commercial banking, insurance companies, electrical corporations, communication business, etc. Those companies frequently classify their customers by means of the information provided by the so-called classifier. Motivated by the need to compare systems of classification, we introduce a new stochastic order which permits the comparison of classifiers. The stochastic order is analysed in detail, providing characterizations and properties as well as connections with other stochastic orders and other classification systems. Such an order is applied to compare some classifiers used by a Spanish commercial banking to analyse the key problem of customer churn, obtaining conclusive results by means of real databases. Namely, the optimal classifier among them in the new stochastic order is obtained.},
  pages = {606--627},
}

@Article{wu_fast_2017,
  title = {A {Fast} {Density} and {Grid} {Based} {Clustering} {Method} for {Data} {With} {Arbitrary} {Shapes} and {Noise}},
  volume = {13},
  issn = {1551-3203, 1941-0050},
  url = {http://ieeexplore.ieee.org/document/7744455/},
  doi = {10.1109/TII.2016.2628747},
  number = {4},
  urldate = {2020-09-07},
  journal = {IEEE Transactions on Industrial Informatics},
  author = {Bo Wu and Bogdan M. Wilamowski},
  month = {aug},
  year = {2017},
  abstract = {This paper presents a density- and grid- based (DGB) clustering method for categorizing data with arbitrary shapes and noise. As most of the conventional clustering approaches work only with round-shaped clusters, other methods are needed to be explored to proceed classification of clusters with arbitrary shapes. Clustering approach by fast search and find of density peaks and density-based spatial clustering of applications with noise, and so many other methods are reported to be capable of completing this task but are limited by their computation time of mutual distances between points or patterns. Without the calculation of mutual distances, this paper presents an alternative method to fulfill clustering of data with any shape and noise even faster and with more efficiency. It was successfully verified in clustering industrial data (e.g., DNA microarray data) and several benchmark datasets with different kinds of noise. It turned out that the proposed DGB clustering method is more efficient and faster in clustering datasets with any shape than the conventional methods.},
  pages = {1620--1628},
}

@InProceedings{mitrovic_scalable_2017,
  address = {Tokyo, Japan},
  title = {Scalable {RFM}-enriched {Representation} {Learning} for {Churn} {Prediction}},
  isbn = {978-1-5090-5004-8},
  url = {http://ieeexplore.ieee.org/document/8259766/},
  doi = {10.1109/DSAA.2017.42},
  urldate = {2020-09-07},
  booktitle = {2017 {IEEE} {International} {Conference} on {Data} {Science} and {Advanced} {Analytics} ({DSAA})},
  publisher = {IEEE},
  author = {Sandra Mitrovic and Gaurav Singh and Bart Baesens and Wilfried Lemahieu and Jochen {de Weerdt}},
  month = {oct},
  year = {2017},
  abstract = {Most of the recent studies on churn prediction in telco utilize social networks built on top of the call (and/or SMS) graphs to derive informative features. However, extracting features from large graphs, especially structural features, is an intricate process both from a methodological and computational perspective. Due to the former, feature extraction in the current literature has mainly been addressed in an ad-hoc and hand-crafted manner. Due to the latter, the full potential of the structural information is unexploited. In this work, we incorporate both interaction and structural information by devising two different ways of enriching original graphs with interaction information, delineated by the well-known RFM model. We circumvent the process of extensive manual feature engineering by enriching the networks and improving the scalability of the renowned node2vec approach to learn node representations. The obtained results demonstrate that our enriched network outperforms baseline RFM-based methods.},
  pages = {79--88},
}

@InProceedings{semrl_churn_2017,
  address = {Krakow},
  title = {Churn prediction model for effective gym customer retention},
  isbn = {978-1-5386-2365-7},
  url = {http://ieeexplore.ieee.org/document/8256385/},
  doi = {10.1109/BESC.2017.8256385},
  urldate = {2020-09-07},
  booktitle = {2017 {International} {Conference} on {Behavioral}, {Economic}, {Socio}-cultural {Computing} ({BESC})},
  publisher = {IEEE},
  author = {Jas Semrl and Alexandru Matei},
  month = {oct},
  year = {2017},
  abstract = {In the fitness industry, rolling gym membership contracts allow customers to terminate a contract with little advanced notice. Customer churn prediction is a well known area in Machine Learning research. Many companies, however, face a data science skills gap when trying to translate this research onto their own datasets and IT infrastructure. In this paper we present a series of experiments that aim to predict customer behaviour, in order to increase gym utilisation and customer retention. We use two off-the-shelf machine learning platforms, so that we can evaluate whether these platforms, used by non ML experts, can help companies improve their services.},
  pages = {1--3},
}

@Article{garcia_intelligent_2017,
  title = {Intelligent data analysis approaches to churn as a business problem: a survey},
  volume = {51},
  issn = {0219-1377, 0219-3116},
  shorttitle = {Intelligent data analysis approaches to churn as a business problem},
  url = {http://link.springer.com/10.1007/s10115-016-0995-z},
  doi = {10.1007/s10115-016-0995-z},
  language = {en},
  number = {3},
  urldate = {2020-09-07},
  journal = {Knowledge and Information Systems},
  author = {David L. García and Àngela Nebot and Alfredo Vellido},
  month = {jun},
  year = {2017},
  pages = {719--774},
  abstract = {Globalization processes and market deregulation policies are rapidly changing the competitive environments of many economic sectors. The appearance of new competitors and technologies leads to an increase in competition and, with it, a growing preoccupation among service-providing companies with creating stronger customer bonds. In this context, anticipating the customer’s intention to abandon the provider, a phenomenon known as churn, becomes a competitive advantage. Such anticipation can be the result of the correct application of information-based knowledge extraction in the form of business analytics. In particular, the use of intelligent data analysis, or data mining, for the analysis of market surveyed information can be of great assistance to churn management. In this paper, we provide a detailed survey of recent applications of business analytics to churn, with a focus on computational intelligence methods. This is preceded by an in-depth discussion of churn within the context of customer continuity management. The survey is structured according to the stages identified as basic for the building of the predictive models of churn, as well as according to the different types of predictive methods employed and the business areas of their application.},
  file = {García et al_2017_Intelligent data analysis approaches to churn as a business problem.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\2DKYMTJM\\García et al_2017_Intelligent data analysis approaches to churn as a business problem.pdf:application/pdf},
}

@Article{amin_customer_2017,
  title = {Customer churn prediction in the telecommunication sector using a rough set approach},
  volume = {237},
  issn = {09252312},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231216314849},
  doi = {10.1016/j.neucom.2016.12.009},
  language = {en},
  urldate = {2020-09-07},
  journal = {Neurocomputing},
  author = {Adnan Amin and Sajid Anwar and Awais Adnan and Muhammad Nawaz and Khalid Alawfi and Amir Hussain and Kaizhu Huang},
  month = {may},
  year = {2017},
  abstract = {Customer churn is a critical and challenging problem affecting business and industry, in particular, the rapidly growing, highly competitive telecommunication sector. It is of substantial interest to both academic researchers and industrial practitioners, interested in forecasting the behavior of customers in order to differentiate the churn from non-churn customers. The primary motivation is the dire need of businesses to retain existing customers, coupled with the high cost associated with acquiring new ones. A review of the field has revealed a lack of efficient, rule-based Customer Churn Prediction (CCP) approaches in the telecommunication sector. This study proposes an intelligent rule-based decision-making technique, based on rough set theory (RST), to extract important decision rules related to customer churn and non-churn. The proposed approach effectively performs classification of churn from non-churn customers, along with prediction of those customers who will churn or may possibly churn in the near future. Extensive simulation experiments are carried out to evaluate the performance of our proposed RST based CCP approach using four rule-generation mechanisms, namely, the Exhaustive Algorithm (EA), Genetic Algorithm (GA), Covering Algorithm (CA) and the LEM2 algorithm (LA). Empirical results show that RST based on GA is the most efficient technique for extracting implicit knowledge in the form of decision rules from the publicly available, benchmark telecom dataset. Further, comparative results demonstrate that our proposed approach offers a globally optimal solution for CCP in the telecom sector, when benchmarked against several state-of-the-art methods. Finally, we show how attribute-level analysis can pave the way for developing a successful customer retention policy that could form an indispensable part of strategic decision making and planning process in the telecom sector.},
  pages = {242--254},
}

@Article{coussement_comparative_2017,
  title = {A comparative analysis of data preparation algorithms for customer churn prediction: {A} case study in the telecommunication industry},
  volume = {95},
  issn = {01679236},
  shorttitle = {A comparative analysis of data preparation algorithms for customer churn prediction},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167923616302020},
  doi = {10.1016/j.dss.2016.11.007},
  language = {en},
  urldate = {2020-09-07},
  journal = {Decision Support Systems},
  author = {Kristof Coussement and Stefan Lessmann and Geert Verstraeten},
  month = {mar},
  year = {2017},
  abstract = {Data preparation is a process that aims to convert independent (categorical and continuous) variables into a form appropriate for further analysis. We examine data-preparation alternatives to enhance the prediction performance for the commonly-used logit model. This study, conducted in a churn prediction modeling context, benchmarks an optimized logit model against eight state-of-the-art data mining techniques that use standard input data, including real-world cross-sectional data from a large European telecommunication provider. The results lead to following conclusions. (i) Analysts better acknowledge that the data-preparation technique they choose actually affects churn prediction performance; we find improvements of up to 14.5% in the area under the receiving operating characteristics curve and 34% in the top decile lift. (ii) The enhanced logistic regression also is competitive with more advanced single and ensemble data mining algorithms. This article concludes with some managerial implications and suggestions for further research, including evidence of the generalizability of the results for other business settings.},
  pages = {27--36},
}

@Article{khodabandehlou_comparison_2017,
  title = {Comparison of supervised machine learning techniques for customer churn prediction based on analysis of customer behavior},
  volume = {19},
  issn = {1328-7265},
  url = {https://www.emerald.com/insight/content/doi/10.1108/JSIT-10-2016-0061/full/html},
  doi = {10.1108/JSIT-10-2016-0061},
  language = {en},
  number = {1/2},
  urldate = {2020-09-07},
  journal = {Journal of Systems and Information Technology},
  author = {Samira Khodabandehlou and Mahmoud {Zivari Rahman}},
  month = {mar},
  year = {2017},
  abstract = {This paper aims to provide a predictive framework of customer churn through six stages for accurate prediction and preventing customer churn in the field of business.
Design/methodology/approach:The six stages are as follows: first, collection of customer behavioral data and preparation of the data; second, the formation of derived variables and selection of influential variables, using a method of discriminant analysis; third, selection of training and testing data and reviewing their proportion; fourth, the development of prediction models using simple, bagging and boosting versions of supervised machine learning; fifth, comparison of churn prediction models based on different versions of machine-learning methods and selected variables; and sixth, providing appropriate strategies based on the proposed model. Findings According to the results, five variables, the number of items, reception of returned items, the discount, the distribution time and the prize beside the recency, frequency and monetary (RFM) variables (RFMITSDP), were chosen as the best predictor variables. The proposed model with accuracy of 97.92 per cent, in comparison to RFM, had much better performance in churn prediction and among the supervised machine learning methods, artificial neural network (ANN) had the highest accuracy, and decision trees (DT) was the least accurate one. The results show the substantially superiority of boosting versions in prediction compared with simple and bagging models. Research limitations/implications: The period of the available data was limited to two years. The research data were limited to only one grocery store whereby it may not be applicable to other industries; therefore, generalizing the results to other business centers should be used with caution. Practical implications: Business owners must try to enforce a clear rule to provide a prize for a certain number of purchased items. Of course, the prize can be something other than the purchased item. Business owners must accept the items returned by the customers for any reasons, and the conditions for accepting returned items and the deadline for accepting the returned items must be clearly communicated to the customers. Store owners must consider a discount for a certain amount of purchase from the store. They have to use an exponential rule to increase the discount when the amount of purchase is increased to encourage customers for more purchase. The managers of large stores must try to quickly deliver the ordered items, and they should use equipped and new transporting vehicles and skilled and friendly workforce for delivering the items. It is recommended that the types of services, the rules for prizes, the discount, the rules for accepting the returned items and the method of distributing the items must be prepared and shown in the store for all the customers to see. The special services and reward rules of the store must be communicated to the customers using new media such as social networks. To predict the customer behaviors based on the data, the future researchers should use the boosting method because it increases efficiency and accuracy of prediction. It is recommended that for predicting the customer behaviors, particularly their churning status, the ANN method be used. To extract and select the important and effective variables influencing customer behaviors, the discriminant analysis method can be used which is a very accurate and powerful method for predicting the classes of the customers. Originality/value: The current study tries to fill this gap by considering five basic and important variables besides RFM in stores, i.e. prize, discount, accepting returns, delay in distribution and the number of items, so that the business owners can understand the role services such as prizes, discount, distribution and accepting returns play in retraining the customers and preventing them from churning. Another innovation of the current study is the comparison of machine-learning methods with their boosting and bagging versions, especially considering the fact that previous studies do not consider the bagging method. The other reason for the study is the conflicting results regarding the superiority of machine-learning methods in a more accurate prediction of customer behaviors, including churning. For example, some studies introduce ANN (Huang et al., 2010; Hung and Wang, 2004; Keramati et al., 2014; Runge et al., 2014), some introduce support vector machine ( Guo-en and Wei-dong, 2008; Vafeiadis et al., 2015; Yu et al., 2011) and some introduce DT (Freund and Schapire, 1996; Qureshi et al., 2013; Umayaparvathi and Iyakutti, 2012) as the best predictor, confusing the users of the results of these studies regarding the best prediction method. The current study identifies the best prediction method specifically in the field of store businesses for researchers and the owners. Moreover, another innovation of the current study is using discriminant analysis for selecting and filtering variables which are important and effective in predicting churners and non-churners, which is not used in previous studies. Therefore, the current study is unique considering the used variables, the method of comparing their accuracy and the method of selecting effective variables.
},
  pages = {65--93},
}

@Article{colchester_survey_2017,
  title = {A {Survey} of {Artificial} {Intelligence} {Techniques} {Employed} for {Adaptive} {Educational} {Systems} within {E}-{Learning} {Platforms}},
  volume = {7},
  issn = {2083-2567},
  url = {https://www.degruyter.com/doi/10.1515/jaiscr-2017-0004},
  doi = {10.1515/jaiscr-2017-0004},
  abstract = {The adaptive educational systems within e-learning platforms are built in response to the fact that the learning process is different for each and every learner. In order to provide adaptive e-learning services and study materials that are tailor-made for adaptive learning, this type of educational approach seeks to combine the ability to comprehend and detect a person’s specific needs in the context of learning with the expertise required to use appropriate learning pedagogy and enhance the learning process. Thus, it is critical to create accurate student profiles and models based upon analysis of their affective states, knowledge level, and their individual personality traits and skills. The acquired data can then be efficiently used and exploited to develop an adaptive learning environment. Once acquired, these learner models can be used in two ways. The first is to inform the pedagogy proposed by the experts and designers of the adaptive educational system. The second is to give the system dynamic self-learning capabilities from the behaviors exhibited by the teachers and students to create the appropriate pedagogy and automatically adjust the e-learning environments to suit the pedagogies. In this respect, artificial intelligence techniques may be useful for several reasons, including their ability to develop and imitate human reasoning and decision-making processes (learning-teaching model) and minimize the sources of uncertainty to achieve an effective learning-teaching context. These learning capabilities ensure both learner and system improvement over the lifelong learning mechanism. In this paper, we present a survey of raised and related topics to the field of artificial intelligence techniques employed for adaptive educational systems within e-learning, their advantages and disadvantages, and a discussion of the importance of using those techniques to achieve more intelligent and adaptive e-learning environments.},
  number = {1},
  urldate = {2020-09-07},
  journal = {Journal of Artificial Intelligence and Soft Computing Research},
  author = {Khalid Colchester and Hani Hagras and Daniyal Alghazzawi and Ghadah Aldabbagh},
  month = {jan},
  year = {2017},
  pages = {47--64},
  file = {Colchester et al_2017_A Survey of Artificial Intelligence Techniques Employed for Adaptive.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\UBV48B4D\\Colchester et al_2017_A Survey of Artificial Intelligence Techniques Employed for Adaptive.pdf:application/pdf},
}

@InProceedings{perianez_churn_2016,
  address = {Montreal, QC, Canada},
  title = {Churn {Prediction} in {Mobile} {Social} {Games}: {Towards} a {Complete} {Assessment} {Using} {Survival} {Ensembles}},
  isbn = {978-1-5090-5206-6},
  shorttitle = {Churn {Prediction} in {Mobile} {Social} {Games}},
  url = {http://ieeexplore.ieee.org/document/7796943/},
  doi = {10.1109/DSAA.2016.84},
  urldate = {2020-09-07},
  booktitle = {2016 {IEEE} {International} {Conference} on {Data} {Science} and {Advanced} {Analytics} ({DSAA})},
  publisher = {IEEE},
  author = {Africa Perianez and Alain Saas and Anna Guitart and Colin Magne},
  month = {oct},
  year = {2016},
  abstract = {Reducing user attrition, i.e. churn, is a broad challenge faced by several industries. In mobile social games, decreasing churn is decisive to increase player retention and rise revenues. Churn prediction models allow to understand player loyalty and to anticipate when they will stop playing a game. Thanks to these predictions, several initiatives can be taken to retain those players who are more likely to churn. Survival analysis focuses on predicting the time of occurrence of a certain event, churn in our case. Classical methods, like regressions, could be applied only when all players have left the game. The challenge arises for datasets with incomplete churning information for all players, as most of them still connect to the game. This is called a censored data problem and is in the nature of churn. Censoring is commonly dealt with survival analysis techniques, but due to the inflexibility of the survival statistical algorithms, the accuracy achieved is often poor. In contrast, novel ensemble learning techniques, increasingly popular in a variety of scientific fields, provide high-class prediction results. In this work, we develop, for the first time in the social games domain, a survival ensemble model which provides a comprehensive analysis together with an accurate prediction of churn. For each player, we predict the probability of churning as function of time, which permits to distinguish various levels of loyalty profiles. Additionally, we assess the risk factors that explain the predicted player survival times. Our results show that churn prediction by survival ensembles significantly improves the accuracy and robustness of traditional analyses, like Cox regression.},
  pages = {564--573},
}

@Article{zhao_optimizing_2016,
  title = {Optimizing security and quality of service in a {Real}-time database system using {Multi}-objective genetic algorithm},
  volume = {64},
  issn = {09574174},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417416303694},
  doi = {10.1016/j.eswa.2016.07.023},
  language = {en},
  urldate = {2020-09-07},
  journal = {Expert Systems with Applications},
  author = {Xuancai Zhao and Qiuzhen Lin and Jianyong Chen and Xiaomin Wang and Jianping Yu and Zhong Ming},
  month = {dec},
  year = {2016},
  abstract = {Both network security and quality of service (QoS) consume computational resource of IT system and thus may evidently affect the application services. In the case of limited computational resource, it is important to model the mutual influence between network security and QoS, which can be concurrently optimized in order to provide a better performance under the available computational resource. In this paper, an evaluation model is accordingly presented to describe the mutual influence of network security and QoS, and then a multi-objective genetic algorithm NSGA-II is revised to optimize the multi-objective model. Using the intrinsic information from the target problem, a new crossover approach is designed to further enhance the optimization performance. Simulation results validate that our algorithm can find a set of Pareto-optimal security policies under different network workloads, which can be provided to the potential users as the differentiated security preferences. These obtained Pareto-optimal security policies not only meet the security requirement of the user, but also provide the optimal QoS under the available computational resource.},
  pages = {11--23},
}

@Article{ma_classification_2016,
  title = {Classification of {Gene} {Expression} {Data} {Using} {Multiobjective} {Differential} {Evolution}},
  volume = {9},
  issn = {1996-1073},
  url = {http://www.mdpi.com/1996-1073/9/12/1061},
  doi = {10.3390/en9121061},
  language = {en},
  number = {12},
  urldate = {2020-09-07},
  journal = {Energies},
  author = {Shijing Ma and Xiangtao Li and Yunhe Wang},
  month = {dec},
  year = {2016},
  pages = {1061},
  abstract = {ene expression data are usually redundant, and only a subset of them presents distinct profiles for different classes of samples. Thus, selecting high discriminative genes from gene expression data has become increasingly interesting in bioinformatics. In this paper, a multiobjective binary differential evolution method (MOBDE) is proposed to select a small subset of informative genes relevant to the classification. In the proposed method, firstly, the Fisher-Markov selector is used to choose top features of gene expression data. Secondly, to make differential evolution suitable for the binary problem, a novel binary mutation method is proposed to balance the exploration and exploitation ability. Thirdly, the multiobjective binary differential evolution is proposed by integrating the summation of normalized objectives and diversity selection into the binary differential evolution algorithm. Finally, the MOBDE algorithm is used for feature selection, and support vector machine (SVM) is used as the classifier with the leave-one-out cross-validation method (LOOCV). In order to show the effectiveness and efficiency of the algorithm, the proposed method is tested on ten gene expression datasets. Experimental results demonstrate that the proposed method is very effective. },
  file = {Ma et al_2016_Classification of Gene Expression Data Using Multiobjective Differential.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\4H4MP9M7\\Ma et al_2016_Classification of Gene Expression Data Using Multiobjective Differential.pdf:application/pdf},
}

@InProceedings{karapinar_churn_2016,
  title = {Churn {Detection} and {Prediction} in {Automotive} {Supply} {Industry}},
  url = {https://fedcsis.org/proceedings/2016/drp/245.html},
  doi = {10.15439/2016F245},
  urldate = {2020-09-07},
  booktitle = {Proceedings of the 2016 Federated Conference on Computer Science and Information Systems},
  author = {Hasan Can Karapınar and Ayca Altay and Gülgün Kayakutlu},
  month = {oct},
  year = {2016},
  pages = {1349--1354},
  abstract = {Companies have both large certified enterprises and small unauthorized service providers as their competitors in the automotive supply industry. As technology related industries undergo more intensive competition, churn detection and prediction become essential to be precautious about leaving customers. The literature for churn detection offers numerous statistical and intelligent methods. In this study, Artificial Neural Networks and Decision Trees are applied to detect the churn in and analyze the validity of these methods for the automotive supply industry. The problem involves both categorical and continuous numerical decision inputs which cannot simultaneously fed into Decision Trees. In this case, continuous inputs should be divided into binary categorical ones by splitting into various intervals which are called buckets. Particle Swarm Optimization algorithm is implemented for finding optimal buckets for the churn problem data. Results indicate that while both algorithms are promising, the bucket tuning for Decision Trees complicate the churn detection process.},
  file = {Karapınar et al_2016_Churn Detection and Prediction in Automotive Supply Industry.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\L6WIUIDD\\Karapınar et al_2016_Churn Detection and Prediction in Automotive Supply Industry.pdf:application/pdf},
}

@InProceedings{mohanty_application_2015,
  address = {Jabalpur, India},
  title = {Application of {Computational} {Intelligence} to {Predict} {Churn} and {Non}-{Churn} of {Customers} in {Indian} {Telecommunication}},
  isbn = {978-1-5090-0076-0},
  url = {http://ieeexplore.ieee.org/document/7546164/},
  doi = {10.1109/CICN.2015.123},
  urldate = {2020-09-07},
  booktitle = {2015 {International} {Conference} on {Computational} {Intelligence} and {Communication} {Networks} ({CICN})},
  publisher = {IEEE},
  author = {Ramakanta Mohanty and K. Jhansi Rani},
  month = {dec},
  year = {2015},
  abstract = {In the modern society, mobile communication became the leading medium of communication. Now the public policies and standardization of mobile communication allows customers to switch from one service provider to another service provider easily. One of the most critical challenges in data and voice telecommunication service industry is retaining customers. The cost of retaining an existing customer is lesser than the cost of getting a new customer. So service providers now shifted their focus from customer acquisition to customer retention. As a result, churn prediction has emerged as the most essential Business Intelligence (BI) application that aims to identify the customers who are about to transfer their service to a competitor i.e. To churn. In this paper, we proposed Counter Propagation Neural Networks (CPNN), Classification and Regression Trees (CART), J48 and fuzzy ARTMAP to predict customer churn and non-churn in telecommunication sector. The dataset analyzed is taken from Indian Telecommunication Service Industry.},
  pages = {598--603},
}

@InProceedings{chidlovskii_domain_2016,
  address = {San Francisco California USA},
  title = {Domain {Adaptation} in the {Absence} of {Source} {Domain} {Data}},
  isbn = {978-1-4503-4232-2},
  url = {https://dl.acm.org/doi/10.1145/2939672.2939716},
  doi = {10.1145/2939672.2939716},
  language = {en},
  urldate = {2020-09-07},
  booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
  publisher = {ACM},
  author = {Boris Chidlovskii and Stephane Clinchant and Gabriela Csurka},
  month = {aug},
  year = {2016},
  abstract = {The overwhelming majority of existing domain adaptation methods makes an assumption of freely available source domain data. An equal access to both source and target data makes it possible to measure the discrepancy between their distributions and to build representations common to both target and source domains. In reality, such a simplifying assumption rarely holds, since source data are routinely a subject of legal and contractual constraints between data owners and data customers. When source domain data can not be accessed, decision making procedures are often available for adaptation nevertheless. These procedures are often presented in the form of classification, identification, ranking etc. rules trained on source data and made ready for a direct deployment and later reuse. In other cases, the owner of a source data is allowed to share a few representative examples such as class means. In this paper we address the domain adaptation problem in real world applications, where the reuse of source domain data is limited to classification rules or a few representative examples. We extend the recent techniques of feature corruption and their marginalization, both in supervised and unsupervised settings. We test and compare them on private and publicly available source datasets and show that significant performance gains can be achieved despite the absence of source data and shortage of labeled target data.},
  pages = {451--460},
}

@Article{xue_survey_2016,
  title = {A {Survey} on {Evolutionary} {Computation} {Approaches} to {Feature} {Selection}},
  volume = {20},
  issn = {1089-778X, 1089-778X, 1941-0026},
  url = {https://ieeexplore.ieee.org/document/7339682/},
  doi = {10.1109/TEVC.2015.2504420},
  number = {4},
  urldate = {2020-09-07},
  journal = {IEEE Transactions on Evolutionary Computation},
  author = {Bing Xue and Mengjie Zhang and Will N. Browne and Xin Yao},
  month = {aug},
  year = {2016},
  pages = {606--626},
  abstract = {Feature selection is an important task in data mining and machine learning to reduce the dimensionality of the data and increase the performance of an algorithm, such as a classification algorithm. However, feature selection is a challenging task due mainly to the large search space. A variety of methods have been applied to solve feature selection problems, where evolutionary computation (EC) techniques have recently gained much attention and shown some success. However, there are no comprehensive guidelines on the strengths and weaknesses of alternative approaches. This leads to a disjointed and fragmented field with ultimately lost opportunities for improving performance and successful applications. This paper presents a comprehensive survey of the state-of-the-art work on EC for feature selection, which identifies the contributions of these different algorithms. In addition, current issues and challenges are also discussed to identify promising areas for future research.},
  file = {Xue et al_2016_A Survey on Evolutionary Computation Approaches to Feature Selection.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\VGV77HZS\\Xue et al_2016_A Survey on Evolutionary Computation Approaches to Feature Selection.pdf:application/pdf},
}

@Article{karakaya_identifying_2016,
  title = {Identifying ({Quasi}) {Equally} {Informative} {Subsets} in {Feature} {Selection} {Problems} for {Classification}: {A} {Max}-{Relevance} {Min}-{Redundancy} {Approach}},
  volume = {46},
  issn = {2168-2267, 2168-2275},
  shorttitle = {Identifying ({Quasi}) {Equally} {Informative} {Subsets} in {Feature} {Selection} {Problems} for {Classification}},
  url = {http://ieeexplore.ieee.org/document/7150365/},
  doi = {10.1109/TCYB.2015.2444435},
  number = {6},
  urldate = {2020-09-07},
  journal = {IEEE Transactions on Cybernetics},
  author = {Gulsah Karakaya and Stefano Galelli and Selin Damla Ahipasaoglu and Riccardo Taormina},
  month = {jun},
  year = {2016},
  abstract = {An emerging trend in feature selection is the development of two-objective algorithms that analyze the tradeoff between the number of features and the classification performance of the model built with these features. Since these two objectives are conflicting, a typical result stands in a set of Pareto-efficient subsets, each having a different cardinality and a corresponding discriminating power. However, this approach overlooks the fact that, for a given cardinality, there can be several subsets with similar information content. The study reported here addresses this problem, and introduces a novel multiobjective feature selection approach conceived to identify: 1) a subset that maximizes the performance of a given classifier and 2) a set of subsets that are quasi equally informative, i.e., have almost same classification performance, to the performance maximizing subset. The approach consists of a wrapper [Wrapper for Quasi Equally Informative Subset Selection (W-QEISS)] built on the formulation of a four-objective optimization problem, which is aimed at maximizing the accuracy of a classifier, minimizing the number of features, and optimizing two entropy-based measures of relevance and redundancy. This allows conducting the search in a larger space, thus enabling the wrapper to generate a large number of Pareto-efficient solutions. The algorithm is compared against the mRMR algorithm, a two-objective wrapper and a computationally efficient filter [Filter for Quasi Equally Informative Subset Selection (F-QEISS)] on 24 University of California, Irvine, (UCI) datasets including both binary and multiclass classification. Experimental results show that W-QEISS has the capability of evolving a rich and diverse set of Pareto-efficient solutions, and that their availability helps in: 1) studying the tradeoff between multiple measures of classification performance and 2) understanding the relative importance of each feature. The quasi equally informative subsets are ide...},
  pages = {1424--1437},
}

@Article{erlandsson_finding_2016,
  title = {Finding {Influential} {Users} in {Social} {Media} {Using} {Association} {Rule} {Learning}},
  volume = {18},
  issn = {1099-4300},
  url = {http://www.mdpi.com/1099-4300/18/5/164},
  doi = {10.3390/e18050164},
  language = {en},
  number = {5},
  urldate = {2020-09-07},
  journal = {Entropy},
  author = {Fredrik Erlandsson and Piotr Bródka and Anton Borg and Henric Johnson},
  month = {apr},
  year = {2016},
  pages = {164},
  abstract = {Influential users play an important role in online social networks since users tend to have an impact on one other. Therefore, the proposed work analyzes users and their behavior in order to identify influential users and predict user participation. Normally, the success of a social media site is dependent on the activity level of the participating users. For both online social networking sites and individual users, it is of interest to find out if a topic will be interesting or not. In this article, we propose association learning to detect relationships between users. In order to verify the findings, several experiments were executed based on social network analysis, in which the most influential users identified from association rule learning were compared to the results from Degree Centrality and Page Rank Centrality. The results clearly indicate that it is possible to identify the most influential users using association rule learning. In addition, the results also indicate a lower execution time compared to state-of-the-art methods.},
  file = {Erlandsson et al_2016_Finding Influential Users in Social Media Using Association Rule Learning.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\YSW7BZVN\\Erlandsson et al. - 2016 - Finding Influential Users in Social Media Using As.pdf:application/pdf},
}

@InProceedings{adaji_predicting_2015,
  address = {Miami, FL, USA},
  title = {Predicting {Churn} of {Expert} {Respondents} in {Social} {Networks} {Using} {Data} {Mining} {Techniques}: {A} {Case} {Study} of {Stack} {Overflow}},
  isbn = {978-1-5090-0287-0},
  shorttitle = {Predicting {Churn} of {Expert} {Respondents} in {Social} {Networks} {Using} {Data} {Mining} {Techniques}},
  url = {http://ieeexplore.ieee.org/document/7424306/},
  doi = {10.1109/ICMLA.2015.120},
  urldate = {2020-09-07},
  booktitle = {2015 {IEEE} 14th {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
  publisher = {IEEE},
  author = {Ifeoma Adaji and Julita Vassileva},
  month = {dec},
  year = {2015},
  abstract = {In Q&A social networks, the few respondents that answer most of the questions are an asset to that network. Being able to predict the churn of these expert respondents will enable the owners of such network put things in place in order to keep them. In this paper, we predicted the churn of expert respondents in Stack Overflow. We identified experts based on the InDegree of the respondents and the value of the incentives earned by these experts from the questions they have answered in the past. Using four data mining techniques: logistic regression, neural networks, support vector machines and random forests, we predicted user churn and evaluated our results with four evaluation metrics: percentage correctly classified, area under receiver operating characteristic curve, precision and recall. Of the four data mining algorithms, random forests performed best with PCC of 76%, ROC area of 0.82, precision of 0.76 and recall of 0.77.},
  pages = {182--189},
}

@Article{ballings_social_2016,
  title = {Social media optimization: {Identifying} an optimal strategy for increasing network size on {Facebook}},
  volume = {59},
  issn = {03050483},
  shorttitle = {Social media optimization},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0305048315001176},
  doi = {10.1016/j.omega.2015.04.017},
  language = {en},
  urldate = {2020-09-07},
  journal = {Omega},
  author = {Michel Ballings and Dirk {Van den Poel} and Matthias Bogaert},
  month = {mar},
  year = {2016},
  abstract = {This paper aims to create an expert system that yields an optimal strategy for increasing network size on Facebook. Data were obtained from 5488 Facebook users by means of a custom-built Facebook application. We computed a total of 426 variables. Using these data we estimated a predictive model of network size which is subsequently used in a prescriptive model. The former is estimated with Random Forest and the latter with a Genetic Algorithm. The results indicate that the proposed expert system can identify an optimal social media strategy. The system delivers concrete recommendations about, for example, the optimal time between status updates. The analysis reveals that network size can be increased by 61% if the optimal strategy is adopted. This study contributes to literature in the following two ways. First it devises a novel prescriptive social media expert system relying on an unprecedented variety of social media data. The results indicate that the system is effective and a viable strategic tool for increasing network size. Second it provides a list of the top drivers allowing future research to build similar systems efficiently.},
  pages = {15--25},
}

@Article{bogaert_added_2016,
  title = {The added value of {Facebook} friends data in event attendance prediction},
  volume = {82},
  issn = {01679236},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167923615002122},
  doi = {10.1016/j.dss.2015.11.003},
  language = {en},
  urldate = {2020-09-07},
  journal = {Decision Support Systems},
  author = {Matthias Bogaert and Michel Ballings and Dirk {Van den Poel}},
  month = {feb},
  year = {2016},
  abstract = {This paper seeks to assess the added value of a Facebook user's friends data in event attendance prediction over and above user data. For this purpose we gathered data of users that have liked an anonymous European soccer team on Facebook. In addition we obtained data from all their friends. In order to assess the added value of friends data we have built two models for five different algorithms (Logistic Regression, Random Forest, Adaboost, Neural Networks and Naive Bayes). The baseline model contained only user data and the augmented model contained both user and friends data. We employed five times two-fold cross-validation and the Wilcoxon signed rank test to validate our findings. The results suggest that the inclusion of friends data in our predictive model increases the area under the receiver operating characteristic curve (AUC). Out of five algorithms, the increase is significant for three algorithms, marginally significant for one algorithm, and not significant for one algorithm. The increase in AUC ranged from 0.21%-points to 0.82%-points. The analyses show that a top predictor is the number of friends that are attending the focal event. To the best of our knowledge this is the first study that evaluates the added value of friends network data over and above user data in event attendance prediction on Facebook. These findings clearly indicate that including network data in event prediction models is a viable strategy for improving model performance.},
  pages = {26--34},
}

@Article{tsai_intangible_2016,
  title = {Intangible assets evaluation: {The} machine learning perspective},
  volume = {175},
  issn = {09252312},
  shorttitle = {Intangible assets evaluation},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231215015003},
  doi = {10.1016/j.neucom.2015.10.041},
  language = {en},
  urldate = {2020-09-07},
  journal = {Neurocomputing},
  author = {Chih-Fong Tsai and Yu-Hsin Lu and Yu-Chung Hung and David C. Yen},
  month = {jan},
  year = {2016},
  abstract = {The lack of regulations and disclosures regarding intangible capital has made it rather difficult for investors and creditors to evaluate a firm׳s intangible value before making the associated investment and loan decisions. This study represents an initial attempt to compare/contrast different types of machine learning techniques and identify the optimal prediction model for intangible assets. In addition, this paper shows that machine learning can be used effectively for the problem of intangible assets evaluation. To be specific, five classification algorithms are considered: decision trees (DT), artificial neural networks (ANN), naïve Bayes, support vector machines (SVM) and k-Nearest Neighbors (k-NN). Consequently, thirty prediction models are constructed for comparison, including five single classifiers, boosting and bagging based classifier ensembles, and the combination of k-means clustering, single classifiers and classifier ensembles. The experimental results show that prediction models combining k-means with boosting/bagging based classifier ensembles perform much better than the other methods in terms of prediction accuracy, ROC Curve, as well as Type I and II errors. In particular, while the best single classifier, k-NN provides 78.24% prediction accuracy, k-means+bagging based DT ensembles provide the best performance to predict intangible assets with a prediction accuracy of 91.60%, 96.40% of ROC Curve and 18.65% of Type I and 6.34% of II errors, respectively.},
  pages = {110--120},
}

@Article{tamaddoni_comparing_2016,
  title = {Comparing {Churn} {Prediction} {Techniques} and {Assessing} {Their} {Performance}: {A} {Contingent} {Perspective}},
  volume = {19},
  issn = {1094-6705, 1552-7379},
  shorttitle = {Comparing {Churn} {Prediction} {Techniques} and {Assessing} {Their} {Performance}},
  url = {http://journals.sagepub.com/doi/10.1177/1094670515616376},
  doi = {10.1177/1094670515616376},
  language = {en},
  number = {2},
  urldate = {2020-09-07},
  journal = {Journal of Service Research},
  author = {Ali Tamaddoni and Stanislav Stakhovych and Michael Ewing},
  month = {may},
  year = {2016},
  abstract = {Customer retention has become a focal priority. However, the process of implementing an effective retention campaign is complex and dependent on firms’ ability to accurately identify both at-risk customers and those worth retaining. Drawing on empirical and simulated data from two online retailers, we evaluate the performance of several parametric and nonparametric churn prediction techniques, in order to identify the optimal modeling approach, dependent on context. Results show that under most circumstances (i.e., varying sample sizes, purchase frequencies, and churn ratios), the boosting technique, a nonparametric method, delivers superior predictability. Furthermore, in cases/contexts where churn is more rare, logistic regression prevails. Finally, wh},
  pages = {123--141},
}

@InProceedings{esteves_churn_2016,
  address = {Porto, Portugal},
  title = {Churn perdiction in the telecom business},
  isbn = {978-1-5090-2641-8},
  url = {http://ieeexplore.ieee.org/document/7829775/},
  doi = {10.1109/ICDIM.2016.7829775},
  urldate = {2020-09-07},
  booktitle = {2016 {Eleventh} {International} {Conference} on {Digital} {Information} {Management} ({ICDIM})},
  publisher = {IEEE},
  author = {Georgina Esteves and Joao Mendes-Moreira},
  month = {sep},
  year = {2016},
  abstract = {elecommunication companies are acknowledging the existing connection between customer satisfaction and company revenues. Customer churn in telecom refers to a customer that ceases his relationship with a company. Churn prediction in telecom has recently gained substantial interest of stakeholders, who noticed that retaining a customer is substantially cheaper that gaining a new one. This research compares six approaches using different algorithms that identify the clients who are closer to abandon their telecom provider. Those algorithms are: KNN, Naive Bayes, C4.5, Random Forest, Ada Boost and ANN. The use of real data provided by We Do technologies extended the refinement time necessary, but ensured that the developed algorithm and model can be applied to real world situations. The models are evaluated according to three criteria: are under curve, sensitivity and specificity, with special weight to the first two criteria. The Random Forest algorithm proved to be the most adequate in all the test cases.},
  pages = {254--259},
}

@InCollection{wierzbicki_predicting_2016,
  address = {Cham},
  title = {Predicting {User} {Participation} in {Social} {Media}},
  volume = {9564},
  isbn = {978-3-319-28360-9 978-3-319-28361-6},
  url = {http://link.springer.com/10.1007/978-3-319-28361-6_10},
  urldate = {2020-09-07},
  booktitle = {Advances in {Network} {Science}},
  publisher = {Springer International Publishing},
  author = {Fredrik Erlandsson and Anton Borg and Henric Johnson and Piotr Bródka},
  editor = {Adam Wierzbicki and Ulrik Brandes and Frank Schweitzer and Dino Pedreschi},
  year = {2016},
  doi = {10.1007/978-3-319-28361-6_10},
  note = {Series Title: Lecture Notes in Computer Science},
  pages = {126--135},
  abstract = {Online social networking services like Facebook provides a popular way for users to participate in different communication groups and discuss relevant topics with each other. While users tend to have an impact on each other, it is important to better understand and analyze users behavior in specific online groups. For social networking sites it is of interest to know if a topic will be interesting for users or not. Therefore, this study examines the prediction of user participation in online social networks discussions, in which we argue that it is possible to predict user participation in a public group using common machine learning techniques. We are predicting user participation based on association rules built with respect to user activeness of current posts. In total, we have crawled and extracted 2,443 active users interacting on 610 posts with over 14,117 comments on Facebook. The results show that the proposed approach has a high level of accuracy and the systematic study clearly depicts the possibility to predict user participation in social networking sites.},
  file = {Erlandsson et al_2016_Predicting User Participation in Social Media.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\3GNMUR5Y\\Erlandsson et al_2016_Predicting User Participation in Social Media.pdf:application/pdf},
}

@Article{gerpott_regaining_2015,
  title = {Regaining drifting mobile communication customers: {Predicting} the odds of success of winback efforts with competing risks regression},
  volume = {42},
  issn = {09574174},
  shorttitle = {Regaining drifting mobile communication customers},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417415003310},
  doi = {10.1016/j.eswa.2015.05.011},
  language = {en},
  number = {21},
  urldate = {2020-09-07},
  journal = {Expert Systems with Applications},
  author = {Torsten J. Gerpott and Nima Ahmadi},
  month = {nov},
  year = {2015},
  abstract = {Mobile network operators (MNOs) make considerable efforts to reduce customer defection (= churn) by trying to motivate customers who announced to cancel their contract at the next legally possible date to withdraw their notification and to sign another postpaid contract or at least to accept a prepaid offer in case that they are unwilling to completely revoke their cancellation. Nevertheless, empirical evidence on factors significantly associated with the odds of success of an MNO’s reactive winback attempt is scarce. As a consequence, this study explores the capability of socio-demographic, contract and service usage characteristics of MNO subscribers as well as their stated primary reason for contract termination to predict the likelihood of a fully successful winback at the individual customer level. In a sample of 305,466 postpaid residential customers of the German subsidiary of a multinational MNO, competing risks regression analysis suggests that younger customers with above average service usage levels who were already in a tariff plan bundling mobile voice and Internet access services, yet had received a subsidized device from their current provider in the past, had not originally signed their contract in the firm’s own outlets and stated they cancelled their contract as a precautionary move or due to tariff level/structure reasons exhibit the highest prospects of full restoration. Moreover, the analysis reveals that the covariates studied and the competing risks regression technique achieve a satisfactory performance in predicting the outcome of the MNO’s customer winback efforts. Results are discussed in terms of basic entry points of MNOs for improving both their reactive winback as well as their new customer acquisition strategies.},
  pages = {7917--7928},
}

@Article{ballings_evaluating_2015,
  title = {Evaluating multiple classifiers for stock price direction prediction},
  volume = {42},
  issn = {09574174},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417415003334},
  doi = {10.1016/j.eswa.2015.05.013},
  language = {en},
  number = {20},
  urldate = {2020-09-07},
  journal = {Expert Systems with Applications},
  author = {Michel Ballings and Dirk {Van den Poel} and Nathalie Hespeels and Ruben Gryp},
  month = {nov},
  year = {2015},
  abstract = {Stock price direction prediction is an important issue in the financial world. Even small improvements in predictive performance can be very profitable. The purpose of this paper is to benchmark ensemble methods (Random Forest, AdaBoost and Kernel Factory) against single classifier models (Neural Networks, Logistic Regression, Support Vector Machines and K-Nearest Neighbor). We gathered data from 5767 publicly listed European companies and used the area under the receiver operating characteristic curve (AUC) as a performance measure. Our predictions are one year ahead. The results indicate that Random Forest is the top algorithm followed by Support Vector Machines, Kernel Factory, AdaBoost, Neural Networks, K-Nearest Neighbors and Logistic Regression. This study contributes to literature in that it is, to the best of our knowledge, the first to make such an extensive benchmark. The results clearly suggest that novel studies in the domain of stock price direction prediction should include ensembles in their sets of algorithms. Our extensive literature review evidently indicates that this is currently not the case.},
  pages = {7046--7056},
}

@Article{coletta_using_2015,
  title = {Using metaheuristics to optimize the combination of classifier and cluster ensembles},
  volume = {22},
  issn = {10692509, 18758835},
  url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/ICA-150485},
  doi = {10.3233/ICA-150485},
  number = {3},
  urldate = {2020-09-07},
  journal = {Integrated Computer-Aided Engineering},
  author = {Luiz F.S. Coletta and Eduardo R. Hruschka and Ayan Acharya and Joydeep Ghosh},
  month = {jun},
  year = {2015},
  abstract = {We investigate how to make a simpler version of an existing algorithm, named C^3E, from Consensus between Classification and Clustering Ensembles, more user-friendly by automatically tuning its main parameters with the use of metaheuristics. In particular, C^{3}E based on a Squared Loss function, C^{3}E-SL, assumes an optimization procedure that takes as input class membership estimates from existing classifiers, as well as a similarity matrix from a cluster ensemble operating solely on the new target data, to provide a consolidated classification of the target data. To do so, two parameters have to be defined a priori, namely: the relative importance of classifier and cluster ensembles and the number of iterations of the algorithm. In some practical applications, these parameters can be optimized via time consuming grid search approaches based on cross-validation procedures. This paper shows that seven metaheuristics for parameter optimization yield classifiers as accurate as those obtained from grid search, but taking half the running time. More precisely, and by assuming a trade-off between user-friendliness and accuracy, experiments performed on twenty real-world datasets suggest that CMA-ES, DE, and SaDE are the best alternatives to optimize the C^{3}E-SL parameters. },
  pages = {229--242},
}

@Article{ballings_crm_2015,
  title = {{CRM} in social media: {Predicting} increases in {Facebook} usage frequency},
  volume = {244},
  issn = {03772217},
  shorttitle = {{CRM} in social media},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0377221715000028},
  doi = {10.1016/j.ejor.2015.01.001},
  language = {en},
  number = {1},
  urldate = {2020-09-07},
  journal = {European Journal of Operational Research},
  author = {Michel Ballings and Dirk {Van den Poel}},
  month = {jul},
  year = {2015},
  abstract = {The purpose of this study is to (1) assess the feasibility of predicting increases in Facebook usage frequency, (2) evaluate which algorithms perform best, (3) and determine which predictors are most important. We benchmark the performance of Logistic Regression, Random Forest, Stochastic Adaptive Boosting, Kernel Factory, Neural Networks and Support Vector Machines using five times twofold cross-validation. The results indicate that it is feasible to create models with high predictive performance. The top performing algorithm was Stochastic Adaptive Boosting with a cross-validated AUC of 0.66 and accuracy of 0.74. The most important predictors include deviation from regular usage patterns, frequencies of likes of specific categories and group memberships, average photo album privacy settings, and recency of comments. Facebook and other social networks alike could use predictions of increases in usage frequency to customize its services such as pacing the rate of advertisements and friend recommendations, or adapting News Feed content altogether. The main contribution of this study is that it is the first to assess the prediction of increases in usage frequency in a social network.},
  pages = {248--260},
}

@Article{xiao_feature-selection-based_2015,
  title = {Feature-selection-based dynamic transfer ensemble model for customer churn prediction},
  volume = {43},
  issn = {0219-1377, 0219-3116},
  url = {http://link.springer.com/10.1007/s10115-013-0722-y},
  doi = {10.1007/s10115-013-0722-y},
  language = {en},
  number = {1},
  urldate = {2020-09-07},
  journal = {Knowledge and Information Systems},
  author = {Jin Xiao and Yi Xiao and Anqiang Huang and Dunhu Liu and Shouyang Wang},
  month = {apr},
  year = {2015},
  abstract = {Customer churn prediction is one of the key steps to maximize the value of customers for an enterprise. It is difficult to get satisfactory prediction effect by traditional models constructed on the assumption that the training and test data are subject to the same distribution, because the customers usually come from different districts and may be subject to different distributions in reality. This study proposes a feature-selection-based dynamic transfer ensemble (FSDTE) model that aims to introduce transfer learning theory for utilizing the customer data in both the target and related source domains. The model mainly conducts a two-layer feature selection. In the first layer, an initial feature subset is selected by GMDH-type neural network only in the target domain. In the second layer, several appropriate patterns from the source domain to target training set are selected, and some features with higher mutual information between them and the class variable are combined with the initial subset to construct a new feature subset. The selection in the second layer is repeated several times to generate a series of new feature subsets, and then, we train a base classifier in each one. Finally, a best base classifier is selected dynamically for each test pattern. The experimental results in two customer churn prediction datasets show that FSDTE can achieve better performance compared with the traditional churn prediction strategies, as well as three existing transfer learning strategies.},
  pages = {29--51},
}

@Article{moeyersoms_including_2015,
  title = {Including high-cardinality attributes in predictive models: {A} case study in churn prediction in the energy sector},
  volume = {72},
  issn = {01679236},
  shorttitle = {Including high-cardinality attributes in predictive models},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167923615000275},
  doi = {10.1016/j.dss.2015.02.007},
  language = {en},
  urldate = {2020-09-07},
  journal = {Decision Support Systems},
  author = {Julie Moeyersoms and David Martens},
  month = {apr},
  year = {2015},
  abstract = {High-cardinality attributes are categorical attributes that contain a very large number of distinct values, like for example: family names, ZIP codes or bank account numbers. Within a predictive modeling setting, such features could be highly informative as it might be useful to know that people live in the same village or pay with the same bank account number. Despite this notable and intuitive advantage, high-cardinality attributes are rarely used in predictive modeling. The main reason for this is that including these attributes by using traditional transformation methods is either impossible due to anonymization of the data (when using semantic grouping of the values) or will vastly increase the dimensionality of the data set (when using dummy encoding), thereby making it difficult or even impossible for most classification techniques to build prediction models. The main contributions of this work are (1) the introduction of several possible transformation functions coming from different domains and contexts, that allow the inclusion of high-cardinality features in predictive models. (2) Using a unique data set of a large energy company with more than 1 million customers, we show that adding such features indeed improves the predictive performance of the model significantly. Moreover, (3) we empirically demonstrate that having more data leads to better prediction models, which is not observed for “traditional” data. As such, we also contribute to the area of big data analytics.},
  pages = {72--81},
}

@Article{bose_detecting_2015,
  title = {Detecting the migration of mobile service customers using fuzzy clustering},
  volume = {52},
  issn = {03787206},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0378720614001359},
  doi = {10.1016/j.im.2014.11.001},
  language = {en},
  number = {2},
  urldate = {2020-09-07},
  journal = {Information \& Management},
  author = {Indranil Bose and Xi Chen},
  month = {mar},
  year = {2015},
  abstract = {Customer clustering is used to build customer profiles which make up the core of a customer centric information system. In this paper, we develop a method for extending the standard fuzzy c-means clustering algorithm using membership functions to detect how customers move between clusters over time. The study leads to the discovery of new usage and revenue patterns for customers, the identification of two groups of customers that exhibit migratory behavior over time, and the determination of specific usage and revenue attributes that impact customer migration. The findings provide insights to mobile services providers about how to detect temporal changes in customer behavior.},
  pages = {227--238},
}

@Article{sanchez_priori-knowledgeactor-critic_2015,
  title = {A priori-knowledge/actor-critic reinforcement learning architecture for computing the mean–variance customer portfolio: {The} case of bank marketing campaigns},
  volume = {46},
  issn = {09521976},
  shorttitle = {A priori-knowledge/actor-critic reinforcement learning architecture for computing the mean–variance customer portfolio},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0952197615001980},
  doi = {10.1016/j.engappai.2015.08.011},
  language = {en},
  urldate = {2020-09-07},
  journal = {Engineering Applications of Artificial Intelligence},
  author = {Emma M. Sánchez and Julio B. Clempner and Alexander S. Poznyak},
  month = {nov},
  year = {2015},
  abstract = {In this paper we propose a novel recurrent reinforcement learning approach for controllable Markov chains that adjusts its policies according to a preprocessing and an actor-critic architecture. The preprocessing is proposed when learning a new task is needed from reinforcement based on a priori knowledge, in order to decrease computation time and not explore and not learn everything from scratch. The actor-critic architecture is based on an iterated quadratic/Lagrange programming maximization algorithm for computing the optimal strategies of the mean–variance customer portfolio. This process can be viewed as a specific form of asynchronous value iteration with optimized computational properties. The use of only the value-maximizing action at each state is unlikely in practice. Then, a specific selection of policies is used to ensure convergence. The reinforcement model proposed predicts a learning process that takes the risk of the customer portfolio into account. The resulting policies dynamically optimize the customer portfolio. We propose to apply three different learning rules, based on the transition matrices, the utilities and the costs, to estimate the objective function for the current policies. In particular, the learning rule related to estimate the real costs imposes restrictions over the formulation of the portfolio: costs cannot be underestimated or overestimated. The learning rules allow the process to make use of past experiences and decide on future actions to take in or around a given state of the Markov chain. We provide implementation details of the learning process and the complete algorithm. In addition, we illustrate our approach with a bank marketing application example for showing the viability of the model for solving realistic problems.},
  pages = {82--92},
}

@Article{guo_estimating_2015,
  title = {Estimating {Social} {Influences} from {Social} {Networking} {Sites}-{Articulated} {Friendships} versus {Communication} {Interactions}: {Social} {Influences} from {Social} {Networking} {Sites}},
  volume = {46},
  issn = {00117315},
  shorttitle = {Estimating {Social} {Influences} from {Social} {Networking} {Sites}-{Articulated} {Friendships} versus {Communication} {Interactions}},
  url = {http://doi.wiley.com/10.1111/deci.12118},
  doi = {10.1111/deci.12118},
  language = {en},
  number = {1},
  urldate = {2020-09-07},
  journal = {Decision Sciences},
  author = {Hong Guo and Praveen Pathak and Hsing Kenneth Cheng},
  month = {feb},
  year = {2015},
  abstract = {Despite the ubiquity of social networking sites, the online social networking industry is in search of effective marketing strategies to better profit from their established user base. Social media marketing strategies build on the premise that the social network of online users can be predicted and social influences among online users can be estimated. However, the existence of various heterogeneous social interactions on social networking sites presents a challenge for social network prediction and social influence estimation. In this article we draw upon the literatures on self‐presentation on social networking sites and signaling in online social networking to categorize six heterogeneous online social interactions on social networking sites into two types—articulated friendships and communication interactions. This article provides empirical evidence for the differences between articulated friendships and communication interactions and the corresponding articulated and communication networks. In order to compare the impacts of the social influences based on these two networks, we utilize support vector machines to build a classifier to predict virtual community membership and we further estimate the marginal effects of these social influences using a two‐stage probit least squares method. We find significant explanatory power of social influences in predicting virtual community membership. Although the communication network is much sparser than the articulated network, social influences based on the communication network achieve similar performance as the articulated network. These findings provide important implications for social media marketing as well as the management of virtual communities.},
  pages = {135--163},
}

@Article{akhondzadeh-noughabi_mining_2015,
  title = {Mining the dominant patterns of customer shifts between segments by using top- \textit{k} and distinguishing sequential rules},
  volume = {53},
  issn = {0025-1747},
  url = {https://www.emerald.com/insight/content/doi/10.1108/MD-09-2014-0551/full/html},
  doi = {10.1108/MD-09-2014-0551},
  language = {en},
  number = {9},
  urldate = {2020-09-07},
  journal = {Management Decision},
  author = {Elham Akhondzadeh-Noughabi and Amir Albadvi},
  month = {oct},
  year = {2015},
  abstract = {Purpose: The purpose of this paper is to detect different behavioral groups and the dominant patterns of customer shifts between segments of different values over time. Design/methodology/approach: A new hybrid methodology is presented based on clustering techniques and mining top-k and distinguishing sequential rules. This methodology is implemented on the data of 14,772 subscribers of a mobile phone operator in Tehran, the capital of Iran. The main data include the call detail records and event detail records data that was acquired from the IT department of this operator. Findings: Seven different behavioral groups of customer shifts were identified. These groups and the corresponding top-k rules represent the dominant patterns of customer behavior. The results also explain the relation of customer switching behavior and segment instability, which is an open problem. Practical implications: The findings can be helpful to improve marketing strategies and decision making and for prediction purposes. The obtained rules are relatively easy to interpret and use; this can strengthen the practicality of results. Originality/value: A new hybrid methodology is proposed that systematically extracts the dominant patterns of customer shifts. This paper also offers a new definition and framework for discovering distinguishing sequential rules. Comparing with Markov chain models, this study captures the customer switching behavior in different levels of value through interpretable sequential rules. This is the first study that uses sequential and distinguishing rules in this domain. },
  pages = {1976--2003},
}

@Article{tsai_performance_2015,
  title = {Performance {Prediction} and {Sensitivity} {Analysis} of {SAW} {Gas} {Sensors}},
  volume = {3},
  issn = {2169-3536},
  url = {http://ieeexplore.ieee.org/document/7272035/},
  doi = {10.1109/ACCESS.2015.2478789},
  urldate = {2020-09-07},
  journal = {IEEE Access},
  author = {Jinn-Tsong Tsai and Kai-Yu Chiu and Jyh-Horng Chou},
  year = {2015},
  pages = {1614--1619},
  abstract = {An improved adaptive neuro-fuzzy inference system (IANFIS) is proposed to build a model to predict the resonant frequency shift performance of surface acoustic wave (SAW) gas sensors. In the proposed IANFIS, by directly minimizing the root-mean-squared-error performance criterion, the Taguchi-genetic learning algorithm is used in the ANFIS to find both the optimal premise and consequent parameters and to simultaneously determine the most suitable membership functions. The five design parameters of SAW gas sensors are considered to be the input variables of the IANFIS model. The input variables include the number of electrode finger pairs, the electrode overlap, the separation distance of two interdigital transducers on the substrate, the dimensions of the stable temperature-cut (ST-cut) quartz substrate, and the electrode thickness. The output variable of the IANFIS model is composed of the resonant frequency shift performance. The results predicted by the proposed IANFIS are compared with those obtained by the back-propagation neural network. The comparison has shown that the performance prediction of resonant frequency shift using the proposed IANFIS is effective. In addition, the sensitivity analyses of the five design parameters have also shown that both the electrode overlap and the dimensions of the ST-cut quartz substrate have the most influence on the resonant frequency shift performance.},
  file = {Tsai et al_2015_Performance Prediction and Sensitivity Analysis of SAW Gas Sensors.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\9FUMMRP4\\Tsai et al_2015_Performance Prediction and Sensitivity Analysis of SAW Gas Sensors.pdf:application/pdf},
}

@Article{gur_ali_dynamic_2014,
  title = {Dynamic churn prediction framework with more effective use of rare event data: {The} case of private banking},
  volume = {41},
  issn = {09574174},
  shorttitle = {Dynamic churn prediction framework with more effective use of rare event data},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417414003595},
  doi = {10.1016/j.eswa.2014.06.018},
  language = {en},
  number = {17},
  urldate = {2020-09-07},
  journal = {Expert Systems with Applications},
  author = {Özden {Gür Ali} and Umut Arıtürk},
  month = {dec},
  year = {2014},
  abstract = {Customer churn prediction literature has been limited to modeling churn in the next (feasible) time period. On the other hand, lead time specific churn predictions can help businesses to allocate retention efforts across time, as well as customers, and identify early triggers and indicators of customer churn. We propose a dynamic churn prediction framework for generating training data from customer records, and leverage it for predicting customer churn within multiple horizons using standard classifiers. Further, we empirically evaluate the proposed approach in a case study about private banking customers in a European bank. The proposed framework includes customer observations from different time periods, and thus addresses the absolute rarity issue that is relevant for the most valuable customer segment of many companies. It also increases the sampling density in the training data and allows the models to generalize across behaviors in different time periods while incorporating the impact of the environmental drivers. As a result, this framework significantly increases the prediction accuracy across prediction horizons compared to the standard approach of one observation per customer; even when the standard approach is modified with oversampling to balance the data, or lags of customer behavior features are added as additional predictors. The proposed approach to dynamic churn prediction involves a set of independently trained horizon-specific binary classifiers that use the proposed dataset generation framework. In the absence of predictive dynamic churn models, we had to benchmark survival analysis which is used predominantly as a descriptive tool. The proposed method outperforms survival analysis in terms of predictive accuracy for all lead times, with a much lower variability. Further, unlike Cox regression, it provides horizon specific ranking of customers in terms of churn probability which allows allocation of retention efforts across customers and time periods.},
  pages = {7889--7903},
}

@Article{lee_comparative_2014,
  title = {Comparative {Study} of {Dimension} {Reduction} {Methods} for {Highly} {Imbalanced} {Overlapping} {Churn} {Data}},
  volume = {13},
  issn = {1598-7248},
  url = {http://koreascience.or.kr/journal/view.jsp?kj=SGHHEA&py=2014&vnc=v13n4&sp=454},
  doi = {10.7232/iems.2014.13.4.454},
  language = {en},
  number = {4},
  urldate = {2020-09-07},
  journal = {Industrial Engineering and Management Systems},
  author = {Sujee Lee and Bonhyo Koo and Kyu-Hwan Jung},
  month = {dec},
  year = {2014},
  pages = {454--462},
  abstract = {Retention of possible churning customer is one of the most important issues in customer relationship management, so companies try to predict churn customers using their large-scale high-dimensional data. This study focuses on dealing with large data sets by reducing the dimensionality. By using six different dimension reduction methods-Principal Component Analysis (PCA), factor analysis (FA), locally linear embedding (LLE), local tangent space alignment (LTSA), locally preserving projections (LPP), and deep auto-encoder-our experiments apply each dimension reduction method to the training data, build a classification model using the mapped data and then measure the performance using hit rate to compare the dimension reduction methods. In the result, PCA shows good performance despite its simplicity, and the deep auto-encoder gives the best overall performance. These results can be explained by the characteristics of the churn prediction data that is highly correlated and overlapped over the classes. We also proposed a simple out-of-sample extension method for the nonlinear dimension reduction methods, LLE and LTSA, utilizing the characteristic of the data. },
  file = {Lee et al_2014_Comparative Study of Dimension Reduction Methods for Highly Imbalanced.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\FC729IUV\\Lee et al_2014_Comparative Study of Dimension Reduction Methods for Highly Imbalanced.pdf:application/pdf},
}

@Article{farquad_churn_2014,
  title = {Churn prediction using comprehensible support vector machine: {An} analytical {CRM} application},
  volume = {19},
  issn = {15684946},
  shorttitle = {Churn prediction using comprehensible support vector machine},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1568494614000507},
  doi = {10.1016/j.asoc.2014.01.031},
  language = {en},
  urldate = {2020-09-07},
  journal = {Applied Soft Computing},
  author = {M.A.H. Farquad and Vadlamani Ravi and S. Bapi Raju},
  month = {jun},
  year = {2014},
  abstract = {Support vector machine (SVM) is currently state-of-the-art for classification tasks due to its ability to model nonlinearities. However, the main drawback of SVM is that it generates “black box” model, i.e. it does not reveal the knowledge learnt during training in human comprehensible form. The process of converting such opaque models into a transparent model is often regarded as rule extraction. In this paper we proposed a hybrid approach for extracting rules from SVM for customer relationship management (CRM) purposes. The proposed hybrid approach consists of three phases. (i) During first phase; SVM-RFE (SVM-recursive feature elimination) is employed to reduce the feature set. (ii) Dataset with reduced features is then used in the second phase to obtain SVM model and support vectors are extracted. (iii) Rules are then generated using Naive Bayes Tree (NBTree) in the final phase. The dataset analyzed in this research study is about Churn prediction in bank credit card customer (Business Intelligence Cup 2004) and it is highly unbalanced with 93.24% loyal and 6.76% churned customers. Further we employed various standard balancing approaches to balance the data and extracted rules. It is observed from the empirical results that the proposed hybrid outperformed all other techniques tested. As the reduced feature dataset is used, it is also observed that the proposed approach extracts smaller length rules, thereby improving the comprehensibility of the system. The generated rules act as an early warning expert system to the bank management.},
  pages = {31--40},
}

@Article{benedek_importance_2014,
  title = {The {Importance} of {Social} {Embeddedness}: {Churn} {Models} at {Mobile} {Providers}: {Social} {Embeddedness} and {Churn}},
  volume = {45},
  issn = {00117315},
  shorttitle = {The {Importance} of {Social} {Embeddedness}},
  url = {http://doi.wiley.com/10.1111/deci.12057},
  doi = {10.1111/deci.12057},
  language = {en},
  number = {1},
  urldate = {2020-09-07},
  journal = {Decision Sciences},
  author = {Gábor Benedek and Ágnes Lublóy and Gyula Vastag},
  month = {feb},
  year = {2014},
  abstract = {This article argues the importance of social embeddedness at mobile providers by examining the effects of customers’ network topological properties on churn probability—the probability of a customer switching from one telecommunication provider to another. This article uses data from regional snowball sampling—the only practically feasible network sampling method—to identify groups with significantly different churn ratios for customers with different network topological properties. Clear evidence indicates that individual network characteristics (node‐level metrics) have considerable impact on churn probabilities. The inclusion of network‐related measures in the churn model allows a longer‐term projection of churners and improves the predictive power of the model. With no possibility to carry out repeated sampling, sample stability was checked through simulation results. On the one hand, this article highlights the importance and effectiveness of the provider's tailored marketing campaigns by showing that customers targeted by direct marketing campaigns are less threatened by churn than nontargeted customers. On the other, this article shows that social embeddedness blocks the impact of the very same marketing efforts. This article forwards the idea that social embeddedness, also prevalent in vendor switching, can be extended to understanding the development of professional societies threatened by membership churn.},
  pages = {175--201},
}

@Article{verbraken_profit_2014,
  title = {Profit optimizing customer churn prediction with {Bayesian} network classifiers},
  volume = {18},
  issn = {15714128, 1088467X},
  url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/IDA-130625},
  doi = {10.3233/IDA-130625},
  number = {1},
  urldate = {2020-09-07},
  journal = {Intelligent Data Analysis},
  author = {Thomas Verbraken and Wouter Verbeke and Bart Baesens},
  editor = {Kate Smith-Miles and Richard Weber},
  month = {jan},
  year = {2014},
  abstract = {Customer churn prediction is becoming an increasingly important business analytics problem for telecom operators. In order to increase the efficiency of customer retention campaigns, churn prediction models need to be accurate as well as compact and interpretable. Although a myriad of techniques for churn prediction has been examined, there has been little attention for the use of Bayesian Network classifiers. This paper investigates the predictive power of a number of Bayesian Network algorithms, ranging from the Naive Bayes classifier to General Bayesian Network classifiers. Furthermore, a feature selection method based on the concept of the Markov Blanket, which is genuinely related to Bayesian Networks, is tested. The performance of the classifiers is evaluated with both the Area under the Receiver Operating Characteristic Curve and the recently introduced Maximum Profit criterion. The Maximum Profit criterion performs an intelligent optimization by targeting this fraction of the customer base which would maximize the profit generated by a retention campaign. The results of the experiments are rigorously tested and indicate that most of the analyzed techniques have a comparable performance. Some methods, however, are more preferred since they lead to compact networks, which enhances the interpretability and comprehensibility of the churn prediction models. },
  pages = {3--24},
}

@InProceedings{prasasti_applicability_2014,
  address = {Bandung, Indonesia},
  title = {Applicability of machine-learning techniques in predicting customer defection},
  isbn = {978-1-4799-3704-2 978-1-4799-3703-5},
  url = {http://ieeexplore.ieee.org/document/6936498/},
  doi = {10.1109/ISTMET.2014.6936498},
  urldate = {2020-09-07},
  booktitle = {2014 {International} {Symposium} on {Technology} {Management} and {Emerging} {Technologies}},
  publisher = {IEEE},
  author = {Niken Prasasti and Hayato Ohwada},
  month = {may},
  year = {2014},
  abstract = {Machine learning is an established method of predicting customer defection from a contractual business. However, no systematic comparison or evaluation of the different machine-learning techniques has been performed. In this study, we provide a comprehensive comparison of different machine-learning techniques with three different data sets of a software company to predict customer defection. The evaluation criteria of the techniques are understandability of the model, convenience of using the model, time efficiency in running the learning model, and performance of predicting customer defection.},
  pages = {157--162},
}

@Article{jiang_research_2014,
  title = {Research on {Customers} {Churn} {Prediction} {Model} {Based} on {Logistic}},
  volume = {989-994},
  issn = {1662-8985},
  url = {https://www.scientific.net/AMR.989-994.1517},
  doi = {10.4028/www.scientific.net/AMR.989-994.1517},
  urldate = {2020-09-07},
  journal = {Advanced Materials Research},
  author = {Min Jiang and Na Chu and Xiao Ming Bi},
  month = {jul},
  year = {2014},
  abstract = {At present, the competition is increasingly fierce between the securities company, whether can effectively prevent the loss of users, reducing loss rate is a difficult problem at present each securities company urgently needs to solve. The model based on the principle of data mining, proposes a prediction method based on Logistic regression algorithm. Prediction model is built based on Logistic regression algorithm and the validity and accuracy of the model is verified by experiment, provides a new method and thinking for the securities company customer churn },
  pages = {1517--1521},
}

@Article{tamaddoni_jahromi_managing_2014,
  title = {Managing {B2B} customer churn, retention and profitability},
  volume = {43},
  issn = {00198501},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S001985011400114X},
  doi = {10.1016/j.indmarman.2014.06.016},
  language = {en},
  number = {7},
  urldate = {2020-09-07},
  journal = {Industrial Marketing Management},
  author = {Ali {Tamaddoni Jahromi} and Stanislav Stakhovych and Michael Ewing},
  month = {oct},
  year = {2014},
  abstract = {It is now widely accepted that firms should direct more effort into retaining existing customers than to attracting new ones. To achieve this, customers likely to defect need to be identified so that they can be approached with tailored incentives or other bespoke retention offers. Such strategies call for predictive models capable of identifying customers with higher probabilities of defecting in the relatively near future. A review of the extant literature on customer churn models reveals that although several predictive models have been developed to model churn in B2C contexts, the B2B context in general, and non-contractual settings in particular, have received less attention in this regard. Therefore, to address these gaps, this study proposes a data-mining approach to model non-contractual customer churn in B2B contexts. Several modeling techniques are compared in terms of their ability to predict true churners. The best performing data-mining technique (boosting) is then applied to develop a profit maximizing retention campaign. Results confirm that the model driven approach to churn prediction and developing retention strategies outperforms commonly used managerial heuristics.},
  pages = {1258--1268},
}

@Article{he_prediction_2014,
  title = {Prediction of {Customer} {Attrition} of {Commercial} {Banks} based on {SVM} {Model}},
  volume = {31},
  issn = {18770509},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050914004633},
  doi = {10.1016/j.procs.2014.05.286},
  language = {en},
  urldate = {2020-09-07},
  journal = {Procedia Computer Science},
  author = {Benlan He and Yong Shi and Qian Wan and Xi Zhao},
  year = {2014},
  abstract = {Currently, Chinese commercial banks are facing triple tremendous pressure, including financial disintermediation, interest rate marketization and Internet finance. Meanwhile, increasing financial consumption demand of customers further intensifies the competition among commercial banks. To increase their profits for continuing operations and enhance the core competitiveness, commercial banks must avoid the loss of customers while acquiring new customers. This paper discusses commercial bank customer churn prediction based on SVM model, and uses random sampling method to improve SVM model, considering the imbalance characteristics of customer data sets. The results show that this method can effectively enhance the prediction accuracy of the selected model.},
  pages = {423--430},
}

@Article{verbeke_social_2014,
  title = {Social network analysis for customer churn prediction},
  volume = {14},
  issn = {15684946},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1568494613003116},
  doi = {10.1016/j.asoc.2013.09.017},
  language = {en},
  urldate = {2020-09-07},
  journal = {Applied Soft Computing},
  author = {Wouter Verbeke and David Martens and Bart Baesens},
  month = {jan},
  year = {2014},
  abstract = {This study examines the use of social network information for customer churn prediction. An alternative modeling approach using relational learning algorithms is developed to incorporate social network effects within a customer churn prediction setting, in order to handle large scale networks, a time dependent class label, and a skewed class distribution. An innovative approach to incorporate non-Markovian network effects within relational classifiers and a novel parallel modeling setup to combine a relational and non-relational classification model are introduced. The results of two real life case studies on large scale telco data sets are presented, containing both networked (call detail records) and non-networked (customer related) information about millions of subscribers. A significant impact of social network effects, including non-Markovian effects, on the performance of a customer churn prediction model is found, and the parallel model setup is shown to boost the profits generated by a retention campaign.},
  pages = {431--446},
}

@InProceedings{nabavi_case_2013,
  address = {shiraz, Iran},
  title = {({Case} {Study}: {Solico} {Food} {Industries} {Group})},
  isbn = {978-1-4673-6490-4 978-1-4673-6489-8},
  shorttitle = {({Case} {Study}},
  url = {http://ieeexplore.ieee.org/document/6620065/},
  doi = {10.1109/IKT.2013.6620065},
  urldate = {2020-09-07},
  booktitle = {The 5th {Conference} on {Information} and {Knowledge} {Technology}},
  publisher = {IEEE},
  author = {Sadaf Nabavi and Shahram Jafari},
  month = {may},
  year = {2013},
  abstract = {In order to succeed in the global competition, organizations need to understand and monitor customers' behavior, so that they could retain them by predicting their preference and behavior before others. Recently, marketing strategies have been changed from product-oriented strategies to customer-oriented strategies and most organizations have focused on customer relationship management. In fact, more organizations have found out that retention of their present customers, as their most valuable asset, is very important. Therefore, with the aim of describing data mining abilities in churn management, and designing and implementation of a customer churn prediction model using a standard CRISP-DM (Cross Industry Standard Process for Data Mining) methodology based on RFM (Recency, Frequency, Monetary) and random forest technique, the database of one of the biggest holdings of the country, Solico food industries group, is explored. Using this model, the customers tending to turn over are identified and effective marketing strategies will be planned for this group. Customer behavior analysis indicates that length of relationship, the relative frequency and the average inter purchase time are among the best predictors.},
  pages = {202--207},
}

@InCollection{hutchison_preventing_2013,
  address = {Berlin, Heidelberg},
  title = {Preventing {Churn} in {Telecommunications}: {The} {Forgotten} {Network}},
  volume = {8207},
  isbn = {978-3-642-41397-1 978-3-642-41398-8},
  shorttitle = {Preventing {Churn} in {Telecommunications}},
  url = {http://link.springer.com/10.1007/978-3-642-41398-8_31},
  urldate = {2020-09-07},
  booktitle = {Advances in {Intelligent} {Data} {Analysis} {XII}},
  publisher = {Springer Berlin Heidelberg},
  author = {Dejan Radosavljevik and Peter {van der Putten}},
  editor = {David Hutchison and Takeo Kanade and Josef Kittler and Jon M. Kleinberg and Friedemann Mattern and John C. Mitchell and Moni Naor and Oscar Nierstrasz and C. {Pandu Rangan} and Bernhard Steffen and Madhu Sudan and Demetri Terzopoulos and Doug Tygar and Moshe Y. Vardi and Gerhard Weikum and Allan Tucker and Frank Höppner and Arno Siebes and Stephen Swift},
  year = {2013},
  doi = {10.1007/978-3-642-41398-8_31},
  note = {Series Title: Lecture Notes in Computer Science},
  abstract = {This paper outlines an approach developed as a part of a company-wide churn management initiative of a major European telecom operator. We are focusing on explanatory churn model for the postpaid segment, assuming that the mobile telecom network, the key resource of operators, is also a churn driver in case it under delivers to customers’ expectations. Typically, insights generated by churn models are deployed in marketing campaigns; our model’s insights are used in network optimization in order to remove the key network related churn drivers and therefore prevent churn, rather than cure it. The insights generated by the model have caused a paradigm shift in managing the network with the operator where the research was conducted.},
  pages = {357--368},
}

@Article{brandner_memetic_2013,
  title = {A memetic approach to construct transductive discrete support vector machines},
  volume = {230},
  issn = {03772217},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0377221713004098},
  doi = {10.1016/j.ejor.2013.05.010},
  language = {en},
  number = {3},
  urldate = {2020-09-07},
  journal = {European Journal of Operational Research},
  author = {Hubertus Brandner and Stefan Lessmann and Stefan Voß},
  month = {nov},
  year = {2013},
  abstract = {Transductive learning involves the construction and application of prediction models to classify a fixed set of decision objects into discrete groups. It is a special case of classification analysis with important applications in web-mining, corporate planning and other areas. This paper proposes a novel transductive classifier that is based on the philosophy of discrete support vector machines. We formalize the task to estimate the class labels of decision objects as a mixed integer program. A memetic algorithm is developed to solve the mathematical program and to construct a transductive support vector machine classifier, respectively. Empirical experiments on synthetic and real-world data evidence the effectiveness of the new approach and demonstrate that it identifies high quality solutions in short time. Furthermore, the results suggest that the class predictions following from the memetic algorithm are significantly more accurate than the predictions of a CPLEX-based reference classifier. Comparisons to other transductive and inductive classifiers provide further support for our approach and suggest that it performs competitive with respect to several benchmarks.},
  pages = {581--595},
}

@Article{coussement_customer_2013,
  title = {Customer churn prediction in the online gambling industry: {The} beneficial effect of ensemble learning},
  volume = {66},
  issn = {01482963},
  shorttitle = {Customer churn prediction in the online gambling industry},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0148296312003530},
  doi = {10.1016/j.jbusres.2012.12.008},
  language = {en},
  number = {9},
  urldate = {2020-09-07},
  journal = {Journal of Business Research},
  author = {Kristof Coussement and Koen W. {De Bock}},
  month = {sep},
  year = {2013},
  abstract = {The online gambling industry is one of the most revenue generating branches of the entertainment business, resulting in fierce competition and saturated markets. Therefore it is essential to efficiently retain gamblers. Churn prediction is a promising new alternative in customer relationship management (CRM) to analyze customer retention. It is the process of identifying gamblers with a high probability to leave the company based on their past behavior. This study investigates whether churn prediction is a valuable option in the CRM palette of the online gambling companies. Using real-life data of poker players at bwin, single algorithms, CART decision trees and generalized additive models are benchmarked to their ensemble counterparts, random forests and GAMens. The results show that churn prediction is a valuable strategy to identify and profile those customers at risk. Furthermore, the performance of the ensembles is more robust and better than the single models.},
  pages = {1629--1636},
}

@Article{aghabayk_exploring_2013,
  title = {Exploring a {Local} {Linear} {Model} {Tree} {Approach} to {Car}-{Following}: {Exploring} a local linear model tree approach to car-following},
  volume = {28},
  issn = {10939687},
  shorttitle = {Exploring a {Local} {Linear} {Model} {Tree} {Approach} to {Car}-{Following}},
  url = {http://doi.wiley.com/10.1111/mice.12011},
  doi = {10.1111/mice.12011},
  language = {en},
  number = {8},
  urldate = {2020-09-07},
  journal = {Computer-Aided Civil and Infrastructure Engineering},
  author = {Kayvan Aghabayk and Nafiseh Forouzideh and William Young},
  month = {sep},
  year = {2013},
  abstract = {Because car‐following (CF) models are fundamental to replicating traffic flow they have received considerable attention over the last 50 years. They are in a continuous state of improvement due to their significant role in traffic microsimulations, intelligent transportation systems, and safety engineering models. This article uses the local linear model tree (LOLIMOT) approach to model driver's CF behavior to incorporate human perceptual imperfections into a CF model. This model defines some localities in the input space. These localities are fuzzy and have overlaps with each other. Specific models for each of the localities are then defined and combined in a fuzzy manner to predict the final output. The model was developed using real world dynamic data sets. Three different data sets were used for training, testing, and validating the model. The performance of the model was compared to a number of existing CF models. The results showed very close agreement between the real data and the LOLIMOT outputs.},
  pages = {581--593},
}

@Article{migueis_customer_2013,
  title = {Customer attrition in retailing: {An} application of {Multivariate} {Adaptive} {Regression} {Splines}},
  volume = {40},
  issn = {09574174},
  shorttitle = {Customer attrition in retailing},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417413003679},
  doi = {10.1016/j.eswa.2013.05.069},
  language = {en},
  number = {16},
  urldate = {2020-09-07},
  journal = {Expert Systems with Applications},
  author = {V.L. Miguéis and Ana Camanho and João {Falcão e Cunha}},
  month = {nov},
  year = {2013},
  abstract = {The profit resulting from customer relationship is essential to ensure companies viability, so an improvement in customer retention is crucial for competitiveness. As such, companies have recognized the importance of customer centered strategies and consequently customer relationship management (CRM) is often at the core of their strategic plans. In this context, a priori knowledge about the risk of a given customer to mitigate or even end the relationship with the provider is valuable information that allows companies to take preventive measures to avoid defection. This paper proposes a model to predict partial defection, using two classification techniques: Logistic regression and Multivariate Adaptive Regression Splines (MARS). The main objective is to compare the performance of MARS with Logistic regression in modeling customer attrition. This paper considers the general form of Logistic regression and Logistic regression combined with a wrapper feature selection approach, such as stepwise approach. The empirical results showed that MARS performs better than Logistic regression when variable selection procedures are not used. However, MARS loses its superiority when Logistic regression is conducted with stepwise feature selection.},
  pages = {6225--6232},
}

@Article{ballings_kernel_2013,
  title = {Kernel {Factory}: {An} ensemble of kernel machines},
  volume = {40},
  issn = {09574174},
  shorttitle = {Kernel {Factory}},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417412012481},
  doi = {10.1016/j.eswa.2012.12.007},
  language = {en},
  number = {8},
  urldate = {2020-09-07},
  journal = {Expert Systems with Applications},
  author = {Michel Ballings and Dirk {Van den Poel}},
  month = {jun},
  year = {2013},
  abstract = {We propose an ensemble method for kernel machines. The training data is randomly split into a number of mutually exclusive partitions defined by a row and column parameter. Each partition forms an input space and is transformed by an automatically selected kernel function into a kernel matrix K. Subsequently, each K is used as training data for a base binary classifier (Random Forest). This results in a number of predictions equal to the number of partitions. A weighted average combines the predictions into one final prediction. To optimize the weights, a genetic algorithm is used. This approach has the advantage of simultaneously promoting (1) diversity, (2) accuracy, and (3) computational speed. (1) Diversity is fostered because the individual K’s are based on a subset of features and observations, (2) accuracy is sought by automatic kernel selection and the genetic algorithm, and (3) computational speed is obtained because the computation of each K can be parallelized. Using five times twofold cross validation we benchmark the classification performance of Kernel Factory against Random Forest and Kernel-Induced Random Forest (KIRF). We find that Kernel Factory has significantly better performance than Kernel-Induced Random Forest. When the right kernel is selected Kernel Factory is also significantly better than Random Forest. In addition, an open-source R-software package of the algorithm (kernelFactory) is available from CRAN.},
  pages = {2904--2913},
}

@Article{kim_churn_2013,
  title = {Churn management optimization with controllable marketing variables and associated management costs},
  volume = {40},
  issn = {09574174},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417412011670},
  doi = {10.1016/j.eswa.2012.10.043},
  language = {en},
  number = {6},
  urldate = {2020-09-07},
  journal = {Expert Systems with Applications},
  author = {Yong Seog Kim and Hyeseon Lee and John D. Johnson},
  month = {may},
  year = {2013},
  pages = {2198--2207},
  abstract = {In this paper, we propose a churn management model based on a partial least square (PLS) optimization method that explicitly considers the management costs of controllable marketing variables for a successful churn management program. A PLS prediction model is first calibrated to estimate the churn probabilities of customers. Then this PLS prediction model is transformed into a control model after relative management costs of controllable marketing variables are estimated through a triangulation method. Finally, a PLS optimization model with marketing objectives and constraints are specified and solved via a sequential quadratic programming method. In our experiments, we observe that while the training and test data sets are dramatically different in terms of churner distributions (50% vs. 1.8%), four controllable variables in three marketing strategies significantly changed through optimization process while other variables only marginally changed. We also observe that the most significant variable in a PLS prediction model does not necessarily change most significantly in our PLS optimization model due to the highest management cost associated, implying differences between a prediction and an optimization model. Finally, two marketing models designed for targeting the subsets of customers based on churn probability or management costs are presented and discussed.},
  file = {Kim et al_2013_Churn management optimization with controllable marketing variables and.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\JSYZMZKR\\Kim et al_2013_Churn management optimization with controllable marketing variables and.pdf:application/pdf},
}

@Article{chen_dynamic_2013,
  title = {Dynamic customer lifetime value prediction using longitudinal data: {An} improved multiple kernel {SVR} approach},
  volume = {43},
  issn = {09507051},
  shorttitle = {Dynamic customer lifetime value prediction using longitudinal data},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705113000348},
  doi = {10.1016/j.knosys.2013.01.022},
  language = {en},
  urldate = {2020-09-07},
  journal = {Knowledge-Based Systems},
  author = {Zhen-Yu Chen and Zhi-Ping Fan},
  month = {may},
  year = {2013},
  abstract = {Customer lifetime value (CLV), as an important metric in customer relationship management (CRM), has attracted widespread attention over the last decade. Most CLV prediction models do not take into consideration the dynamics of the customer purchase behavior and changes of the marketing environment such as the adoption of different promotion policies. In this study, a framework for the dynamic CLV prediction using longitudinal data is presented. In the framework, both the dynamic customer purchase behavior and customized promotions are considered. An improved multiple kernel support vector regression (MK-SVR) approach is developed to predict the future CLV and select the best promotion using both the customer behavioral variables and controlled variable about multiple promotions. Computational experiments using two databases show that the MK-SVR exhibits good prediction performance and the usage of longitudinal data in the MK-SVR facilitate the dynamic prediction and promotion optimization.},
  pages = {123--134},
}

@InProceedings{galanis_classification_2013,
  address = {Budapest, Hungary},
  title = {Classification of emotional speech units in call centre interactions},
  isbn = {978-1-4799-1546-0 978-1-4799-1543-9},
  url = {http://ieeexplore.ieee.org/document/6719279/},
  doi = {10.1109/CogInfoCom.2013.6719279},
  urldate = {2020-09-07},
  booktitle = {2013 {IEEE} 4th {International} {Conference} on {Cognitive} {Infocommunications} ({CogInfoCom})},
  publisher = {IEEE},
  author = {Dimitrios Galanis and Sotiris Karabetsos and Maria Koutsombogera and Harris Papageorgiou and Anna Esposito and Maria-Teresa Riviello},
  month = {dec},
  year = {2013},
  abstract = {Detecting emotional traits in call centre interactions can be beneficial to the quality management of the services provided, since this reveals the positioning of both speakers, i.e. satisfaction or frustration and anger on the customers' side, and stress detection, disappointment mitigation or failure to provide the requested service on the operators' side. This paper describes a machine learning approach to classify emotional speech units occurring in a call centre dataset by employing emotion-related labels, automatically extracted acoustic features as well as additional context-related features.},
  pages = {403--406},
}

@Article{wang_credit_2013,
  title = {A credit assessment mechanism for wireless telecommunication debt collection: an empirical study},
  volume = {11},
  issn = {1617-9846, 1617-9854},
  shorttitle = {A credit assessment mechanism for wireless telecommunication debt collection},
  url = {http://link.springer.com/10.1007/s10257-012-0192-x},
  doi = {10.1007/s10257-012-0192-x},
  language = {en},
  number = {3},
  urldate = {2020-09-07},
  journal = {Information Systems and e-Business Management},
  author = {Hsiu-Yu Wang and Chechen Liao and Cheng-Hsiung Kao},
  month = {sep},
  year = {2013},
  abstract = {The wireless telecommunication market in Taiwan is now saturated. As the competition between wireless service carriers intensifies, retaining customers becomes more difficult. To prevent companies from increasing bad debt and experiencing customer churn, this study aims to apply data mining method to build a credit assessment mechanism that can effectively evaluate customer credit risks and help wireless service carriers to enhance the quality of debt collection processes by customizing collection strategies for various customer groups. The application of the proposed mechanism to related problems in a wireless telecommunication company in Taiwan has shown satisfactory effectiveness, accounting for a savings of $2 million of a total $500 million annual revenue. A mere 0.4 % savings is significant, given that wireless service carriers in Taiwan typically allocate 2–4 % of their revenue to uncollectible debts.},
  pages = {357--375},
}

@Article{ballings_customer_2012,
  title = {Customer event history for churn prediction: {How} long is long enough?},
  volume = {39},
  issn = {09574174},
  shorttitle = {Customer event history for churn prediction},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417412008615},
  doi = {10.1016/j.eswa.2012.07.006},
  language = {en},
  number = {18},
  urldate = {2020-09-07},
  journal = {Expert Systems with Applications},
  author = {Michel Ballings and Dirk {Van den Poel}},
  month = {dec},
  year = {2012},
  pages = {13517--13522},
  abstract = {The key question of this study is: How long should customer event history be for customer churn prediction? While most studies in predictive churn modeling aim to improve models by data augmentation or algorithm improvement, this study focuses on a another dimension: time window optimization with respect to predictive performance. This paper first presents a formalization of the time window selection strategy, along with a literature review. Next, using logistic regression, classification trees and bagging in combination with classification trees, this study analyzes the improvement in churn-model performance by extending customer event history from one to sixteen years. The results show that, after the fifth additional year, predictive performance is only marginally increased, meaning that the company in this study can discard 69% of its data with almost no decrease in predictive performance. The practical implication is that analysts can substantially decrease data-related burdens, such as data storage, preparation and analysis. This is particularly valuable in times of big data when decreasing computational complexity is paramount.},
  file = {Ballings_Van den Poel_2012_Customer event history for churn prediction.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\Z82EFAFB\\Ballings_Van den Poel_2012_Customer event history for churn prediction.pdf:application/pdf},
}

@Article{kim_sequential_2012,
  title = {Sequential manifold learning for efficient churn prediction},
  volume = {39},
  issn = {09574174},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417412007853},
  doi = {10.1016/j.eswa.2012.05.069},
  language = {en},
  number = {18},
  urldate = {2020-09-07},
  journal = {Expert Systems with Applications},
  author = {Kyoungok Kim and Jaewook Lee},
  month = {dec},
  year = {2012},
  abstract = {Nowadays, thanks to the rapid evolvement of information technology, an explosively large amount of information with very high-dimensional features for customers is being accumulated in companies. These companies, in turn, are exerting every effort to develop more efficient churn prediction models for managing customer relationships effectively. In this paper, a novel method is proposed to deal with a high-dimensional large data set for constructing better churn prediction models. The proposed method starts by partitioning a data set into small-sized data subsets, and applies sequential manifold learning to reduce high-dimensional features and give consistent results for combined data subsets. The performance of the constructed churn prediction model using the proposed method is tested using an E-commerce data set by comparing it with other existing methods. The proposed method works better and is much faster for high-dimensional large data sets without the need for retraining the original data set to reduce the dimensions of new test samples.},
  pages = {13328--13337},
}

@Article{chen_hierarchical_2012,
  title = {A hierarchical multiple kernel support vector machine for customer churn prediction using longitudinal behavioral data},
  volume = {223},
  issn = {03772217},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S037722171200495X},
  doi = {10.1016/j.ejor.2012.06.040},
  language = {en},
  number = {2},
  urldate = {2020-09-07},
  journal = {European Journal of Operational Research},
  author = {Zhen-Yu Chen and Zhi-Ping Fan and Minghe Sun},
  month = {dec},
  year = {2012},
  abstract = {The availability of abundant data posts a challenge to integrate static customer data and longitudinal behavioral data to improve performance in customer churn prediction. Usually, longitudinal behavioral data are transformed into static data before being included in a prediction model. In this study, a framework with ensemble techniques is presented for customer churn prediction directly using longitudinal behavioral data. A novel approach called the hierarchical multiple kernel support vector machine (H-MK-SVM) is formulated. A three phase training algorithm for the H-MK-SVM is developed, implemented and tested. The H-MK-SVM constructs a classification function by estimating the coefficients of both static and longitudinal behavioral variables in the training process without transformation of the longitudinal behavioral data. The training process of the H-MK-SVM is also a feature selection and time subsequence selection process because the sparse non-zero coefficients correspond to the variables selected. Computational experiments using three real-world databases were conducted. Computational results using multiple criteria measuring performance show that the H-MK-SVM directly using longitudinal behavioral data performs better than currently available classifiers.},
  pages = {461--472},
}

@Article{chen_distributed_2012,
  title = {Distributed customer behavior prediction using multiplex data: {A} collaborative {MK}-{SVM} approach},
  volume = {35},
  issn = {09507051},
  shorttitle = {Distributed customer behavior prediction using multiplex data},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705112001190},
  doi = {10.1016/j.knosys.2012.04.023},
  language = {en},
  urldate = {2020-09-07},
  journal = {Knowledge-Based Systems},
  author = {Zhen-Yu Chen and Zhi-Ping Fan},
  month = {nov},
  year = {2012},
  abstract = {In the customer-centered marketplace, the understanding of customer behavior is a critical success factor. The big databases in an organization usually involve multiplex data such as static, time series, symbolic sequential and textual data which are separately stored in different databases of different sections. It poses a challenge to traditional centralized customer behavior prediction. In this study, a novel approach called collaborative multiple kernel support vector machine (C-MK-SVM) is developed for distributed customer behavior prediction using multiplex data. The alternating direction method of multipliers (ADMM) is used for the global optimization of the distributed sub-models in C-MK-SVM. Computational experiments on a practical retail dataset are reported. Computational results show that C-MK-SVM exhibits better customer behavior prediction performance and higher computational speed than support vector machine and multiple kernel support vector machine.},
  pages = {111--119},
}

@Article{benoit_improving_2012,
  title = {Improving customer retention in financial services using kinship network information},
  volume = {39},
  issn = {09574174},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417412006136},
  doi = {10.1016/j.eswa.2012.04.016},
  language = {en},
  number = {13},
  urldate = {2020-09-07},
  journal = {Expert Systems with Applications},
  author = {Dries F. Benoit and Dirk {Van den Poel}},
  month = {oct},
  year = {2012},
  abstract = {This study investigates the advantage of social network mining in a customer retention context. A company that is able to identify likely churners in an early stage can take appropriate steps to prevent these potential churners from actually churning and subsequently increase profit. Academics and practitioners are constantly trying to optimize their predictive-analytics models by searching for better predictors. The aim of this study is to investigate if, in addition to the conventional sets of variables (socio-demographics, purchase history, etc.), kinship network based variables improve the predictive power of customer retention models. Results show that the predictive power of the churn model can indeed be improved by adding the social network (SNA-) based variables. Including network structure measures (i.e. degree, betweenness centrality and density) increase predictive accuracy, but contextual network based variables turn out to have the highest impact on discriminating churners from non-churners. For the majority of the latter type of network variables, the importance in the model is even higher than the individual level counterpart variable.},
  pages = {11435--11442},
}

@Article{abdi_forecasting_2012,
  title = {Forecasting of short-term traffic-flow based on improved neurofuzzy models via emotional temporal difference learning algorithm},
  volume = {25},
  issn = {09521976},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0952197611001643},
  doi = {10.1016/j.engappai.2011.09.011},
  language = {en},
  number = {5},
  urldate = {2020-09-07},
  journal = {Engineering Applications of Artificial Intelligence},
  author = {Javad Abdi and Behzad Moshiri and Baher Abdulhai and Ali Khaki Sedigh},
  month = {aug},
  year = {2012},
  abstract = {Bounded rationally idea, rather that optimization idea, have result and better performance in decision making theory. Bounded rationality is the idea in decision making, rationality of individuals is limited by the information they have, the cognitive limitations of their minds, and the finite amount of time they have to make decisions. The emotional theory is an important topic presented in this field. The new methods in the direction of purposeful forecasting issues, which are based on cognitive limitations, are presented in this study. The presented algorithms in this study are emphasizes to rectify the learning the peak points, to increase the forecasting accuracy, to decrease the computational time and comply the multi-object forecasting in the algorithms. The structure of the proposed algorithms is based on approximation of its current estimate according to previously learned estimates. The short term traffic flow forecasting is a real benchmark that has been studied in this area. Traffic flow is a good measure of traffic activity. The time-series data used for fitting the proposed models are obtained from a two lane street I-494 in Minnesota City, USA. The research discuss the strong points of new method based on neurofuzzy and limbic system structure such as Locally Linear Neurofuzzy network (LLNF) and Brain Emotional Learning Based Intelligent Controller (BELBIC) models against classical and other intelligent methods such as Radial Basis Function (RBF), Takagi–Sugeno (T–S) neurofuzzy, and Multi-Layer Perceptron (MLP), and the effect of noise on the performance of the models is also considered. Finally, findings confirmed the significance of structural brain modeling beyond the classical artificial neural networks.},
  pages = {1022--1042},
}

@Article{tsai_determinants_2012,
  title = {Determinants of intangible assets value: {The} data mining approach},
  volume = {31},
  issn = {09507051},
  shorttitle = {Determinants of intangible assets value},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705112000470},
  doi = {10.1016/j.knosys.2012.02.007},
  language = {en},
  urldate = {2020-09-07},
  journal = {Knowledge-Based Systems},
  author = {Chih-Fong Tsai and Yu-Hsin Lu and David C. Yen},
  month = {jul},
  year = {2012},
  abstract = {It is very important for investors and creditors to understand the critical factors affecting a firm’s value before making decisions about investments and loans. Since the knowledge-based economy has evolved, the method for creating firm value has transferred from traditional physical assets to intangible knowledge. Therefore, valuation of intangible assets has become a widespread topic of interest in the future of the economy. This study takes advantage of feature selection, an important data-preprocessing step in data mining, to identify important and representative factors affecting intangible assets. Particularly, five feature selection methods are considered, which include principal component analysis (PCA), stepwise regression (STEPWISE), decision trees (DT), association rules (AR), and genetic algorithms (GA). In addition, multi-layer perceptron (MLP) neural networks are used as the prediction model in order to understand which features selected from these five methods can allow the prediction model to perform best. Based on the chosen dataset containing 61 variables, the experimental result shows that combining the results from multiple feature selection methods performs the best. GA ∩ STEPWISE, DT ∪ PCA, and the DT single feature selection method generate approximately 75% prediction accuracy, which select 26, 22, and 7 variables respectively.},
  pages = {67--77},
}

@Article{de_bock_reconciling_2012,
  title = {Reconciling performance and interpretability in customer churn prediction using ensemble learning based on generalized additive models},
  volume = {39},
  issn = {09574174},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417412000164},
  doi = {10.1016/j.eswa.2012.01.014},
  language = {en},
  number = {8},
  urldate = {2020-09-07},
  journal = {Expert Systems with Applications},
  author = {Koen W. {De Bock} and Dirk {Van den Poel}},
  month = {jun},
  year = {2012},
  abstract = {To build a successful customer churn prediction model, a classification algorithm should be chosen that fulfills two requirements: strong classification performance and a high level of model interpretability. In recent literature, ensemble classifiers have demonstrated superior performance in a multitude of applications and data mining contests. However, due to an increased complexity they result in models that are often difficult to interpret. In this study, GAMensPlus, an ensemble classifier based upon generalized additive models (GAMs), in which both performance and interpretability are reconciled, is presented and evaluated in a context of churn prediction modeling. The recently proposed GAMens, based upon Bagging, the Random Subspace Method and semi-parametric GAMs as constituent classifiers, is extended to include two instruments for model interpretability: generalized feature importance scores, and bootstrap confidence bands for smoothing splines. In an experimental comparison on data sets of six real-life churn prediction projects, the competitive performance of the proposed algorithm over a set of well-known benchmark algorithms is demonstrated in terms of four evaluation metrics. Further, the ability of the technique to deliver valuable insight into the drivers of customer churn is illustrated in a case study on data from a European bank. Firstly, it is shown how the generalized feature importance scores allow the analyst to identify the relative importance of churn predictors in function of the criterion that is used to measure the quality of the model predictions. Secondly, the ability of GAMensPlus to identify nonlinear relationships between predictors and churn probabilities is demonstrated.},
  pages = {6816--6826},
}

@Article{verbeke_new_2012,
  title = {New insights into churn prediction in the telecommunication sector: {A} profit driven data mining approach},
  volume = {218},
  issn = {03772217},
  shorttitle = {New insights into churn prediction in the telecommunication sector},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0377221711008599},
  doi = {10.1016/j.ejor.2011.09.031},
  language = {en},
  number = {1},
  urldate = {2020-09-07},
  journal = {European Journal of Operational Research},
  author = {Wouter Verbeke and Karel Dejaeger and David Martens and Joon Hur and Bart Baesens},
  month = {apr},
  year = {2012},
  abstract = {Customer churn prediction models aim to indicate the customers with the highest propensity to attrite, allowing to improve the efficiency of customer retention campaigns and to reduce the costs associated with churn. Although cost reduction is their prime objective, churn prediction models are typically evaluated using statistically based performance measures, resulting in suboptimal model selection. Therefore, in the first part of this paper, a novel, profit centric performance measure is developed, by calculating the maximum profit that can be generated by including the optimal fraction of customers with the highest predicted probabilities to attrite in a retention campaign. The novel measure selects the optimal model and fraction of customers to include, yielding a significant increase in profits compared to statistical measures. In the second part an extensive benchmarking experiment is conducted, evaluating various classification techniques applied on eleven real-life data sets from telecom operators worldwide by using both the profit centric and statistically based performance measures. The experimental results show that a small number of variables suffices to predict churn with high accuracy, and that oversampling generally does not improve the performance significantly. Finally, a large group of classifiers is found to yield comparable performance.},
  pages = {211--229},
}

@Article{farquad_analytical_2012,
  title = {Analytical {CRM} in banking and finance using {SVM}: a modified active learning-based rule extraction approach},
  volume = {6},
  issn = {1750-0664, 1750-0672},
  shorttitle = {Analytical {CRM} in banking and finance using {SVM}},
  url = {http://www.inderscience.com/link.php?id=46470},
  doi = {10.1504/IJECRM.2012.046470},
  language = {en},
  number = {1},
  urldate = {2020-09-07},
  journal = {International Journal of Electronic Customer Relationship Management},
  author = {M.A.H. Farquad and V. Ravi and S. Bapi Raju},
  year = {2012},
  abstract = {This paper presents advancement to modified active learning-based approach in an eclectic framework for extracting if-then rules from support vector machine (SVM) for customer relationship management (CRM) purposes. The proposed approach comprises of three major phases: 1) feature selection using SVM-RFE (recursive feature elimination); 2) active learning for synthetic data generation; 3) rule generation using decision tree (DT) and Naive Bayes tree (NBTree). Finance problems solved in this study are churn prediction in bank credit cards customers and fraud detection in insurance. Based on sensitivity measure, the empirical results suggest that the proposed modified active learning-based rule extraction approach yielded best sensitivity and length and number of rules is reduced resulting in improved comprehensibility. Feature selection leads to the most important attributes of the customers and extracted rules serves as early warning system to the management to enforce better CRM practices and detect/avoid possible frauds.},
  pages = {48},
}

@Article{zhang_predicting_2012,
  title = {Predicting customer churn through interpersonal influence},
  volume = {28},
  issn = {09507051},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705111002693},
  doi = {10.1016/j.knosys.2011.12.005},
  language = {en},
  urldate = {2020-09-07},
  journal = {Knowledge-Based Systems},
  author = {Xiaohang Zhang and Ji Zhu and Shuhua Xu and Yan Wan},
  month = {apr},
  year = {2012},
  abstract = {Preventing customer churn is an important task for many enterprises and requires customer churn prediction. This paper investigates the effects of interpersonal influence on the accuracy of customer churn predictions and proposes a novel prediction model that is based on interpersonal influence and that combines the propagation process and customers’ personalized characters. Our contributions include the following: (1) the effects of interpersonal influence on prediction accuracy are evaluated while including determinants that other researchers proved effective, and several models are constructed based on machine learning and statistical methods and compared, assuring the validity of the evaluation; and (2) a novel prediction model based on interpersonal influence and information propagation is proposed. The dataset used in the empirical study was obtained from a leading mobile telecommunication service provider and contains the traditional and network attributes of over one million customers. The empirical results show that traditional classification models that incorporate interpersonal influence can greatly improve prediction accuracy, and our proposed prediction model outperforms the traditional models.},
  pages = {97--104},
}

@Article{xiao_dynamic_2012,
  title = {Dynamic classifier ensemble model for customer classification with imbalanced class distribution},
  volume = {39},
  issn = {09574174},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417411013686},
  doi = {10.1016/j.eswa.2011.09.059},
  language = {en},
  number = {3},
  urldate = {2020-09-07},
  journal = {Expert Systems with Applications},
  author = {Jin Xiao and Ling Xie and Changzheng He and Xiaoyi Jiang},
  month = {feb},
  year = {2012},
  abstract = {Customer classification is widely used in customer relationship management including churn prediction, credit scoring, cross-selling and so on. In customer classification, an important yet challenging problem is the imbalance of data distribution. In this paper, we combine ensemble learning with cost-sensitive learning, and propose a dynamic classifier ensemble method for imbalanced data (DCEID). For each test customer, it can adaptively select out the more appropriate one from the two kinds of dynamic ensemble approach: dynamic classifier selection (DCS) and dynamic ensemble selection (DES). Meanwhile, new cost-sensitive selection criteria for DCS and DES are constructed respectively to improve the classification ability for imbalanced data. We apply this method to a credit scoring dataset in UCI and a real churn prediction dataset from a telecommunication company. The experimental results show that the classification performance of DCEID is not only better than some static ensemble methods such as weighted random forests and improved balanced random forests, but also better than the existing DCS and DES strategies.},
  pages = {3668--3675},
}

@Article{migueis_predicting_2012,
  title = {Predicting partial customer churn using {Markov} for discrimination for modeling first purchase sequences},
  volume = {6},
  issn = {1862-5347, 1862-5355},
  url = {http://link.springer.com/10.1007/s11634-012-0121-3},
  doi = {10.1007/s11634-012-0121-3},
  language = {en},
  number = {4},
  urldate = {2020-09-07},
  journal = {Advances in Data Analysis and Classification},
  author = {Vera L. Miguéis and Dirk {Van den Poel} and Ana S. Camanho and João {Falcão e Cunha}},
  month = {dec},
  year = {2012},
  abstract = {Currently, in order to remain competitive companies are adopting customer centered strategies and consequently customer relationship management is gaining increasing importance. In this context, customer retention deserves particular attention. This paper proposes a model for partial churn detection in the retail grocery sector that includes as a predictor the similarity of the products’ first purchase sequence with churner and non-churner sequences. The sequence of first purchase events is modeled using Markov for discrimination. Two classification techniques are used in the empirical study: logistic regression and random forests. A real sample of approximately 95,000 new customers is analyzed taken from the data warehouse of a European retailing company. The empirical results reveal the relevance of the inclusion of a products’ sequence likelihood in partial churn prediction models, as well as the supremacy of logistic regression when compared with random forests.},
  pages = {337--353},
}

@InCollection{casillas_improving_2012,
  address = {Berlin, Heidelberg},
  title = {Improving {Customer} {Churn} {Prediction} by {Data} {Augmentation} {Using} {Pictorial} {Stimulus}-{Choice} {Data}},
  volume = {171},
  isbn = {978-3-642-30863-5 978-3-642-30864-2},
  url = {http://link.springer.com/10.1007/978-3-642-30864-2_21},
  urldate = {2020-09-07},
  booktitle = {Management {Intelligent} {Systems}},
  publisher = {Springer Berlin Heidelberg},
  author = {Michel Ballings and Dirk {Van den Poel} and Emmanuel Verhagen},
  editor = {Jorge Casillas and Francisco J. Martínez-López and Juan Manuel {Corchado Rodríguez}},
  year = {2012},
  doi = {10.1007/978-3-642-30864-2_21},
  note = {Series Title: Advances in Intelligent Systems and Computing},
  pages = {217--226},
  abstract = {The purpose of this paper is to determine the added value of pictorial stimulus-choice data in customer churn prediction. Using Random Forests and 5 times 2 fold cross-validation, this study analyzes how much pictorial stimulus – choice data and survey data increase the AUC of a churn model over and above administrative, operational and complaints data. The finding is that pictorial-stimulus choice data significantly increases AUC of models with administrative and operational data. The practical implication of this finding is that companies should start considering mining pictorial data from social media sites (e.g. Pinterest), in order to augment their internal customer database. This study is original in that it is the first that assesses the added value of pictorial stimulus-choice data in predictive models. This is important because more and more social media websites are focusing on pictures.},
  file = {Ballings et al_2012_Improving Customer Churn Prediction by Data Augmentation Using Pictorial.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\ML7B72HR\\Ballings et al_2012_Improving Customer Churn Prediction by Data Augmentation Using Pictorial.pdf:application/pdf},
}

@Article{chen_diagnosing_2012,
  title = {{DIAGNOSING} {ASSETS} {IMPAIRMENT} {BY} {USING} {RANDOM} {FORESTS} {MODEL}},
  volume = {11},
  issn = {0219-6220, 1793-6845},
  url = {https://www.worldscientific.com/doi/abs/10.1142/S0219622012500046},
  doi = {10.1142/S0219622012500046},
  abstract = {This study develops a diagnosing model to examine the outcomes of assets write-off in enriching the literatures of assets impairment. Prior studies employed the Logit, linear and Tobit regression models to classify the determination of assets impairment and to diagnose the magnitude of the impairment, respectively. However, the drivers of assets write-off are somewhat complicated explicitly or implicitly, these models are unlikely to provide fairly satisfactory results. To improve the diagnosis, the Random Forests model is used for the classification determining and the magnitude diagnosing of assets impairment in this study. The result reveals that the Random Forests model outperforms the Logit and linear regression models in each case with variables selected by individual wrapping approach. This study also demonstrates diagnostic checks for both models with similar selected variables. The results are robust to these various specifications.},
  language = {en},
  number = {01},
  urldate = {2020-09-07},
  journal = {International Journal of Information Technology \& Decision Making},
  author = {Ching-Lung Chen and Chei-Wei Wu},
  month = {jan},
  year = {2012},
  pages = {77--102},
}

@InProceedings{ling_xie_feature_2011,
  address = {Guiyang, China},
  title = {Feature selection based transfer ensemble model for customer churn prediction},
  isbn = {978-1-4577-0246-4 978-1-4577-0247-1},
  url = {http://ieeexplore.ieee.org/document/6081258/},
  doi = {10.1109/ICSSEM.2011.6081258},
  urldate = {2020-09-07},
  booktitle = {2011 {International} {Conference} on {System} science, {Engineering} design and {Manufacturing} informatization},
  publisher = {IEEE},
  author = {{Ling Xie} and {Dan Li} and Jin Xiao},
  month = {oct},
  year = {2011},
  abstract = {It is difficult to get satisfactory customer churn prediction effect for the traditional model, because the class distribution of customer data is often imbalanced, and the available data in target task is little. This paper combines the transfer learning with the ensemble learning, and proposes a feature selection based transfer ensemble model (FSTE). It utilizes the customer data in both the related source domain and target domain, selects a series of feature subsets, obtains the corresponding training subsets by mapping; further, it trains a number of classifiers and gets the final customer churn prediction result by integrating the prediction results. The empirical results show that FSTE can achieve better customer churn prediction performance compared with the traditional churn prediction model, and some existing transfer learning models such as TFS, TrBagg and TrAdboost.},
  pages = {134--137},
}

@InProceedings{wang_transfer_2011,
  address = {Nanjing, Jiangsu, China},
  title = {Transfer {Ensemble} {Model} for {Customer} {Churn} {Prediction} with {Imbalanced} {Class} {Distribution}},
  isbn = {978-1-4577-1419-1 978-0-7695-4522-6},
  url = {http://ieeexplore.ieee.org/document/6113613/},
  doi = {10.1109/ICM.2011.397},
  urldate = {2020-09-07},
  booktitle = {2011 {International} {Conference} of {Information} {Technology}, {Computer} {Engineering} and {Management} {Sciences}},
  publisher = {IEEE},
  author = {Yuan Wang and Jin Xiao},
  month = {sep},
  year = {2011},
  abstract = {Customer churn prediction is an important issue in customer relationship management. The class distribution of customer data is often imbalanced, which may affect the performance of churn prediction model greatly. This paper combines transfer learning and multiple classifiers ensemble, and proposes a transfer ensemble model for imbalanced data (TEMID). This method focuses on using transfer learning and sampling to enlarge the available training set and balance it respectively. What's more, it also uses multiple classifiers ensemble method to implement the classification. The performance of TEMID and some existing transfer learning algorithms are compared in two class imbalanced datasets. The results show that the TEMID methods can actually improve the performance of the customer churn prediction.},
  pages = {177--181},
}

@Article{nie_credit_2011,
  title = {Credit card churn forecasting by logistic regression and decision tree},
  volume = {38},
  issn = {09574174},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417411009237},
  doi = {10.1016/j.eswa.2011.06.028},
  language = {en},
  number = {12},
  urldate = {2020-09-07},
  journal = {Expert Systems with Applications},
  author = {Guangli Nie and Wei Rowe and Lingling Zhang and Yingjie Tian and Yong Shi},
  month = {nov},
  year = {2011},
  abstract = {In this paper, two data mining algorithms are applied to build a churn prediction model using credit card data collected from a real Chinese bank. The contribution of four variable categories: customer information, card information, risk information, and transaction activity information are examined. The paper analyzes a process of dealing with variables when data is obtained from a database instead of a survey. Instead of considering the all 135 variables into the model directly, it selects the certain variables from the perspective of not only correlation but also economic sense. In addition to the accuracy of analytic results, the paper designs a misclassification cost measurement by taking the two types error and the economic sense into account, which is more suitable to evaluate the credit card churn prediction model. The algorithms used in this study include logistic regression and decision tree which are proven mature and powerful classification algorithms. The test result shows that regression performs a little better than decision tree.},
  pages = {15273--15285},
}

@Article{lessmann_tuning_2011,
  title = {Tuning metaheuristics: {A} data mining based approach for particle swarm optimization},
  volume = {38},
  issn = {09574174},
  shorttitle = {Tuning metaheuristics},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417411005914},
  doi = {10.1016/j.eswa.2011.04.075},
  language = {en},
  number = {10},
  urldate = {2020-09-07},
  journal = {Expert Systems with Applications},
  author = {Stefan Lessmann and Marco Caserta and Idel Montalvo Arango},
  month = {sep},
  year = {2011},
  abstract = {The paper is concerned with practices for tuning the parameters of metaheuristics. Settings such as, e.g., the cooling factor in simulated annealing, may greatly affect a metaheuristic’s efficiency as well as effectiveness in solving a given decision problem. However, procedures for organizing parameter calibration are scarce and commonly limited to particular metaheuristics. We argue that the parameter selection task can appropriately be addressed by means of a data mining based approach. In particular, a hybrid system is devised, which employs regression models to learn suitable parameter values from past moves of a metaheuristic in an online fashion. In order to identify a suitable regression method and, more generally, to demonstrate the feasibility of the proposed approach, a case study of particle swarm optimization is conducted. Empirical results suggest that characteristics of the decision problem as well as search history data indeed embody information that allows suitable parameter values to be determined, and that this type of information can successfully be extracted by means of nonlinear regression models.},
  pages = {12826--12838},
}

@InProceedings{choi_learnability_2011,
  address = {Nice, France},
  title = {Learnability of latent position network models},
  isbn = {978-1-4577-0569-4},
  url = {http://ieeexplore.ieee.org/document/5967748/},
  doi = {10.1109/SSP.2011.5967748},
  urldate = {2020-09-07},
  booktitle = {2011 {IEEE} {Statistical} {Signal} {Processing} {Workshop} ({SSP})},
  publisher = {IEEE},
  author = {David S. Choi and Patrick J. Wolfe},
  month = {jun},
  year = {2011},
  abstract = {The latent position model is a well known model for social network analysis which has also found application in other fields, such as analysis of marketing and e-commerce data. In such applications, the data sets are increasingly massive and only partially observed, giving rise to the possibility of overfitting by the model. Using tools from statistical learning theory, we bound the VC dimension of the latent position model, leading to bounds on the overfit of the model. We find that the overfit can decay to zero with increasing network size even if only a vanishing fraction of the total network is observed. However, the amount of observed data on a per-node basis should increase with the size of the graph.},
  pages = {521--524},
}

@Article{bonchi_social_2011,
  title = {Social {Network} {Analysis} and {Mining} for {Business} {Applications}},
  volume = {2},
  issn = {2157-6904, 2157-6912},
  url = {https://dl.acm.org/doi/10.1145/1961189.1961194},
  doi = {10.1145/1961189.1961194},
  language = {en},
  number = {3},
  urldate = {2020-09-07},
  journal = {ACM Transactions on Intelligent Systems and Technology},
  author = {Francesco Bonchi and Carlos Castillo and Aristides Gionis and Alejandro Jaimes},
  month = {apr},
  year = {2011},
  pages = {1--37},
  abstract = {Social network analysis has gained significant attention in recent years, largely due to the success of online social networking and media-sharing sites, and the consequent availability of a wealth of social network data. In spite of the growing interest, however, there is little understanding of the potential business applications of mining social networks. While there is a large body of research on different problems and methods for social network mining, there is a gap between the techniques developed by the research community and their deployment in real-world applications. Therefore the potential business impact of these techniques is still largely unexplored. In this article we use a business process classification framework to put the research topics in a business context and provide an overview of what we consider key problems and techniques in social network analysis and mining from the perspective of business applications. In particular, we discuss data acquisition and preparation, trust, expertise, community structure, network dynamics, and information propagation. In each case we present a brief overview of the problem, describe state-of-the art approaches, discuss business application examples, and map each of the topics to a business process classification framework. In addition, we provide insights on prospective business applications, challenges, and future research directions. The main contribution of this article is to provide a state-of-the-art overview of current techniques while providing a critical perspective on business applications of social network analysis and mining.},
  file = {Bonchi et al_2011_Social Network Analysis and Mining for Business Applications.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\PA7F4GSV\\Bonchi et al_2011_Social Network Analysis and Mining for Business Applications.pdf:application/pdf},
}

@Article{verbeke_building_2011,
  title = {Building comprehensible customer churn prediction models with advanced rule induction techniques},
  volume = {38},
  issn = {09574174},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417410008067},
  doi = {10.1016/j.eswa.2010.08.023},
  language = {en},
  number = {3},
  urldate = {2020-09-07},
  journal = {Expert Systems with Applications},
  author = {Wouter Verbeke and David Martens and Christophe Mues and Bart Baesens},
  month = {mar},
  year = {2011},
  abstract = {Customer churn prediction models aim to detect customers with a high propensity to attrite. Predictive accuracy, comprehensibility, and justifiability are three key aspects of a churn prediction model. An accurate model permits to correctly target future churners in a retention marketing campaign, while a comprehensible and intuitive rule-set allows to identify the main drivers for customers to churn, and to develop an effective retention strategy in accordance with domain knowledge. This paper provides an extended overview of the literature on the use of data mining in customer churn prediction modeling. It is shown that only limited attention has been paid to the comprehensibility and the intuitiveness of churn prediction models. Therefore, two novel data mining techniques are applied to churn prediction modeling, and benchmarked to traditional rule induction techniques such as C4.5 and RIPPER. Both AntMiner+ and ALBA are shown to induce accurate as well as comprehensible classification rule-sets. AntMiner+ is a high performing data mining technique based on the principles of Ant Colony Optimization that allows to include domain knowledge by imposing monotonicity constraints on the final rule-set. ALBA on the other hand combines the high predictive accuracy of a non-linear support vector machine model with the comprehensibility of the rule-set format. The results of the benchmarking experiments show that ALBA improves learning of classification techniques, resulting in comprehensible models with increased performance. AntMiner+ results in accurate, comprehensible, but most importantly justifiable models, unlike the other modeling techniques included in this study.},
  pages = {2354--2364},
}

@Article{verikas_mining_2011,
  title = {Mining data with random forests: {A} survey and results of new tests},
  volume = {44},
  issn = {00313203},
  shorttitle = {Mining data with random forests},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320310003973},
  doi = {10.1016/j.patcog.2010.08.011},
  language = {en},
  number = {2},
  urldate = {2020-09-07},
  journal = {Pattern Recognition},
  author = {A. Verikas and A. Gelzinis and M. Bacauskiene},
  month = {feb},
  year = {2011},
  abstract = {Random forests (RF) has become a popular technique for classification, prediction, studying variable importance, variable selection, and outlier detection. There are numerous application examples of RF in a variety of fields. Several large scale comparisons including RF have been performed. There are numerous articles, where variable importance evaluations based on the variable importance measures available from RF are used for data exploration and understanding. Apart from the literature survey in RF area, this paper also presents results of new tests regarding variable rankings based on RF variable importance measures. We studied experimentally the consistency and generality of such rankings. Results of the studies indicate that there is no evidence supporting the belief in generality of such rankings. A high variance of variable importance evaluations was observed in the case of small number of trees and small data sets.},
  pages = {330--349},
}

@Article{vasu_hybrid_2011,
  title = {A hybrid under-sampling approach for mining unbalanced datasets: applications to banking and insurance},
  volume = {3},
  issn = {1759-1163, 1759-1171},
  shorttitle = {A hybrid under-sampling approach for mining unbalanced datasets},
  url = {http://www.inderscience.com/link.php?id=38812},
  doi = {10.1504/IJDMMM.2011.038812},
  language = {en},
  number = {1},
  urldate = {2020-09-07},
  journal = {International Journal of Data Mining, Modelling and Management},
  author = {Madireddi Vasu and Vadlamani Ravi},
  year = {2011},
  abstract = {In solving unbalanced classification problems, machine learning algorithms are overwhelmed by the majority class and consequently misclassify the minority class observations. Here, we propose a hybrid under-sampling approach to improve the performance of classifiers. The proposed approach first employs k-reverse nearest neighbour (kRNN) method to detect the outliers from majority class. After removing the outliers, using K-means clustering, K-clusters are selected to further reduce the influence of the majority class. Then, we employed support vector machine (SVM), logistic regression (LR), multi layer perceptron (MLP), radial basis function network (RBF), group method of data handling (GMDH), genetic programming (GP) and decision tree (J48) for classification purpose. The effectiveness of the proposed approach was demonstrated on datasets taken from insurance fraud detection and credit card churn in banking domain. Ten-fold cross validation method was used in the study. It is observed that the proposed approach improved the performance of the classifiers.},
  pages = {75},
}

@Article{kianmehr_fuzzy_2011,
  title = {A fuzzy prediction model for calling communities},
  volume = {8},
  issn = {1470-9503, 1741-5225},
  url = {http://www.inderscience.com/link.php?id=37162},
  doi = {10.1504/IJNVO.2011.037162},
  language = {en},
  number = {1/2},
  urldate = {2020-09-07},
  journal = {International Journal of Networking and Virtual Organisations},
  author = {Keivan Kianmehr and Reda Alhajj},
  year = {2011},
  abstract = {The analysis of logs related to social communities has recently received considerable attention for its importance in shedding light on social concerns by identifying different groups, and hence helps in resolving issues like predicting terrorist groups. In general, identifying calling communities can be used to determine a particular customer's value according to the general pattern of behaviour of the community that the customer belongs to; this helps in creating an effective targeted marketing design, which is significantly important for increasing profitability. In the telecommunications industry, machine-learning techniques have been applied to the Call Detail Record (CDR) for predicting customer behaviour such as churn prediction. In this paper, we pursue the identification of the calling communities and demonstrate how cluster analysis can be used to effectively identify communities using information derived from the CDR data. We use the information extracted from the cluster analysis to identify customer calling patterns. Customer calling patterns are then input to a classification algorithm to generate a classifier model for predicting the calling communities of a customer. We apply two different classification methods: the Support Vector Machine (SVM) algorithm and the fuzzy genetic classifier. The latter method is used for possibly assigning a customer to different classes with different degrees of membership. The reported test results demonstrate the applicability and effectiveness of the proposed approach.},
  pages = {75},
}

@InProceedings{karnstedt_churn_2010,
  address = {Minneapolis, MN, USA},
  title = {Churn in {Social} {Networks}: {A} {Discussion} {Boards} {Case} {Study}},
  isbn = {978-1-4244-8439-3},
  shorttitle = {Churn in {Social} {Networks}},
  url = {http://ieeexplore.ieee.org/document/5590431/},
  doi = {10.1109/SocialCom.2010.40},
  urldate = {2020-09-07},
  booktitle = {2010 {IEEE} {Second} {International} {Conference} on {Social} {Computing}},
  publisher = {IEEE},
  author = {Marcel Karnstedt and Tara Hennessy and Jeffrey Chan and Conor Hayes},
  month = {aug},
  year = {2010},
  abstract = {Churn has been identified as an important issue in a wide range of industries. In social networks, churn represents a significant risk for the health and functioning of communities. However, the importance and actual meaning of churn in social networks is almost unexplored. This work provides a general view on these issues and discusses aspects that are especially relevant to discussion boards. We provide a broad literature review on “traditional” churn analysis and prediction and highlight the specialities of churn in social networks. We further present an empirical analysis of a churn definition particularly appropriate for discussion boards and propose future research directions for predicting churn in social networks, focusing on the importance of social roles, influence and influence diffusion.},
  pages = {233--240},
}

@InProceedings{han_fuzzy-rough_2010,
  address = {Yantai, China},
  title = {Fuzzy-rough k-nearest neighbor algorithm for imbalanced data sets learning},
  isbn = {978-1-4244-5931-5},
  url = {http://ieeexplore.ieee.org/document/5569116/},
  doi = {10.1109/FSKD.2010.5569116},
  urldate = {2020-09-07},
  booktitle = {2010 {Seventh} {International} {Conference} on {Fuzzy} {Systems} and {Knowledge} {Discovery}},
  publisher = {IEEE},
  author = {Hui Han and Binghuan Mao},
  month = {aug},
  year = {2010},
  abstract = {Learning from imbalanced data sets presents a new challenge to machine learning community, as traditional methods are biased to majority classes and produce poor detection rate of minority classes. This paper presents a new approach, namely fuzzy-rough k-nearest neighbor algorithm for imbalanced data sets learning to improve the classification performance of minority class. The approach defines fuzzy membership function that is in favor of minority class and constructs fuzzy equivalent relation between the unlabeled instance and its k nearest neighbors. The approach takes the fuzziness and roughness of the nearest neighbors of an instance into consideration, and can reduce the disturbance of majority class to minority class. Experiments show that our new approach improves not only the classification performance of minority class more effectively, but also the classification performance of the whole data set comparing with other methods.},
  pages = {1286--1290},
}

@InCollection{hutchison_rule_2010,
  address = {Berlin, Heidelberg},
  title = {Rule {Extraction} from {Support} {Vector} {Machine} {Using} {Modified} {Active} {Learning} {Based} {Approach}: {An} {Application} to {CRM}},
  volume = {6276},
  isbn = {978-3-642-15386-0 978-3-642-15387-7},
  shorttitle = {Rule {Extraction} from {Support} {Vector} {Machine} {Using} {Modified} {Active} {Learning} {Based} {Approach}},
  url = {http://link.springer.com/10.1007/978-3-642-15387-7_50},
  urldate = {2020-09-07},
  booktitle = {Knowledge-{Based} and {Intelligent} {Information} and {Engineering} {Systems}},
  publisher = {Springer Berlin Heidelberg},
  author = {David Hutchison and Takeo Kanade and Josef Kittler and Jon M. Kleinberg and Friedemann Mattern and John C. Mitchell and Moni Naor and Oscar Nierstrasz and C. {Pandu Rangan} and Bernhard Steffen and Madhu Sudan and Demetri Terzopoulos and Doug Tygar and Moshe Y. Vardi and Gerhard Weikum and M. A. H. Farquad and V. Ravi and S. Bapi Raju},
  editor = {Rossitza Setchi and Ivan Jordanov and Robert J. Howlett and Lakhmi C. Jain},
  year = {2010},
  doi = {10.1007/978-3-642-15387-7_50},
  note = {Series Title: Lecture Notes in Computer Science},
  abstract = {Despite superior generalization performance Support vector machines (SVMs) generate black box models. The process of converting such opaque models into transparent model is often regarded as rule extraction. This paper presents a new approach for rule extraction from SVMs using modified active learning based approach (mALBA), to predict churn in bank credit cards. The dataset is obtained from Business Intelligence Cup 2004, which is highly unbalanced with 93% loyal and 7% churned customers’ data. Since identifying churner is paramount from business perspective, therefore considering sensitivity alone, the empirical results suggest that the proposed rule extraction approach using mALBA yielded the best sensitivity compared to other classifiers.},
  pages = {461--470},
}

@InProceedings{yean_customer_2010,
  address = {Kuala Lumpur, Malaysia},
  title = {Customer relationship management: {Computer}-assisted tools for customer lifetime value prediction},
  isbn = {978-1-4244-6715-0},
  shorttitle = {Customer relationship management},
  url = {http://ieeexplore.ieee.org/document/5561487/},
  doi = {10.1109/ITSIM.2010.5561487},
  urldate = {2020-09-07},
  booktitle = {2010 {International} {Symposium} on {Information} {Technology}},
  publisher = {IEEE},
  author = {Lim Chia Yean and K T Vincent Khoo},
  month = {jun},
  year = {2010},
  abstract = {A customer lifetime value (CLV) can be used as an indicator for assessing a customer's worthiness. It is often used among the medium to large organizations for predicting future revenues from their customers. Many research projects have been conducted to enhance the individual prediction models and formulae. However, there are no obvious formal approaches to ensure the consistency and customizability of a prediction process and its major elements. This paper proposes a basic set of computer-assisted tools for a CLV prediction process. The semi-automated tools could be used by the business analysts of an organization to customize the existing templates; record, monitor, and track the work done in each phase of the process. In this paper, various challenges are discussed together with their possible solutions. The tools are specifically designed and implemented for the derivation of customizable formulae and models among the different customer segments of a non-profit organization. In the near future, more research work would be conducted to enhance the capabilities of the tools to cater for possible collaborative CLV predictions among the interested parties on the variables, algorithms, and the subsystems of a model base management system and the associated knowledge base management system.},
  pages = {1180--1185},
}

@Article{risselada_staying_2010,
  title = {Staying {Power} of {Churn} {Prediction} {Models}},
  volume = {24},
  issn = {10949968},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1094996810000253},
  doi = {10.1016/j.intmar.2010.04.002},
  language = {en},
  number = {3},
  urldate = {2020-09-07},
  journal = {Journal of Interactive Marketing},
  author = {Hans Risselada and Peter C. Verhoef and Tammo H.A. Bijmolt},
  month = {aug},
  year = {2010},
  abstract = {In this paper, we study the staying power of various churn prediction models. Staying power is defined as the predictive performance of a model in a number of periods after the estimation period. We examine two methods, logit models and classification trees, both with and without applying a bagging procedure. Bagging consists of averaging the results of multiple models that have each been estimated on a bootstrap sample from the original sample. We test the models using customer data of two firms from different industries, namely the internet service provider and insurance markets. The results show that the classification tree in combination with a bagging procedure outperforms the other three methods. It is shown that the ability to identify high risk customers of this model is similar for the in-period and one-period-ahead forecasts. However, for all methods the staying power is rather low, as the predictive performance deteriorates considerably within a few periods after the estimation period. This is due to the fact that both the parameter estimates change over time and the fact that the variables that are significant differ between periods. Our findings indicate that churn models should be adapted regularly. We provide a framework for database analysts to reconsider their methods used for churn modeling and to assess for how long they can use an estimated model.},
  pages = {198--208},
}

@InProceedings{khakabi_data_2010,
  address = {Liverpool, United Kingdom},
  title = {Data {Mining} {Applications} in {Customer} {Churn} {Management}},
  isbn = {978-1-4244-5984-1},
  url = {http://ieeexplore.ieee.org/document/5416090/},
  doi = {10.1109/ISMS.2010.49},
  urldate = {2020-09-07},
  booktitle = {2010 {International} {Conference} on {Intelligent} {Systems}, {Modelling} and {Simulation}},
  publisher = {IEEE},
  author = {Sahand KhakAbi and Mohammad R. Gholamian and Morteza Namvar},
  month = {jan},
  year = {2010},
  abstract = {According to the lack of a comprehensive literature review in the area of application of data mining in customer churn management, which has become a central issue in customer relationship management nowadays, this paper tackles to provide a brief review of researches in that field from two perspectives: techniques used and statistical reports. From the first point of view, a taxonomy based on the models exploited in papers is provided. The latter perspective gives insight into the trend, place and frequency of publications in the mentioned area. Also some of recent papers are summarized for the sake of interested readers. It is suggested that this paper will point out the gaps and strengths in this research issue, which might be of interest for contributors and researches and business managers respectively.},
  pages = {220--225},
}

@Article{tsai_variable_2010,
  title = {Variable selection by association rules for customer churn prediction of multimedia on demand},
  volume = {37},
  issn = {09574174},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417409006459},
  doi = {10.1016/j.eswa.2009.06.076},
  language = {en},
  number = {3},
  urldate = {2020-09-07},
  journal = {Expert Systems with Applications},
  author = {Chih-Fong Tsai and Mao-Yuan Chen},
  month = {mar},
  year = {2010},
  abstract = {Multimedia on demand (MOD) is an interactive system that provides a number of value-added services in addition to traditional TV services, such as video on demand and interactive online learning. This opens a new marketing and managerial problem for the telecommunication industry to retain valuable MOD customers. Data mining techniques have been widely applied to develop customer churn prediction models, such as neural networks and decision trees in the domain of mobile telecommunication. However, much related work focuses on developing the prediction models per se. Few studies consider the pre-processing step during data mining whose aim is to filter out unrepresentative data or information. This paper presents the important processes of developing MOD customer churn prediction models by data mining techniques. They contain the pre-processing stage for selecting important variables by association rules, which have not been applied before, the model construction stage by neural networks (NN) and decision trees (DT), which are widely adapted in the literature, and four evaluation measures including prediction accuracy, precision, recall, and F-measure, all of which have not been considered to examine the model performance. The source data are based on one telecommunication company providing the MOD services in Taiwan, and the experimental results show that using association rules allows the DT and NN models to provide better prediction performances over a chosen validation dataset. In particular, the DT model performs better than the NN model. Moreover, some useful and important rules in the DT model, which show the factors affecting a high proportion of customer churn, are also discussed for the marketing and managerial purpose.},
  pages = {2006--2015},
}

@Article{coussement_improved_2010,
  title = {Improved marketing decision making in a customer churn prediction context using generalized additive models},
  volume = {37},
  issn = {09574174},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417409007325},
  doi = {10.1016/j.eswa.2009.07.029},
  language = {en},
  number = {3},
  urldate = {2020-09-07},
  journal = {Expert Systems with Applications},
  author = {Kristof Coussement and Dries F. Benoit and Dirk {Van den Poel}},
  month = {mar},
  year = {2010},
  abstract = {Nowadays, companies are investing in a well-considered CRM strategy. One of the cornerstones in CRM is customer churn prediction, where one tries to predict whether or not a customer will leave the company. This study focuses on how to better support marketing decision makers in identifying risky customers by using Generalized Additive Models (GAM). Compared to Logistic Regression, GAM relaxes the linearity constraint which allows for complex non-linear fits to the data. The contributions to the literature are three-fold: (i) it is shown that GAM is able to improve marketing decision making by better identifying risky customers; (ii) it is shown that GAM increases the interpretability of the churn model by visualizing the non-linear relationships with customer churn identifying a quasi-exponential, a U, an inverted U or a complex trend and (iii) marketing managers are able to significantly increase business value by applying GAM in this churn prediction context.},
  pages = {2132--2143},
}

@Article{tsai_data_2010,
  title = {Data {Mining} {Techniques} in {Customer} {Churn} {Prediction}},
  volume = {3},
  issn = {18744796},
  url = {http://www.benthamdirect.org/pages/content.php?CSENG/2010/00000003/00000001/0004CSENG.SGM},
  doi = {10.2174/1874479611003010028},
  language = {en},
  number = {1},
  urldate = {2020-09-07},
  journal = {Recent Patents on Computer Science},
  author = {Chih-Fong Tsai and Yu-Hsin Lu},
  month = {feb},
  year = {2010},
  pages = {28--32},
  file = {Tsai_Lu_2010_Data Mining Techniques in Customer Churn Prediction.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\9KENCIQX\\Tsai_Lu_2010_Data Mining Techniques in Customer Churn Prediction.pdf:application/pdf},
}

@Article{thomas_consumer_2010,
  title = {Consumer finance: challenges for operational research},
  volume = {61},
  issn = {0160-5682, 1476-9360},
  shorttitle = {Consumer finance},
  url = {https://www.tandfonline.com/doi/full/10.1057/jors.2009.104},
  doi = {10.1057/jors.2009.104},
  language = {en},
  number = {1},
  urldate = {2020-09-07},
  journal = {Journal of the Operational Research Society},
  author = {L C Thomas},
  month = {jan},
  year = {2010},
  pages = {41--52},
  abstract = {Consumer finance has become one of the most important areas of banking, both because of the amount of money being lent and the impact of such credit on global economy and the realisation that the credit crunch of 2008 was partly due to incorrect modelling of the risks in such lending. This paper reviews the development of credit scoring—the way of assessing risk in consumer finance—and what is meant by a credit score. It then outlines 10 challenges for Operational Research to support modelling in consumer finance. Some of these involve developing more robust risk assessment systems, whereas others are to expand the use of such modelling to deal with the current objectives of lenders and the new decisions they have to make in consumer finance.},
  file = {Thomas_2010_Consumer finance.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\CLVYUZDU\\Thomas_2010_Consumer finance.pdf:application/pdf},
}

@InCollection{hutchison_data_2009,
  address = {Berlin, Heidelberg},
  title = {Data {Mining} {Using} {Rules} {Extracted} from {SVM}: {An} {Application} to {Churn} {Prediction} in {Bank} {Credit} {Cards}},
  volume = {5908},
  isbn = {978-3-642-10645-3 978-3-642-10646-0},
  shorttitle = {Data {Mining} {Using} {Rules} {Extracted} from {SVM}},
  url = {http://link.springer.com/10.1007/978-3-642-10646-0_47},
  language = {en},
  urldate = {2020-09-07},
  booktitle = {Rough {Sets}, {Fuzzy} {Sets}, {Data} {Mining} and {Granular} {Computing}},
  publisher = {Springer Berlin Heidelberg},
  author = {M. A. H. Farquad and V. Ravi and S. Bapi Raju},
  editor = {David Hutchison and Takeo Kanade and Josef Kittler and Jon M. Kleinberg and Friedemann Mattern and John C. Mitchell and Moni Naor and Oscar Nierstrasz and C. {Pandu Rangan} and Bernhard Steffen and Madhu Sudan and Demetri Terzopoulos and Doug Tygar and Moshe Y. Vardi and Gerhard Weikum and Hiroshi Sakai and Mihir Kumar Chakraborty and Aboul Ella Hassanien and Dominik Ślęzak and William Zhu},
  year = {2009},
  doi = {10.1007/978-3-642-10646-0_47},
  abstract = {In this work, an eclectic procedure for rule extraction from Support Vector Machine is proposed, where Tree is generated using Naïve Bayes Tree (NBTree) resulting in the SVM+NBTree hybrid. The data set analyzed in this paper is about churn prediction in bank credit cards and is obtained from Business Intelligence Cup 2004. The data set under consideration is highly unbalanced with 93.11% loyal and 6.89% churned customers. Since identifying churner is of paramount importance from business perspective, sensitivity of classification model is more critical. Using the available, original unbalanced data only, we observed that the proposed hybrid SVM+NBTree yielded the best sensitivity compared to other classifiers.},
  note = {Series Title: Lecture Notes in Computer Science},
  pages = {390--397},
}

@Article{lessmann_reference_2009,
  title = {A reference model for customer-centric data mining with support vector machines},
  volume = {199},
  issn = {03772217},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0377221708010515},
  doi = {10.1016/j.ejor.2008.12.017},
  language = {en},
  number = {2},
  urldate = {2020-09-07},
  journal = {European Journal of Operational Research},
  author = {Stefan Lessmann and Stefan Voß},
  month = {dec},
  year = {2009},
  abstract = {Supervised classification is an important part of corporate data mining to support decision making in customer-centric planning tasks. The paper proposes a hierarchical reference model for support vector machine based classification within this discipline. The approach balances the conflicting goals of transparent yet accurate models and compares favourably to alternative classifiers in a large-scale empirical evaluation in real-world customer relationship management applications. Recent advances in support vector machine oriented research are incorporated to approach feature, instance and model selection in a unified framework.},
  pages = {520--530},
}

@Article{benoit_benefits_2009,
  title = {Benefits of quantile regression for the analysis of customer lifetime value in a contractual setting: {An} application in financial services},
  volume = {36},
  issn = {09574174},
  shorttitle = {Benefits of quantile regression for the analysis of customer lifetime value in a contractual setting},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417409000712},
  doi = {10.1016/j.eswa.2009.01.031},
  language = {en},
  number = {7},
  urldate = {2020-09-07},
  journal = {Expert Systems with Applications},
  author = {Dries F. Benoit and Dirk {Van den Poel}},
  month = {sep},
  year = {2009},
  abstract = {The move towards a customer-centred approach to marketing, coupled with the increasing availability of customer transaction data, has led to an interest in understanding and estimating customer lifetime value (CLV). Several authors point out that, when evaluating customer profitability, profitable customers are rare compared to the unprofitable ones. In spite of this, most authors fail to recognize the implications of these skewed distributions on the performance of models they use. In this study, we propose analyzing CLV by means of quantile regression. In a financial services application, we show that this technique provides management more in-depth insights into the effects of the covariates that are missed with linear regression. Moreover, we show that in the common situation where interest is in a top-customer segment, quantile regression outperforms linear regression. The method also has the ability of constructing prediction intervals. Combining the CLV point estimate with the prediction intervals leads to a new segmentation scheme that is the first to account for uncertainty in the predictions. This segmentation is ideally suited for managing the portfolio of customers.},
  pages = {10475--10484},
}

@Article{glady_modeling_2009,
  title = {Modeling churn using customer lifetime value},
  volume = {197},
  issn = {03772217},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0377221708004840},
  doi = {10.1016/j.ejor.2008.06.027},
  language = {en},
  number = {1},
  urldate = {2020-09-07},
  journal = {European Journal of Operational Research},
  author = {Nicolas Glady and Bart Baesens and Christophe Croux},
  month = {aug},
  year = {2009},
  abstract = {The definition and modeling of customer loyalty have been central issues in customer relationship management since many years. Recent papers propose solutions to detect customers that are becoming less loyal, also called churners. The churner status is then defined as a function of the volume of commercial transactions. In the context of a Belgian retail financial service company, our first contribution is to redefine the notion of customer loyalty by considering it from a customer-centric viewpoint instead of a product-centric one. We hereby use the customer lifetime value (CLV) defined as the discounted value of future marginal earnings, based on the customer’s activity. Hence, a churner is defined as someone whose CLV, thus the related marginal profit, is decreasing. As a second contribution, the loss incurred by the CLV decrease is used to appraise the cost to misclassify a customer by introducing a new loss function. In the empirical study, we compare the accuracy of various classification techniques commonly used in the domain of churn prediction, including two cost-sensitive classifiers. Our final conclusion is that since profit is what really matters in a commercial environment, standard statistical accuracy measures for prediction need to be revised and a more profit oriented focus may be desirable.},
  pages = {402--411},
}

@Article{nassirimofakham_electronic_2009,
  title = {Electronic promotion to new customers using {mkNN} learning},
  volume = {179},
  issn = {00200255},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0020025508004003},
  doi = {10.1016/j.ins.2008.09.019},
  language = {en},
  number = {3},
  urldate = {2020-09-07},
  journal = {Information Sciences},
  author = {F Nassirimofakham and M Nematbakhsh and A Baraanidastjerdi and N Ghasemaghaee},
  month = {jan},
  year = {2009},
  abstract = {In recent years, several techniques have been proposed to model electronic promotions for existing customers. However, these techniques are not applicable for new customers with no previous profile or behavior data. This study models promotions to new customers in an electronic marketplace. We introduce a multi-valued k-Nearest Neighbor (mkNN) learning capability for modeling promotions to new customers. In this modified learning algorithm, instead of a single product category, the seller sends the new customer a promotion on a variable set of m categories (where m is a variable) with the highest rank of desirability among the most similar previous customers. Previous studies consider sellers’ profits in promotion and marketing models. In addition to the sellers’ profits, three important factors – annoyance of customers, sellers’ reputations, and customers’ anonymity – are considered in this study. Without considering the customer’s profile, we minimize unrelated and disliked offers to reduce the customer’s annoyance and elevate the seller’s reputation. The promotion models are evaluated in two separate experiments on populations with different degrees of optimism: (1) with fixed number of customers; and (2) in a fixed period of time. The evaluation is based on the parameters of customer population size and behavior as well as time interval, seller payoff, seller reputation, and the number of promotions canceled by the customers. The simulation results demonstrate that the proposed mkNN-based promotion strategies are moderately efficient with respect to all parameters for providing services in a large population. In addition, purchasing preferences of past customers, which are based on periodic promotions that a seller sends to customers, can generate future rapidly expanding demands in the market. By using these approaches, an advertising company can send acceptable promotions to customers without having specific profile information.},
  pages = {248--266},
}

@Article{xie_customer_2009,
  title = {Customer churn prediction using improved balanced random forests},
  volume = {36},
  issn = {09574174},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417408004326},
  doi = {10.1016/j.eswa.2008.06.121},
  language = {en},
  number = {3},
  urldate = {2020-09-07},
  journal = {Expert Systems with Applications},
  author = {Yaya Xie and Xiu Li and E.W.T. Ngai and Weiyun Ying},
  month = {apr},
  year = {2009},
  abstract = {Churn prediction is becoming a major focus of banks in China who wish to retain customers by satisfying their needs under resource constraints. In churn prediction, an important yet challenging problem is the imbalance in the data distribution. In this paper, we propose a novel learning method, called improved balanced random forests (IBRF), and demonstrate its application to churn prediction. We investigate the effectiveness of the standard random forests approach in predicting customer churn, while also integrating sampling techniques and cost-sensitive learning into the approach to achieve a better performance than most existing algorithms. The nature of IBRF is that the best features are iteratively learned by altering the class distribution and by putting higher penalties on misclassification of the minority class. We apply the method to a real bank customer churn data set. It is found to improve prediction accuracy significantly compared with other algorithms, such as artificial neural networks, decision trees, and class-weighted core support vector machines (CWC-SVM). Moreover, IBRF also produces better prediction results than other random forests algorithms such as balanced random forests and weighted random forests.},
  pages = {5445--5449},
}

@Article{lima_domain_2009,
  title = {Domain knowledge integration in data mining using decision tables: case studies in churn prediction},
  volume = {60},
  issn = {0160-5682, 1476-9360},
  shorttitle = {Domain knowledge integration in data mining using decision tables},
  url = {https://www.tandfonline.com/doi/full/10.1057/jors.2008.161},
  doi = {10.1057/jors.2008.161},
  language = {en},
  number = {8},
  urldate = {2020-09-07},
  journal = {Journal of the Operational Research Society},
  author = {E Lima and C Mues and B Baesens},
  month = {aug},
  year = {2009},
  abstract = {Companies' interest in customer relationship modelling and key issues such as customer lifetime value and churn has substantially increased over the years. However, the complexity of building, interpreting and applying these models creates obstacles for their implementation. The main contribution of this paper is to show how domain knowledge can be incorporated in the data mining process for churn prediction, viz. through the evaluation of coefficient signs in a logistic regression model, and secondly, by analysing a decision table (DT) extracted from a decision tree or rule-based classifier. An algorithm to check DTs for violations of monotonicity constraints is presented, which involves the repeated application of condition reordering and table contraction to detect counter-intuitive patterns. Both approaches are applied to two telecom data sets to empirically demonstrate how domain knowledge can be used to ensure the interpretability of the resulting models.},
  pages = {1096--1106},
}

@Article{coussement_improving_2009,
  title = {Improving customer attrition prediction by integrating emotions from client/company interaction emails and evaluating multiple classifiers},
  volume = {36},
  issn = {09574174},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S095741740800479X},
  doi = {10.1016/j.eswa.2008.07.021},
  language = {en},
  number = {3},
  urldate = {2020-09-07},
  journal = {Expert Systems with Applications},
  author = {Kristof Coussement and Dirk Van den Poel},
  month = {apr},
  year = {2009},
  abstract = {Predicting customer churn with the purpose of retaining customers is a hot topic in academy as well as in today’s business environment. Targeting the right customers for a specific retention campaign carries a high priority. This study focuses on two aspects in which churn prediction models could be improved by (i) relying on customer information type diversity and (ii) choosing the best performing classification technique. (i) With the upcoming interest in new media (e.g. blogs, emails, ...), client/company interactions are facilitated. Consequently, new types of information are available which generate new opportunities to increase the prediction power of a churn model. This study contributes to the literature by finding evidence that adding emotions expressed in client/company emails increases the predictive performance of an extended RFM churn model. As a substantive contribution, an in-depth study of the impact of the emotionality indicators on churn behavior is done. (ii) This study compares three classification techniques – i.e. Logistic Regression, Support Vector Machines and Random Forests – to distinguish churners from non-churners. This paper shows that Random Forests is a viable opportunity to improve predictive performance compared to Support Vector Machines and Logistic Regression which both exhibit an equal performance.},
  pages = {6127--6134},
}

@Article{burez_handling_2009,
  title = {Handling class imbalance in customer churn prediction},
  volume = {36},
  issn = {09574174},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417408002121},
  doi = {10.1016/j.eswa.2008.05.027},
  language = {en},
  number = {3},
  urldate = {2020-09-07},
  journal = {Expert Systems with Applications},
  author = {J. Burez and D. {Van den Poel}},
  month = {apr},
  year = {2009},
  pages = {4626--4636},
  abstract = {Customer churn is often a rare event in service industries, but of great interest and great value. Until recently, however, class imbalance has not received much attention in the context of data mining [Weiss, G. M. (2004). Mining with rarity: A unifying framework. SIGKDD Explorations, 6(1), 7–19]. In this study, we investigate how we can better handle class imbalance in churn prediction. Using more appropriate evaluation metrics (AUC, lift), we investigated the increase in performance of sampling (both random and advanced under-sampling) and two specific modelling techniques (gradient boosting and weighted random forests) compared to some standard modelling techniques. AUC and lift prove to be good evaluation metrics. AUC does not depend on a threshold, and is therefore a better overall evaluation metric compared to accuracy. Lift is very much related to accuracy, but has the advantage of being well used in marketing practice [Ling, C., & Li, C. (1998). Data mining for direct marketing problems and solutions. In Proceedings of the fourth international conference on knowledge discovery and data mining (KDD-98). New York, NY: AAAI Press]. Results show that under-sampling can lead to improved prediction accuracy, especially when evaluated with AUC. Unlike Ling and Li [Ling, C., & Li, C. (1998). Data mining for direct marketing problems and solutions. In Proceedings of the fourth international conference on knowledge discovery and data mining (KDD-98). New York, NY: AAAI Press], we find that there is no need to under-sample so that there are as many churners in your training set as non churners. Results show no increase in predictive performance when using the advanced sampling technique CUBE in this study. This is in line with findings of Japkowicz [Japkowicz, N. (2000). The class imbalance problem: significance and strategies. In Proceedings of the 2000 international conference on artificial intelligence (IC-AI’2000): Special track on inductive learning, Las Vegas, Nevada], who noted that using sophisticated sampling techniques did not give any clear advantage. Weighted random forests, as a cost-sensitive learner, performs significantly better compared to random forests, and is therefore advised. It should, however always be compared to logistic regression. Boosting is a very robust classifier, but never outperforms any other technique.},
  file = {Burez_Van den Poel_2009_Handling class imbalance in customer churn prediction.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\HRXS8RRJ\\Burez and Van den Poel - 2009 - Handling class imbalance in customer churn predict.pdf:application/pdf},
}

@InProceedings{shao_construction_2008,
  address = {Jinan, Shandong, China},
  title = {Construction of {Bayesian} {Classifiers} with {GA} for {Predicting} {Customer} {Retention}},
  isbn = {978-0-7695-3304-9},
  url = {http://ieeexplore.ieee.org/document/4666835/},
  doi = {10.1109/ICNC.2008.724},
  urldate = {2020-09-07},
  booktitle = {2008 {Fourth} {International} {Conference} on {Natural} {Computation}},
  publisher = {IEEE},
  author = {Hongmei Shao and Gaofeng Zheng and Fengxian An},
  year = {2008},
  abstract = {In this paper, a novel Bayesian classifier model is constructed based on genetic algorithms (GA) for the prediction of customer churn. Experiment results have shown that it not only matches potential churning customers well from the large amount of validation data but also shows higher classifying precision compared with the other three Bayesian classifier models. Recently, the model has been adopted by a Japanese enterprise to successfully handle some business problems of potential churning customers prediction.},
  pages = {181--185},
}

@Article{burez_separating_2008,
  title = {Separating financial from commercial customer churn: {A} modeling step towards resolving the conflict between the sales and credit department},
  volume = {35},
  issn = {09574174},
  shorttitle = {Separating financial from commercial customer churn},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417407002813},
  doi = {10.1016/j.eswa.2007.07.036},
  language = {en},
  number = {1-2},
  urldate = {2020-09-07},
  journal = {Expert Systems with Applications},
  author = {J Burez and D Vandenpoel},
  month = {jul},
  year = {2008},
  pages = {497--514},
  abstract = {In subscription services, customers who leave the company can be divided into two groups: customers who do not renew their fixed-term contract at the end of that contract, and others who just stop paying during their contract to which they are legally bound. Those two separate processes are often modeled together in a so-called churn-prediction model, but are actually two different processes. The first type of churn can be considered commercial churn, i.e., customers making a studied choice not to renew their subscriptions. The second phenomenon is defined as financial churn, people who stop paying because they can no longer afford the service. The so-called marketing dilemma arises, as conflicting interests exist between the sales and marketing department on the one hand, and the legal and credit department on the other hand. This paper shows that the two different processes mentioned can be separated by using information from the internal database of the company and that previous bad-payment behavior is more important as a driver for financial than for commercial churn. Finally, it is shown on real-life data that one can more accurately predict financial churn than commercial churn (increasing within period as well as out-of-period prediction performance). Conversely, when trying to persuade customers to stay with the company, the impact of ‘loyalty’ actions is far greater with potential commercial churners as compared to financial churners. Evidence comes from a real-life field experiment.},
  file = {Burez_Vandenpoel_2008_Separating financial from commercial customer churn.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\QECS9HY9\\Burez_Vandenpoel_2008_Separating financial from commercial customer churn.pdf:application/pdf},
}

@Article{coussement_churn_2008,
  title = {Churn prediction in subscription services: {An} application of support vector machines while comparing two parameter-selection techniques},
  volume = {34},
  issn = {09574174},
  shorttitle = {Churn prediction in subscription services},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417406002806},
  doi = {10.1016/j.eswa.2006.09.038},
  language = {en},
  number = {1},
  urldate = {2020-09-07},
  journal = {Expert Systems with Applications},
  author = {Kristof Coussement and Dirk {Van den Poel}},
  month = {jan},
  year = {2008},
  abstract = {CRM gains increasing importance due to intensive competition and saturated markets. With the purpose of retaining customers, academics as well as practitioners find it crucial to build a churn prediction model that is as accurate as possible. This study applies support vector machines in a newspaper subscription context in order to construct a churn model with a higher predictive performance. Moreover, a comparison is made between two parameter-selection techniques, needed to implement support vector machines. Both techniques are based on grid search and cross-validation. Afterwards, the predictive performance of both kinds of support vector machine models is benchmarked to logistic regression and random forests. Our study shows that support vector machines show good generalization performance when applied to noisy marketing data. Nevertheless, the parameter optimization procedure plays an important role in the predictive performance. We show that only when the optimal parameter-selection procedure is applied, support vector machines outperform traditional logistic regression, whereas random forests outperform both kinds of support vector machines. As a substantive contribution, an overview of the most important churn drivers is given. Unlike ample research, monetary value and frequency do not play an important role in explaining churn in this subscription-services application. Even though most important churn predictors belong to the category of variables describing the subscription, the influence of several client/company-interaction variables cannot be neglected.},
  pages = {313--327},
}

@Article{donkers_modeling_2007,
  title = {Modeling {CLV}: {A} test of competing models in the insurance industry},
  volume = {5},
  issn = {1570-7156, 1573-711X},
  shorttitle = {Modeling {CLV}},
  url = {http://link.springer.com/10.1007/s11129-006-9016-y},
  doi = {10.1007/s11129-006-9016-y},
  language = {en},
  number = {2},
  urldate = {2020-09-07},
  journal = {Quantitative Marketing and Economics},
  author = {Bas Donkers and Peter C. Verhoef and Martijn G. {de Jong}},
  month = {jun},
  year = {2007},
  pages = {163--190},
  abstract = {Customer Lifetime Value (CLV) is one of the key metrics in marketing and is considered an important segmentation base. This paper studies the capabilities of a range of models to predict CLV in the insurance industry. The simplest models can be constructed at the customer relationship level, i.e. aggregated across all services. The more complex models focus on the individual services, paying explicit attention to cross buying, but also retention. The models build on a plethora of approaches used in the existing literature and include a status quo model, a Tobit II model, univariate and multivariate choice models, and duration models. For all models, CLV for each customer is computed for a four-year time horizon. We find that the simple models perform well. The more complex models are expected to better capture the richness of relationship development. Surprisingly, this does not lead to substantially better CLV predictions.},
  file = {Donkers et al_2007_Modeling CLV.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\XWKPQF9V\\Donkers et al_2007_Modeling CLV.pdf:application/pdf},
}

@Article{burez_crm_2007,
  title = {{CRM} at a pay-{TV} company: {Using} analytical models to reduce customer attrition by targeted marketing for subscription services},
  volume = {32},
  issn = {09574174},
  shorttitle = {{CRM} at a pay-{TV} company},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417405003374},
  doi = {10.1016/j.eswa.2005.11.037},
  language = {en},
  number = {2},
  urldate = {2020-09-07},
  journal = {Expert Systems with Applications},
  author = {Jonathan Burez and Dirk {Van den Poel}},
  month = {feb},
  year = {2007},
  pages = {277--288},
  abstract = {The early detection of potential churners enables companies to target these customers using specific retention actions, and subsequently increase profits. This analytical CRM (Customer Relationship Management) approach is illustrated using real-life data of a European pay-TV company. Their very high churn rate has had a devastating effect on their customer base. This paper first develops different churn-prediction models: the introduction of Markov chains in churn prediction, and a random forest model are benchmarked to a basic logistic model. The most appropriate model is subsequently used to target those customers with a high churn probability in a field experiment. Three alternative courses of marketing action are applied: giving free incentives, organizing special customer events, obtaining feedback on customer satisfaction through questionnaires. The results of this field experiment show that profits can be doubled using our churn-prediction model. Moreover, profits vary enormously with respect to the selected retention action, indicating that a customer satisfaction questionnaire yields the best results, a phenomenon known in the psychological literature as the ‘mere-measurement effect’.},
  file = {Burez_Van den Poel_2007_CRM at a pay-TV company.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\97UR8STW\\Burez_Van den Poel_2007_CRM at a pay-TV company.pdf:application/pdf},
}

@Article{buckinx_predicting_2007,
  title = {Predicting customer loyalty using the internal transactional database},
  volume = {32},
  issn = {09574174},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417405003143},
  doi = {10.1016/j.eswa.2005.11.004},
  language = {en},
  number = {1},
  urldate = {2020-09-07},
  journal = {Expert Systems with Applications},
  author = {Wouter Buckinx and Geert Verstraeten and Dirk {Van den Poel}},
  month = {jan},
  year = {2007},
  abstract = {Loyalty and targeting are central topics in Customer Relationship Management. Yet, the information that resides in customer databases only records transactions at a single company, whereby customer loyalty is generally unavailable. In this study, we enrich the customer database with a prediction of a customer’s behavioral loyalty such that it can be deployed for targeted marketing actions without the necessity to measure the loyalty of every single customer. To this end, we compare multiple linear regression with two state-of-the-art machine learning techniques (random forests and automatic relevance determination neural networks), and we show that (i) a customer’s behavioral loyalty can be predicted to a reasonable degree using the transactional database, (ii) given that overfitting is controlled for by the variable-selection procedure we propose in this study, a multiple linear regression model significantly outperforms the other models, (iii) the proposed variable-selection procedure has a beneficial impact on the reduction of multicollinearity, and (iv) the most important indicator of behavioral loyalty consists of the variety of products previously purchased.},
  pages = {125--134},
}

@Article{lemmens_bagging_2006,
  title = {Bagging and {Boosting} {Classification} {Trees} to {Predict} {Churn}},
  volume = {43},
  issn = {0022-2437, 1547-7193},
  url = {http://journals.sagepub.com/doi/10.1509/jmkr.43.2.276},
  doi = {10.1509/jmkr.43.2.276},
  language = {en},
  number = {2},
  urldate = {2020-09-07},
  journal = {Journal of Marketing Research},
  author = {Aurélie Lemmens and Christophe Croux},
  month = {may},
  year = {2006},
  pages = {276--286},
  abstract = {In this article, the authors explore the bagging and boosting classification techniques. They apply the two techniques to a customer database of an anonymous U.S. wireless telecommunications company, and both significantly improve accuracy in predicting churn. This higher predictive performance could ultimately lead to incremental profits for companies that use these methods. Furthermore, the results recommend the use of a balanced sampling scheme when predicting a rare event from large data sets, but this requires an appropriate bias correction.},
  file = {Lemmens_Croux_2006_Bagging and Boosting Classification Trees to Predict Churn.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\J3K3IUX2\\Lemmens_Croux_2006_Bagging and Boosting Classification Trees to Predict Churn.pdf:application/pdf},
}

@Article{lariviere_predicting_2005,
  title = {Predicting customer retention and profitability by using random forests and regression forests techniques},
  volume = {29},
  issn = {09574174},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417405000965},
  doi = {10.1016/j.eswa.2005.04.043},
  language = {en},
  number = {2},
  urldate = {2020-09-07},
  journal = {Expert Systems with Applications},
  author = {B Lariviere and D Vandenpoel},
  month = {aug},
  year = {2005},
  abstract = {In an era of strong customer relationship management (CRM) emphasis, firms strive to build valuable relationships with their existing customer base. In this study, we attempt to better understand three important measures of customer outcome: next buy, partial-defection and customers' profitability evolution. By means of random forests techniques we investigate a broad set of explanatory variables, including past customer behavior, observed customer heterogeneity and some typical variables related to intermediaries. We analyze a real-life sample of 100,000 customers taken from the data warehouse of a large European financial services company. Two types of random forests techniques are employed to analyze the data: random forests are used for binary classification, whereas regression forests are applied for the models with linear dependent variables. Our research findings demonstrate that both random forests techniques provide better fit for the estimation and validation sample compared to ordinary linear regression and logistic regression models. Furthermore, we find evidence that the same set of variables have a different impact on buying versus defection versus profitability behavior. Our findings suggest that past customer behavior is more important to generate repeat purchasing and favorable profitability evolutions, while the intermediary's role has a greater impact on the customers' defection proneness. Finally, our results demonstrate the benefits of analyzing different customer outcome variables simultaneously, since an extended investigation of the next buy–partial-defection–customer profitability triad indicates that one cannot fully understand a particular outcome without understanding the other related behavioral outcome variables.},
  pages = {472--484},
}

@Article{buckinx_customer_2005,
  title = {Customer base analysis: partial defection of behaviourally loyal clients in a non-contractual {FMCG} retail setting},
  volume = {164},
  issn = {03772217},
  shorttitle = {Customer base analysis},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0377221703009184},
  doi = {10.1016/j.ejor.2003.12.010},
  language = {en},
  number = {1},
  urldate = {2020-09-07},
  journal = {European Journal of Operational Research},
  author = {Wouter Buckinx and Dirk {Van den Poel}},
  month = {jul},
  year = {2005},
  abstract = {Customer relationship management (CRM) enjoys increasing attention as a countermeasure to switching behaviour of customers. Because foregone profits of (partially) defected customers can be significant, an increase of the retention rate can be very profitable. In this paper we focus on the treatment of a company's most behaviourally loyal customers in a non-contractual setting. We build a model in order to predict partial defection by behaviourally loyal clients using three classification techniques: Logistic regression, automatic relevance determination (ARD) Neural Networks and Random Forests. Focusing on partial attrition of high-frequency shoppers who exhibit a regular visit pattern may overcome the problem of unidentifiability of total defection in non-contractual settings. Classification accuracy (PCC) and area under the receiver operating characteristic curve (AUC) are used to evaluate classifier performance on a test/hold-out sample. Using real-life data from an FMCG retailer, we show that future partial defection can be successfully predicted, i.e. exceeding the benchmark hurdle of the null model. There are no significant differences in terms of performance among alternative classification techniques. Similar to direct-marketing applications we find that past behavioural variables, more specifically RFM variables (recency, frequency, and monetary value) are the best predictors of partial customer defection. This set of variables complements demographic variables confirming findings by other authors about its importance in predicting churn behaviour. Moreover, additional variables (listed in decreasing order of importance) such as the length of customer relationship, mode of payment, buying behaviour across categories, usage of promotions and brand purchase behaviour are shown to be moderately useful to incorporate in attrition models.},
  pages = {252--268},
}

@Article{wai-ho_au_novel_2003,
  title = {A novel evolutionary data mining algorithm with applications to churn prediction},
  volume = {7},
  issn = {1089-778X},
  url = {http://ieeexplore.ieee.org/document/1255389/},
  doi = {10.1109/TEVC.2003.819264},
  language = {en},
  number = {6},
  urldate = {2020-09-07},
  journal = {IEEE Transactions on Evolutionary Computation},
  author = {{Wai-Ho Au} and K.C.C. Chan and {Xin Yao}},
  month = {dec},
  year = {2003},
  pages = {532--545},
  abstract = {Classification is an important topic in data mining research. Given a set of data records, each of which belongs to one of a number of predefined classes, the classification problem is concerned with the discovery of classification rules that can allow records with unknown class membership to be correctly classified. Many algorithms have been developed to mine large data sets for classification models and they have been shown to be very effective. However, when it comes to determining the likelihood of each classification made, many of them are not designed with such purpose in mind. For this, they are not readily applicable to such problems as churn prediction. For such an application, the goal is not only to predict whether or not a subscriber would switch from one carrier to another, it is also important that the likelihood of the subscriber's doing so be predicted. The reason for this is that a carrier can then choose to provide a special personalized offer and services to those subscribers who are predicted with higher likelihood to churn. Given its importance, we propose a new data mining algorithm, called data mining by evolutionary learning (DMEL), to handle classification problems of which the accuracy of each predictions made has to be estimated. In performing its tasks, DMEL searches through the possible rule space using an evolutionary approach that has the following characteristics: 1) the evolutionary process begins with the generation of an initial set of first-order rules (i.e., rules with one conjunct/condition) using a probabilistic induction technique and based on these rules, rules of higher order (two or more conjuncts) are obtained iteratively; 2) when identifying interesting rules, an objective interestingness measure is used; 3) the fitness of a chromosome is defined in terms of the probability that the attribute values of a record can be correctly determined using the rules it encodes; and 4) the likelihood of predictions (or classifications...},
  file = {Wai-Ho Au et al_2003_A novel evolutionary data mining algorithm with applications to churn prediction.pdf:C\:\\nuvem\\OneDrive - Instituto Politécnico de Santarém\\zotero\\storage\\BPVMQWST\\Wai-Ho Au et al_2003_A novel evolutionary data mining algorithm with applications to churn prediction.pdf:application/pdf},
}

@Article{wei_turning_2002,
  title = {Turning telecommunications call details to churn prediction: a data mining approach},
  volume = {23},
  issn = {09574174},
  shorttitle = {Turning telecommunications call details to churn prediction},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417402000301},
  doi = {10.1016/S0957-4174(02)00030-1},
  language = {en},
  number = {2},
  urldate = {2020-09-07},
  journal = {Expert Systems with Applications},
  author = {Chih-Ping Wei and I-Tang Chiu},
  month = {aug},
  year = {2002},
  abstract = {As deregulation, new technologies, and new competitors open up the mobile telecommunications industry, churn prediction and management has become of great concern to mobile service providers. A mobile service provider wishing to retain its subscribers needs to be able to predict which of them may be at-risk of changing services and will make those subscribers the focus of customer retention efforts. In response to the limitations of existing churn-prediction systems and the unavailability of customer demographics in the mobile telecommunications provider investigated, we propose, design, and experimentally evaluate a churn-prediction technique that predicts churning from subscriber contractual information and call pattern changes extracted from call details. This proposed technique is capable of identifying potential churners at the contract level for a specific prediction time-period. In addition, the proposed technique incorporates the multi-classifier class-combiner approach to address the challenge of a highly skewed class distribution between churners and non-churners. The empirical evaluation results suggest that the proposed call-behavior-based churn-prediction technique exhibits satisfactory predictive effectiveness when more recent call details are employed for the churn prediction model construction. Furthermore, the proposed technique is able to demonstrate satisfactory or reasonable predictive power within the one-month interval between model construction and churn prediction. Using a previous demographics-based churn-prediction system as a reference, the lift factors attained by our proposed technique appear largely satisfactory.},
  pages = {103--112},
}
