@Article{MARTINS2020102215,
  title = {Deep neural network for complex open-water wetland mapping using high-resolution WorldView-3 and airborne LiDAR data},
  journal = {International Journal of Applied Earth Observation and Geoinformation},
  volume = {93},
  pages = {102215},
  year = {2020},
  issn = {0303-2434},
  doi = {https://doi.org/10.1016/j.jag.2020.102215},
  url = {http://www.sciencedirect.com/science/article/pii/S0303243420303081},
  author = {Vitor S. Martins and Amy L. Kaleita and Brian K. Gelder and Gustavo W. Nagel and Daniel A. Maciel},
  keywords = {Deep learning, Small wetlands, Machine learning, Optical and LiDAR data, PCA},
  abstract = {Wetland inventory maps are essential information for the conservation and management of natural wetland areas. The classification framework is crucial for successful mapping of complex wetlands, including the model selection, input variables and training procedures. In this context, deep neural network (DNN) is a powerful technique for remote sensing image classification, but this model application for wetland mapping has not been discussed in the previous literature, especially using commercial WorldView-3 data. This study developed a new framework for wetland mapping using DNN algorithm and WorldView-3 image in the Millrace Flats Wildlife Management Area, Iowa, USA. The study area has several wetlands with a variety of shapes and sizes, and the minimum mapping unit was defined as 20 m2 (0.002 ha). A set of potential variables was derived from WorldView-3 and auxiliary LiDAR data, and a feature selection procedure using principal components analysis (PCA) was used to identify the most important variables for wetland classification. Furthermore, traditional machine learning methods (support vector machine, random forest and k-nearest neighbor) were also implemented for the comparison of results. In general, the results show that DNN achieved satisfactory results in the study area (overall accuracy = 93.33 %), and we observed a high spatial overlap between reference and classified wetland polygons (Jaccard index ∼0.8). Our results confirm that PCA-based feature selection was effective in the optimization of DNN performance, and vegetation and textural indices were the most informative variables. In addition, the comparison of results indicated that DNN classification achieved relatively similar accuracies to other methods. The total classification errors vary from 0.104 to 0.111 among the methods, and the overlapped areas between reference and classified polygons range between 87.93 and 93.33 %. Finally, the findings of this study have three main implications. First, the integration of DNN model and WorldView-3 image is useful for wetland mapping at 1.2-m, but DNN results did not outperform other methods in this study area. Second, the feature selection was important for model performance, and the combination of most relevant input parameters contributes to the success of all tested models. Third, the spatial resolution of WorldView-3 is appropriate to preserve the shape and extent of small wetlands, while the application of medium resolution image (30-m) has a negative impact on the accurate delineation of these areas. Since commercial satellite data are becoming more affordable for remote sensing users, this study provides a framework that can be utilized to integrate very high-resolution imagery and deep learning in the classification of complex wetland areas.},
}

@Article{HILAS200911559,
  title = {Designing an expert system for fraud detection in private telecommunications networks},
  journal = {Expert Systems with Applications},
  volume = {36},
  number = {9},
  pages = {11559 - 11569},
  year = {2009},
  issn = {0957-4174},
  doi = {https://doi.org/10.1016/j.eswa.2009.03.031},
  url = {http://www.sciencedirect.com/science/article/pii/S095741740900284X},
  author = {Constantinos S. Hilas},
  keywords = {Fraud detection, User modeling, Expert systems, Telecommunications, Data mining applications},
  abstract = {Telecommunications fraud not only burdens telecom provider’s accountings but burdens individual users as well. The latter are particularly affected in the case of superimposed fraud where the fraudster uses a legitimate user’s account in parallel with the user. These cases are usually identified after user complaints for excess billing. However, inside the network of a large firm or organization, superimposed fraud may go undetected for some time. The present paper deals with the detection of fraudulent telecom activity inside large organizations’ premises. Focus is given on superimposed fraud detection. The problem is attacked via the construction of an expert system which incorporates both the network administrator’s expert knowledge and knowledge derived from the application of data mining techniques on real world data.},
}

@Article{WANG2015388,
  title = {Fuzzy Inference Algorithm based on Quantitative Association Rules},
  journal = {Procedia Computer Science},
  volume = {61},
  pages = {388 - 394},
  year = {2015},
  note = {Complex Adaptive Systems San Jose, CA November 2-4, 2015},
  issn = {1877-0509},
  doi = {https://doi.org/10.1016/j.procs.2015.09.166},
  url = {http://www.sciencedirect.com/science/article/pii/S1877050915029968},
  author = {Ling Wang and Ji-Yuan Dong and Shu-Lin Li},
  keywords = {quantitative association rules, apriori algorithm, discretization, TS fuzzy rules ;},
  abstract = {In order to develop a data mining system to extract the fuzzy inference rules from the data, in this paper a fuzzy inference algorithm based on quantitative association rule (FI-QAR) is proposed. First, a discretization algorithm based on an improved clustering for each dimension data is adopted, and then the quantitative results are represented in the form of a Nominal variables matrix to compute the support and confidence level in the Apriori algorithm for quantitative association rules mining. On the basis of this, the quantitative association fuzzy rules are reconstructed by combing with TS fuzzy model to realize fuzzy inference, which can be applied to predict the output class and precise output. Experiment results demonstrated that the proposed algorithm is feasible and practical.},
}

@Article{MOSTAFA2013627,
  title = {Citizens as consumers: Profiling e-government services’ users in Egypt via data mining techniques},
  journal = {International Journal of Information Management},
  volume = {33},
  number = {4},
  pages = {627 - 641},
  year = {2013},
  issn = {0268-4012},
  doi = {https://doi.org/10.1016/j.ijinfomgt.2013.03.007},
  url = {http://www.sciencedirect.com/science/article/pii/S0268401213000510},
  author = {Mohamed M. Mostafa and Ahmed A. El-Masry},
  keywords = {e-Government services, Consumer profiling, Neural networks, Data mining, Egypt},
  abstract = {This study uses data mining techniques to examine the effect of various demographic, cognitive and psychographic factors on Egyptian citizens’ use of e-government services. Data mining uses a broad family of computationally intensive methods that include decision trees, neural networks, rule induction, machine learning and graphic visualization. Three artificial neural network models (multi-layer perceptron neural network [MLP], probabilistic neural network [PNN] and self-organizing maps neural network [SOM]) and three machine learning techniques (classification and regression trees [CART], multivariate adaptive regression splines [MARS], and support vector machines [SVM]) are compared to a standard statistical method (linear discriminant analysis [LDA]). The variable sets considered are sex, age, educational level, e-government services perceived usefulness, ease of use, compatibility, subjective norms, trust, civic mindedness, and attitudes. The study shows how it is possible to identify various dimensions of e-government services usage behavior by uncovering complex patterns in the dataset, and also shows the classification abilities of data mining techniques.},
}

@Article{MARTINEZ2020588,
  title = {A machine learning framework for customer purchase prediction in the non-contractual setting},
  journal = {European Journal of Operational Research},
  volume = {281},
  number = {3},
  pages = {588 - 596},
  year = {2020},
  note = {Featured Cluster: Business Analytics: Defining the field and identifying a research agenda},
  issn = {0377-2217},
  doi = {https://doi.org/10.1016/j.ejor.2018.04.034},
  url = {http://www.sciencedirect.com/science/article/pii/S0377221718303370},
  author = {Andrés Martínez and Claudia Schmuck and Sergiy Pereverzyev and Clemens Pirker and Markus Haltmeier},
  keywords = {Analytics, Purchase prediction, Sales forecast, Non-contractual setting, Machine learning},
  abstract = {Predicting future customer behavior provides key information for efficiently directing resources at sales and marketing departments. Such information supports planning the inventory at the warehouse and point of sales, as well strategic decisions during manufacturing processes. In this paper, we develop advanced analytics tools that predict future customer behavior in the non-contractual setting. We establish a dynamic and data driven framework for predicting whether a customer is going to make purchase at the company within a certain time frame in the near future. For that purpose, we propose a new set of customer relevant features that derives from times and values of previous purchases. These customer features are updated every month, and state of the art machine learning algorithms are applied for purchase prediction. In our studies, the gradient tree boosting method turns out to be the best performing method. Using a data set containing more than 10 000 customers and a total number of 200 000 purchases we obtain an accuracy score of 89% and an AUC value of 0.95 for predicting next moth purchases on the test data set.},
}

@Article{CASCIO2019284,
  title = {Training trends: Macro, micro, and policy issues},
  journal = {Human Resource Management Review},
  volume = {29},
  number = {2},
  pages = {284 - 297},
  year = {2019},
  note = {Advancing Training for the 21st Century},
  issn = {1053-4822},
  doi = {https://doi.org/10.1016/j.hrmr.2017.11.001},
  url = {http://www.sciencedirect.com/science/article/pii/S105348221730089X},
  author = {Wayne F. Cascio},
  keywords = {Globalization, Demographic changes, Training trends, Digital technology and work, Labor markets, Non-standard workers, Middle skills, Vocational education, Micro-learning},
  abstract = {The scope of the training enterprise is vast, the field is dynamic, and multi-level issues confront training researchers. After identifying three “mega trends” – globalization, technology, and demographic changes - this paper reviews training trends at the macro level, the micro level, and emerging policy issues and links each one to the mega trends. The macro-level trends - increasing demands for personal and professional development by job seekers and employees, the effects of digital technology on work, structural changes in labor markets, increasing training opportunities for non-standard workers, and training as an important aspect of an employer's brand - reflect broad trends in the economy. Micro-level trends - better understanding of requirements for effective learning; use of short, digital lessons; and options for optimizing learning and preventing skill and knowledge decay - each focus on improving the quality of training. Policy issues - training needs in small and medium-sized enterprises, the need for “middle skills”, and vocational education - raise vexing issues for all stakeholders. Together, macro, micro, and policy issues reflect ongoing challenges for researchers, practitioners, and policy makers everywhere.},
}

@Article{FARQUAD201431,
  title = {Churn prediction using comprehensible support vector machine: An analytical CRM application},
  journal = {Applied Soft Computing},
  volume = {19},
  pages = {31 - 40},
  year = {2014},
  issn = {1568-4946},
  doi = {https://doi.org/10.1016/j.asoc.2014.01.031},
  url = {http://www.sciencedirect.com/science/article/pii/S1568494614000507},
  author = {M.A.H. Farquad and Vadlamani Ravi and S. Bapi Raju},
  keywords = {Churn prediction, Support vector machine, Rule extraction, Naive Bayes Tree, Machine learning and customer relationship management},
  abstract = {Support vector machine (SVM) is currently state-of-the-art for classification tasks due to its ability to model nonlinearities. However, the main drawback of SVM is that it generates “black box” model, i.e. it does not reveal the knowledge learnt during training in human comprehensible form. The process of converting such opaque models into a transparent model is often regarded as rule extraction. In this paper we proposed a hybrid approach for extracting rules from SVM for customer relationship management (CRM) purposes. The proposed hybrid approach consists of three phases. (i) During first phase; SVM-RFE (SVM-recursive feature elimination) is employed to reduce the feature set. (ii) Dataset with reduced features is then used in the second phase to obtain SVM model and support vectors are extracted. (iii) Rules are then generated using Naive Bayes Tree (NBTree) in the final phase. The dataset analyzed in this research study is about Churn prediction in bank credit card customer (Business Intelligence Cup 2004) and it is highly unbalanced with 93.24% loyal and 6.76% churned customers. Further we employed various standard balancing approaches to balance the data and extracted rules. It is observed from the empirical results that the proposed hybrid outperformed all other techniques tested. As the reduced feature dataset is used, it is also observed that the proposed approach extracts smaller length rules, thereby improving the comprehensibility of the system. The generated rules act as an early warning expert system to the bank management.},
}

@Article{SHIRAZI2019238,
  title = {A big data analytics model for customer churn prediction in the retiree segment},
  journal = {International Journal of Information Management},
  volume = {48},
  pages = {238 - 253},
  year = {2019},
  issn = {0268-4012},
  doi = {https://doi.org/10.1016/j.ijinfomgt.2018.10.005},
  url = {http://www.sciencedirect.com/science/article/pii/S0268401218301518},
  author = {Farid Shirazi and Mahbobeh Mohammadi},
  keywords = {Big data, Business intelligence, Churn prediction model, Hadoop, Customer lifetime value, Classification, Regression tree},
  abstract = {Undoubtedly, the change in consumers’ choices and expectations, stemming from the emerging technology and also significant availability of different products and services, created a highly competitive landscape in various customer service sectors, including the financial industry. Accordingly, the Canadian banking industry has also become highly competitive due to the threats and disruptions caused by not only direct competitors, but also new entrants to the market. The primary objective of this paper is to construct a predictive churn model by utilizing big data, including the structured archival data, integrated with unstructured data from sources such as online web pages, the number of website visits and phone conversation logs, for the first time in the financial industry. It also examines the effect of different aspects of customers’ behavior on churning decisions. The Datameer big data analytics tool on the Hadoop platform and predictive techniques using the SAS business intelligence system were applied to study the client retirement journey path and to create a churn prediction model. By deploying the above systems, we were able to uncover a wealth of data and information associated with over 3 million customers’ records within the retiree segment of the target bank, from 2011 to 2015.},
}

@Article{ROSENTRETER2020111472,
  title = {Towards large-scale mapping of local climate zones using multitemporal Sentinel 2 data and convolutional neural networks},
  journal = {Remote Sensing of Environment},
  volume = {237},
  pages = {111472},
  year = {2020},
  issn = {0034-4257},
  doi = {https://doi.org/10.1016/j.rse.2019.111472},
  url = {http://www.sciencedirect.com/science/article/pii/S0034425719304912},
  author = {Johannes Rosentreter and Ron Hagensieker and Björn Waske},
  keywords = {Local climate zones, Convolutional neural network, Sentinel 2},
  abstract = {In recent years, the concept of Local Climate Zones (LCZs) has become a new standard in the research of urban landscapes. LCZs outline a classification scheme, which is designed to categorize urban and rural surfaces according to their climate-relevant properties, irrespective of local building materials or cultural background. We present a novel workflow for a high-resolution derivation of LCZs using multi-temporal Sentinel 2 (S2) composites and supervised Convolutional Neural Networks (CNNs). We assume that CNNs, due to their potential invariance to size and illumination of objects, are best suited to predict the highly context-based LCZs on a large scale. As a first step, the proposed workflow includes a fully automated generation of cloud-free S2 composites. These composites serve as training data basis for the LCZ classifications carried out over eight German cities. Results show that by using a CNN, overall accuracies can be increased by an average of 16.5 and 4.8 percentage points when compared to a pixel-based and a texture-based Random Forest approach, respectively. If sufficient training data is available, CNN models proved to be robust in classifying unknown cities and achieved overall accuracies of up to 86.5%. The proposed method constitutes a feasible approach for automated, large scale mapping of LCZs, and could be the preferred alternative for LCZ classifications in upcoming studies.},
}

@Article{PAOLANTI2020100276,
  title = {Multidisciplinary Pattern Recognition applications: A review},
  journal = {Computer Science Review},
  volume = {37},
  pages = {100276},
  year = {2020},
  issn = {1574-0137},
  doi = {https://doi.org/10.1016/j.cosrev.2020.100276},
  url = {http://www.sciencedirect.com/science/article/pii/S1574013719300899},
  author = {Marina Paolanti and Emanuele Frontoni},
  keywords = {Pattern recognition, Machine learning, Deep learning, Hidden Markov models},
  abstract = {Pattern recognition (PR) is the study of how machines can examine the environment, learn to distinguish patterns of interest from their background, and make reliable and feasible decisions regarding the categories of the patterns. However, even after almost 70 years of research, the design of an application based on pattern recognizer remains an ambiguous goal. Moreover, currently, there are huge volumes of data that must be dealt with, which include image, video, text and web documents; DNA; microarray gene data; etc. Among the various frameworks in which pattern recognition has been traditionally formulated, the statistical and machine learning approaches have been most comprehensively studied and employed in practice. Recently, deep learning techniques and methods have been receiving increasing attention. The main objective of this review is to summarize PR applications, departing from the major algorithms used for their design. The PR approaches are subdivided into three main methods: machine learning, statistical, and deep learning. In order to evidence the multidisciplinary aspects of PR applications, attention has been focused on latest PR methods applied to five fields of research: biomedical and biology, retail, surveillance, social media intelligence, and digital cultural heritage. In this paper, we discuss in detail the recent advances of PR approaches and propose the main applications within each field. We also present challenges and benchmarks in terms of advantages and disadvantages of the selected method in each field. A wide set of examples of applications in various domains are also provided, along with the specific method applied.},
}

@Article{MALTHOUSE2013270,
  title = {Managing Customer Relationships in the Social Media Era: Introducing the Social CRM House},
  journal = {Journal of Interactive Marketing},
  volume = {27},
  number = {4},
  pages = {270 - 280},
  year = {2013},
  note = {Social Media and Marketing},
  issn = {1094-9968},
  doi = {https://doi.org/10.1016/j.intmar.2013.09.008},
  url = {http://www.sciencedirect.com/science/article/pii/S1094996813000431},
  author = {Edward C. Malthouse and Michael Haenlein and Bernd Skiera and Egbert Wege and Michael Zhang},
  keywords = {Customer relationship management, Social media, Engagement, Information technology, Customer insight, Employees, Key performance indicator},
  abstract = {CRM has traditionally referred to a company managing relationships with customers. The rise of social media, which has connected and empowered customers, challenges this fundamental raison d'etre. This paper examines how CRM needs to adapt to the rise of social media. The convergence of social media and CRM creates pitfalls and opportunities, which are explored. We organize this discussion around the new “social CRM house,” and discuss how social media engagement affects the house's core areas (i.e., acquisition, retention, and termination) and supporting business areas (i.e., people, IT, performance evaluation, metrics and overall marketing strategy). Pitfalls discussed include the organization's lack of control over message diffusion, big and unstructured data sets, privacy, data security, the shortage of qualified manpower, measuring the ROI of social media marketing initiatives, strategies for managing employees, integrating customer touch points, and content marketing.},
}

@Article{CHEN2020687,
  title = {Merging anomalous data usage in wireless mobile telecommunications: Business analytics with a strategy-focused data-driven approach for sustainability},
  journal = {European Journal of Operational Research},
  volume = {281},
  number = {3},
  pages = {687 - 705},
  year = {2020},
  note = {Featured Cluster: Business Analytics: Defining the field and identifying a research agenda},
  issn = {0377-2217},
  doi = {https://doi.org/10.1016/j.ejor.2019.02.046},
  url = {http://www.sciencedirect.com/science/article/pii/S0377221719301948},
  author = {Yi-Ting Chen and Edward W. Sun and Yi-Bing Lin},
  keywords = {Analytics, Artificial intelligence, Data mining, Decision support systems, OR in telecommunications, Validation of OR Computations},
  abstract = {Mobile internet usage has exploded with the mass popularity of smartphones that offer more convenient and efficient ways of doing anything from watching movies, playing games, and streaming music. Understanding the patterns of data usage is thus essential for strategy-focused data-driven business analytics. However, data usage has several unique stylized facts (such as high dimensionality, heteroscedasticity, and sparsity) due to a great variety of user behaviour. To manage these facts, we propose a novel density-based subspace clustering approach (i.e., a three-stage iterative optimization procedure) for intelligent segmentation of consumer data usage/demand. We discuss the characteristics of the proposed method and illustrate its performance in both simulation with synthetic data and business analytics with real data. In a field experiment of wireless mobile telecommunications for data-driven strategic design and managerial implementation, we show that our method is adequate for business analytics and plausible for sustainability in search of business value.},
}

@Article{TAMADDONIJAHROMI20141258,
  title = {Managing B2B customer churn, retention and profitability},
  journal = {Industrial Marketing Management},
  volume = {43},
  number = {7},
  pages = {1258 - 1268},
  year = {2014},
  issn = {0019-8501},
  doi = {https://doi.org/10.1016/j.indmarman.2014.06.016},
  url = {http://www.sciencedirect.com/science/article/pii/S001985011400114X},
  author = {Ali {Tamaddoni Jahromi} and Stanislav Stakhovych and Michael Ewing},
  keywords = {B2B customer churn, Data mining, Non-contractual setting, Retention campaign, Profit},
  abstract = {It is now widely accepted that firms should direct more effort into retaining existing customers than to attracting new ones. To achieve this, customers likely to defect need to be identified so that they can be approached with tailored incentives or other bespoke retention offers. Such strategies call for predictive models capable of identifying customers with higher probabilities of defecting in the relatively near future. A review of the extant literature on customer churn models reveals that although several predictive models have been developed to model churn in B2C contexts, the B2B context in general, and non-contractual settings in particular, have received less attention in this regard. Therefore, to address these gaps, this study proposes a data-mining approach to model non-contractual customer churn in B2B contexts. Several modeling techniques are compared in terms of their ability to predict true churners. The best performing data-mining technique (boosting) is then applied to develop a profit maximizing retention campaign. Results confirm that the model driven approach to churn prediction and developing retention strategies outperforms commonly used managerial heuristics.},
}

@Article{VELASCO2020106054,
  title = {Deep Learning loss model for large-scale low voltage smart grids},
  journal = {International Journal of Electrical Power & Energy Systems},
  volume = {121},
  pages = {106054},
  year = {2020},
  issn = {0142-0615},
  doi = {https://doi.org/10.1016/j.ijepes.2020.106054},
  url = {http://www.sciencedirect.com/science/article/pii/S0142061519340542},
  author = {Jose Angel Velasco and Hortensia Amaris and Monica Alonso},
  keywords = {Deep learning, Smart grids, Uncertainty, Probabilistic modelling, Clustering, Phase imbalance},
  abstract = {Distribution systems operators (DSOs) encounter the challenge of managing network losses in large geographical areas with hundreds of secondary substations and thousands of customers and with an ever-increasing presence of renewable energy sources. This situation complicates the estimation process of power loss, which is paramount to improve the network energy efficiency level in the context of the European Union energy policies. Thus, this article presents a methodology to estimate power losses in large-scale low voltage (LV) smart grids. The methodology is based on a deep-learning loss model to infer the network technical losses considering a large rollout of smart meters, a high penetration of distributed generation (DG) and unbalanced operation, among other network characteristics. The methodology has been validated in a large-scale LV distribution area in Madrid (Spain). The proposed methodology has proven to be a potential network loss estimation tool to improve the energy efficiency level in large-scale smart grids with a high penetration of distributed resources. The accuracy of the proposed methodology outperforms that of the state-of-the-art loss estimation methods, exhibiting a rapid convergence which allows for its use in real-time operations.},
}

@Article{ABBASIMEHR2020106435,
  title = {An optimized model using LSTM network for demand forecasting},
  journal = {Computers & Industrial Engineering},
  volume = {143},
  pages = {106435},
  year = {2020},
  issn = {0360-8352},
  doi = {https://doi.org/10.1016/j.cie.2020.106435},
  url = {http://www.sciencedirect.com/science/article/pii/S0360835220301698},
  author = {Hossein Abbasimehr and Mostafa Shabani and Mohsen Yousefi},
  keywords = {Demand prediction, Time series forecasting, Statistical methods, LSTM, RNN},
  abstract = {In a business environment with strict competition among firms, accurate demand forecasting is not straightforward. In this paper, a forecasting method is proposed, which has a strong capability of predicting highly fluctuating demand data. Therefore, in this paper we propose a demand forecasting method based on multi-layer LSTM networks. The proposed method automatically selects the best forecasting model by considering different combinations of LSTM hyperparameters for a given time series using the grid search method. It has the ability to capture nonlinear patterns in time series data, while considering the inherent characteristics of non-stationary time series data. The proposed method is compared with some well-known time series forecasting techniques from both statistical and computational intelligence methods using demand data of a furniture company. These methods include autoregressive integrated moving average (ARIMA), exponential smoothing (ETS), artificial neural network (ANN), K-nearest neighbors (KNN), recurrent neural network (RNN), support vector machines (SVM) and single layer LSTM. The experimental results indicate that the proposed method is superior among the tested methods in terms of performance measures.},
}

@Article{BOSE20091,
  title = {Quantitative models for direct marketing: A review from systems perspective},
  journal = {European Journal of Operational Research},
  volume = {195},
  number = {1},
  pages = {1 - 16},
  year = {2009},
  issn = {0377-2217},
  doi = {https://doi.org/10.1016/j.ejor.2008.04.006},
  url = {http://www.sciencedirect.com/science/article/pii/S0377221708003512},
  author = {Indranil Bose and Xi Chen},
  keywords = {Marketing, Data mining, Customer profiling, Customer targeting, Statistical modeling, Performance evaluation},
  abstract = {In this paper, quantitative models for direct marketing models are reviewed from a systems perspective. A systems view consists of input, processing, and output and the six key activities of direct marketing that take place within these constituent parts. A discussion about inputs for direct marketing models is provided by describing the various types of data used, by determining the significance of the data, and by addressing the issue of selection of appropriate data. Two types of models, statistical and machine learning based, are popularly used for conducting direct marketing activities. The advantages and disadvantages of these two approaches are discussed along with enhancements to these models. The evaluation of output for direct marketing models is done on the basis of accuracy and profitability. Some challenges in conducting research in the area of quantitative direct marketing models are listed and some significant research questions are proposed.},
}

@Article{MORTENSON2015583,
  title = {Operational research from Taylorism to Terabytes: A research agenda for the analytics age},
  journal = {European Journal of Operational Research},
  volume = {241},
  number = {3},
  pages = {583 - 595},
  year = {2015},
  issn = {0377-2217},
  doi = {https://doi.org/10.1016/j.ejor.2014.08.029},
  url = {http://www.sciencedirect.com/science/article/pii/S037722171400664X},
  author = {Michael J. Mortenson and Neil F. Doherty and Stewart Robinson},
  keywords = {Analytics, Big data, Data visualisation, History of OR, History of computing},
  abstract = {The growing attention and prominence afforded to analytics presents a genuine challenge for the operational research community. Many in the community have recognised this growth and sought to align themselves with analytics. For instance, the US operational research society INFORMS now offers analytics related conferences, certification and a magazine. However, as shown in this research, the volume of analytics-orientated studies in journals associated with operational research is comparatively low. This paper seeks to address this paradox by seeking to better understand what analytics is, and how operational research is related to it. To do so literature from a range of academic disciplines is analysed, in what is conceived as concurrent histories in the shared tradition of a management paradigm spread over the last 100 years. The findings of this analysis reveal new insights as to how operational research exists within an ecosystem shared with several other disciplines, and how interactions and ripple effects diffuse knowledge and ideas between each. Whilst this ecosystem is developed and evolved through interdisciplinary collaborations, individual disciplines are cast into competition for the attention of the same business users. These findings are further explored by discussing the implication this has for operational research, as well as considering what directions future research may take to maximise the potential value of these relationships.},
}

@Article{BRANDNER2013581,
  title = {A memetic approach to construct transductive discrete support vector machines},
  journal = {European Journal of Operational Research},
  volume = {230},
  number = {3},
  pages = {581 - 595},
  year = {2013},
  issn = {0377-2217},
  doi = {https://doi.org/10.1016/j.ejor.2013.05.010},
  url = {http://www.sciencedirect.com/science/article/pii/S0377221713004098},
  author = {Hubertus Brandner and Stefan Lessmann and Stefan Voß},
  keywords = {Data mining, Transductive learning, Support vector machines, Memetic algorithms, Combinatorial optimization},
  abstract = {Transductive learning involves the construction and application of prediction models to classify a fixed set of decision objects into discrete groups. It is a special case of classification analysis with important applications in web-mining, corporate planning and other areas. This paper proposes a novel transductive classifier that is based on the philosophy of discrete support vector machines. We formalize the task to estimate the class labels of decision objects as a mixed integer program. A memetic algorithm is developed to solve the mathematical program and to construct a transductive support vector machine classifier, respectively. Empirical experiments on synthetic and real-world data evidence the effectiveness of the new approach and demonstrate that it identifies high quality solutions in short time. Furthermore, the results suggest that the class predictions following from the memetic algorithm are significantly more accurate than the predictions of a CPLEX-based reference classifier. Comparisons to other transductive and inductive classifiers provide further support for our approach and suggest that it performs competitive with respect to several benchmarks.},
}

@Article{VASHISHTHA2019112834,
  title = {Fuzzy rule based unsupervised sentiment analysis from social media posts},
  journal = {Expert Systems with Applications},
  volume = {138},
  pages = {112834},
  year = {2019},
  issn = {0957-4174},
  doi = {https://doi.org/10.1016/j.eswa.2019.112834},
  url = {http://www.sciencedirect.com/science/article/pii/S0957417419305366},
  author = {Srishti Vashishtha and Seba Susan},
  keywords = {Social media, Twitter, Sentiment analysis, Fuzzy rule, Lexicon},
  abstract = {In this paper, we compute the sentiment of social media posts using a novel set of fuzzy rules involving multiple lexicons and datasets. The proposed fuzzy system integrates Natural Language Processing techniques and Word Sense Disambiguation using a novel unsupervised nine fuzzy rule based system to classify the post into: positive, negative or neutral sentiment class. We perform a comparative analysis of our method on nine public twitter datasets, three sentiment lexicons, four state-of-the-art approaches for unsupervised Sentiment Analysis and one state-of-the-art method for supervised machine learning. Traditionally, Sentiment Analysis of twitter data is performed using a single lexicon. Our results can give an insight to researchers to choose which lexicon is best for social media. The fusion of fuzzy logic with lexicons for sentiment classification provides a new paradigm in Sentiment Analysis. Our method can be adapted to any lexicon and any dataset (two-class or three-class sentiment). The experiments on benchmark datasets yield higher performance for our approach as compared to the state-of-the-art.},
}

@Article{LABRADORRIVAS2020106602,
  title = {Faults in smart grid systems: Monitoring, detection and classification},
  journal = {Electric Power Systems Research},
  volume = {189},
  pages = {106602},
  year = {2020},
  issn = {0378-7796},
  doi = {https://doi.org/10.1016/j.epsr.2020.106602},
  url = {http://www.sciencedirect.com/science/article/pii/S0378779620304065},
  author = {Angel Esteban {Labrador Rivas} and Taufik Abrão},
  keywords = {Smart grid, Electric power system, Fault monitoring, Fault detection, Fault classification, 5G communication systems},
  abstract = {Smart Grid (SG) is a multidisciplinary concept related to the power system update and improvement. SG implies real-time information with specific communication requirements. System reliability relies on the best capabilities for monitoring and controlling the grid. Among other aspects, SG applications involve three main challenges, sufficient real-time capable measurement units, managing large data sets, and two-way low-latency communications. Considering fault detection and classification a key factor to SG reliability, this work provides a systematic review of SG faults from the most significant research databases and state-of-the-art research papers aiming at creating a comprehensive classification framework on the relevant requirements. This paper includes in detail the classification of different fault scenarios in a comprehensive framework that involves system-level of application, e.g., transmission, distribution, commercial, DG, and EV. To this end, We analyze and indicate relevant topics for future developments related to the monitoring and fault detection and classification in SG systems.},
}

@Article{MAHMOUD2017292,
  title = {Adaptive intelligent techniques for microgrid control systems: A survey},
  journal = {International Journal of Electrical Power & Energy Systems},
  volume = {90},
  pages = {292 - 305},
  year = {2017},
  issn = {0142-0615},
  doi = {https://doi.org/10.1016/j.ijepes.2017.02.008},
  url = {http://www.sciencedirect.com/science/article/pii/S0142061516325042},
  author = {Magdi S. Mahmoud and Nezar M. Alyazidi and Mohamed I. Abouheaf},
  keywords = {Adaptive control, Intelligent technique, Particle swarm optimization, Fuzzy logic, Distributed generations, Islanded micro-grid},
  abstract = {This paper introduces a survey on the adaptive and intelligent methods that have been applied to microgrids systems. Interestingly, the adaptive technique is effectively exercised in various control issues including stability, tracking error, and parameter uncertainties. Adaptive control has been extremely developed by using intelligent algorithms to automatically tune the control parameters namely fuzzy logic, particle swarm optimization, bacterial search algorithm, and etc. The objective is to evaluate and classify the design control methods and evaluation algorithms for the microgrid systems to maintain stability, reliability, and load variations by adjusting the controller parameters especially in standalone operation mode. The stability of islanded microgrids are constantly impacted by the related loads. A significant part of the research on an islanded microgrid involves droop control technique. In normal operation, distributed generation units and storage units provide power quality control. Once a shutdown is occurred, microgrid can be isolated from the main grid and operate in a local grid to support the local loads. Thus, distributed generations co-operate storage units to sustain the stability of the islanded microgrid.},
}

@Article{KHALID2020102275,
  title = {A survey on hyperparameters optimization algorithms of forecasting models in smart grid},
  journal = {Sustainable Cities and Society},
  volume = {61},
  pages = {102275},
  year = {2020},
  issn = {2210-6707},
  doi = {https://doi.org/10.1016/j.scs.2020.102275},
  url = {http://www.sciencedirect.com/science/article/pii/S2210670720304960},
  author = {Rabiya Khalid and Nadeem Javaid},
  keywords = {Forecasting, Hyperparameters, Parameter tuning, Data preprocessing, Training algorithms, Outliers in data, Processing time},
  abstract = {Forecasting in the smart grid (SG) plays a vital role in maintaining the balance between demand and supply of electricity, efficient energy management, better planning of energy generation units and renewable energy sources and their dispatching and scheduling. Existing forecasting models are being used and new models are developed for a wide range of SG applications. These algorithms have hyperparameters which need to be optimized carefully before forecasting. The optimized values of these algorithms increase the forecasting accuracy up to a significant level. In this paper, we present a brief literature review of forecasting models and the optimization methods used to tune their hyperparameters. In addition, we have also discussed the data preprocessing methods. A comparative analysis of these forecasting models, according to their hyperparameter optimization, error methods and preprocessing methods, is also presented. Besides, we have critically analyzed the existing optimization and data preprocessing models and highlighted the important findings. A survey of existing survey papers is also presented and their recency score is computed based on the number of recent papers reviewed in them. By recent, we mean that the year in which a survey paper is published and its previous three years. Finally, future research directions are discussed in detail.},
}

@Article{XIA200871,
  title = {Model of Customer Churn Prediction on Support Vector Machine},
  journal = {Systems Engineering - Theory & Practice},
  volume = {28},
  number = {1},
  pages = {71 - 77},
  year = {2008},
  issn = {1874-8651},
  doi = {https://doi.org/10.1016/S1874-8651(09)60003-X},
  url = {http://www.sciencedirect.com/science/article/pii/S187486510960003X},
  author = {Guo-en XIA and Wei-dong JIN},
  keywords = {customer churn, support vector machine (SVM), telecommunication industry},
  abstract = {To improve the prediction abilities of machine learning methods, a support vector machine (SVM) on structural risk minimization was applied to customer churn prediction. Researching customer churn prediction cases both in home and foreign carries, the method was compared with artifical neural network, decision tree, logistic regression, and naive bayesian classifier. It is found that the method enjoys the best accuracy rate, hit rate, covering rate, and lift coefficient, and therefore, provides an effective measurement for customer churn prediction.},
}

@Article{SEAL2020106016,
  title = {Fuzzy c-means clustering using Jeffreys-divergence based similarity measure},
  journal = {Applied Soft Computing},
  volume = {88},
  pages = {106016},
  year = {2020},
  issn = {1568-4946},
  doi = {https://doi.org/10.1016/j.asoc.2019.106016},
  url = {http://www.sciencedirect.com/science/article/pii/S1568494619307987},
  author = {Ayan Seal and Aditya Karlekar and Ondrej Krejcar and Consuelo Gonzalo-Martin},
  keywords = {Jeffreys-divergence, Jeffreys-divergence based similarity measure, Fuzzy c-means, Jeffreys-fuzzy-c-means clustering},
  abstract = {In clustering, similarity measure has been one of the major factors for discovering the natural grouping of a given dataset by identifying hidden patterns. To determine a suitable similarity measure is an open problem in clustering analysis for several years. The purpose of this study is to make known a divergence based similarity measure. The notion of the proposed similarity measure is derived from Jeffrey-divergence. Various features of the proposed similarity measure are explained. Afterwards we develop fuzzy c-means (FCM) by making use of the proposed similarity measure, which guarantees to converge to local minima. The various characteristics of the modified FCM algorithm are also addressed. Some well known real-world and synthetic datasets are considered for the experiments. In addition to that two remote sensing image datasets are also adopted in this work to illustrate the effectiveness of the proposed FCM over some existing methods. All the obtained results demonstrate that FCM with divergence based proposed similarity measure outperforms three latest FCM algorithms.},
}

@Article{BOSE2015227,
  title = {Detecting the migration of mobile service customers using fuzzy clustering},
  journal = {Information & Management},
  volume = {52},
  number = {2},
  pages = {227 - 238},
  year = {2015},
  issn = {0378-7206},
  doi = {https://doi.org/10.1016/j.im.2014.11.001},
  url = {http://www.sciencedirect.com/science/article/pii/S0378720614001359},
  author = {Indranil Bose and Xi Chen},
  keywords = {Clustering, Customer behavior, Migration, Mobile services, Temporal data, Usage patterns},
  abstract = {Customer clustering is used to build customer profiles which make up the core of a customer centric information system. In this paper, we develop a method for extending the standard fuzzy c-means clustering algorithm using membership functions to detect how customers move between clusters over time. The study leads to the discovery of new usage and revenue patterns for customers, the identification of two groups of customers that exhibit migratory behavior over time, and the determination of specific usage and revenue attributes that impact customer migration. The findings provide insights to mobile services providers about how to detect temporal changes in customer behavior.},
}

@Article{HUBERTY2015992,
  title = {Can we vote with our tweet? On the perennial difficulty of election forecasting with social media},
  journal = {International Journal of Forecasting},
  volume = {31},
  number = {3},
  pages = {992 - 1007},
  year = {2015},
  issn = {0169-2070},
  doi = {https://doi.org/10.1016/j.ijforecast.2014.08.005},
  url = {http://www.sciencedirect.com/science/article/pii/S0169207014001058},
  author = {Mark Huberty},
  keywords = {Evaluating forecasts, Surveys, Model selection, Elections, Social media},
  abstract = {Social media and other “big” data promise new sources of information for tracking and forecasting electoral contests in democratic societies. This paper discusses the use of social media, and Twitter in particular, for forecasting elections in the United States, Germany, and other democracies. All known forecasting methods based on social media have failed when subjected to the demands of true forward-looking electoral forecasting. These failures appear to be due to fundamental properties of social media, rather than to methodological or algorithmic difficulties. In short, social media do not, and probably never will, offer a stable, unbiased, representative picture of the electorate; and convenience samples of social media lack sufficient data to fix these problems post hoc. Hence, while these services may, as others in this volume discuss, offer new ways of reaching prospective voters, the data that they generate will not replace polling as a means of assessing the sentiment or intentions of the electorate.},
}

@Article{KIANMEHR20096218,
  title = {Calling communities analysis and identification using machine learning techniques},
  journal = {Expert Systems with Applications},
  volume = {36},
  number = {3, Part 2},
  pages = {6218 - 6226},
  year = {2009},
  issn = {0957-4174},
  doi = {https://doi.org/10.1016/j.eswa.2008.07.072},
  url = {http://www.sciencedirect.com/science/article/pii/S0957417408004685},
  author = {Keivan Kianmehr and Reda Alhajj},
  keywords = {Social communities, Classification, Clustering, Customer behavior, Machine learning},
  abstract = {The analysis of social communities related logs has recently received considerable attention for its importance in shedding light on social concerns by identifying different groups, and hence helps in resolving issues like predicting terrorist groups. In the customer analysis domain, identifying calling communities can be used for determining a particular customer’s value according to the general pattern behavior of the community that the customer belongs to; this helps the effective targeted marketing design, which is significantly important for increasing profitability. In telecommunication industry, machine learning techniques have been applied to the Call Detail Record (CDR) for predicting customer behavior such as churn prediction. In this paper, we pursue identifying the calling communities and demonstrate how cluster analysis can be used to effectively identify communities using information derived from the CDR data. We use the information extracted from the cluster analysis to identify customer calling patterns. Customers calling patterns are then given to a classification algorithm to generate a classifier model for predicting the calling communities of a customer. We apply different machine learning techniques to build classifier models and compare them in terms of classification accuracy and computational performance. The reported test results demonstrate the applicability and effectiveness of the proposed approach.},
}

@Article{BARFAR2017115,
  title = {Applying behavioral economics in predictive analytics for B2B churn: Findings from service quality data},
  journal = {Decision Support Systems},
  volume = {101},
  pages = {115 - 127},
  year = {2017},
  issn = {0167-9236},
  doi = {https://doi.org/10.1016/j.dss.2017.06.006},
  url = {http://www.sciencedirect.com/science/article/pii/S0167923617301185},
  author = {Arash Barfar and Balaji Padmanabhan and Alan Hevner},
  keywords = {Organizational decision analytics, B2B service operations, Churn, Service quality, Decision-making, Rationality, Bounded rationality, Heuristics, Adaptive toolbox, Somatic states},
  abstract = {Motivated by the long-standing debate on rationality in behavioral economics and the potential of theory-driven predictive analytics, this paper examines the link between service quality and B2B churn. Using longitudinal B2B transactional data with service quality indicators provided by a large company, we present evidence that both rationality and bounded-rationality assumptions play significant roles in predicting organizational decisions on churn. Specifically, variables that relate to the assumed rationality of organizations appear to provide accurate predictions while, at the same time, variables that capture boundedly rational decision rules appear to play a role through “somatic states” that make organizations more sensitive to the rational variables. In addition to presenting a novel approach for predicting organizational decisions on churn, this paper offers theoretical and managerial insights as well as opportunities for future research at the intersection of behavioral economics and predictive analytics for decision-making.},
}

@InCollection{PONDERSUTTON20161,
  title = {Chapter 1 - The Automating of Open Source Intelligence},
  editor = {Robert Layton and Paul A. Watters},
  booktitle = {Automating Open Source Intelligence},
  publisher = {Syngress},
  address = {Boston},
  pages = {1 - 20},
  year = {2016},
  isbn = {978-0-12-802916-9},
  doi = {https://doi.org/10.1016/B978-0-12-802916-9.00001-4},
  url = {http://www.sciencedirect.com/science/article/pii/B9780128029169000014},
  author = {Agate M. Ponder-Sutton},
  keywords = {privacy, ethics, automation, surveillance, machine learning, statistics},
  abstract = {Open source intelligence (OSINT) is intelligence that is synthesized using publicly available data. We will discuss the current state of OSINT and data science. The changes in the analysts and users will be explored. We will cover data analysis, automated data gathering, APIs, and tools; algorithms including supervised and unsupervised learning, geolocational methods, de-anonymization. How do all these things interact within OSINT including ethics and context? Now that open intelligence has become more open and playing fields are leveling, the need to ensure and encourage positive use is even stronger.},
}

@InCollection{YAN2018249,
  title = {Chapter 11 - InsurTech and FinTech: Banking and Insurance Enablement},
  editor = {David {Lee Kuo Chuen} and Robert Deng},
  booktitle = {Handbook of Blockchain, Digital Finance, and Inclusion, Volume 1},
  publisher = {Academic Press},
  pages = {249 - 281},
  year = {2018},
  isbn = {978-0-12-810441-5},
  doi = {https://doi.org/10.1016/B978-0-12-810441-5.00011-7},
  url = {http://www.sciencedirect.com/science/article/pii/B9780128104415000117},
  author = {Tan Choon Yan and Paul Schulte and David {LEE Kuo Chuen}},
  keywords = {Insurance technology, Digital revolution, Digital finance, Insurance sector},
  abstract = {This chapter surveys the landscape of insurance technology and its potential from the perspective of enablement for financial and insurance services. Digital revolution is occurring in a sector that has hardly changed over the last 300 years. Surprising to many, recent innovation in China's digital finance space has shown that emerging entities can and will disrupt the multitrillion-dollar industry. With big data and Blockchain, the impact on insurance sector is going to be a lot faster and more significant than most people will anticipate.},
}

@Article{CHEN2012111,
  title = {Distributed customer behavior prediction using multiplex data: A collaborative MK-SVM approach},
  journal = {Knowledge-Based Systems},
  volume = {35},
  pages = {111 - 119},
  year = {2012},
  issn = {0950-7051},
  doi = {https://doi.org/10.1016/j.knosys.2012.04.023},
  url = {http://www.sciencedirect.com/science/article/pii/S0950705112001190},
  author = {Zhen-Yu Chen and Zhi-Ping Fan},
  keywords = {Customer relationship management, Behavior analysis, Customer churn prediction, Customer purchase prediction, Support vector machine, Multiple kernel learning},
  abstract = {In the customer-centered marketplace, the understanding of customer behavior is a critical success factor. The big databases in an organization usually involve multiplex data such as static, time series, symbolic sequential and textual data which are separately stored in different databases of different sections. It poses a challenge to traditional centralized customer behavior prediction. In this study, a novel approach called collaborative multiple kernel support vector machine (C-MK-SVM) is developed for distributed customer behavior prediction using multiplex data. The alternating direction method of multipliers (ADMM) is used for the global optimization of the distributed sub-models in C-MK-SVM. Computational experiments on a practical retail dataset are reported. Computational results show that C-MK-SVM exhibits better customer behavior prediction performance and higher computational speed than support vector machine and multiple kernel support vector machine.},
}

@Article{RISSELADA2010198,
  title = {Staying Power of Churn Prediction Models},
  journal = {Journal of Interactive Marketing},
  volume = {24},
  number = {3},
  pages = {198 - 208},
  year = {2010},
  issn = {1094-9968},
  doi = {https://doi.org/10.1016/j.intmar.2010.04.002},
  url = {http://www.sciencedirect.com/science/article/pii/S1094996810000253},
  author = {Hans Risselada and Peter C. Verhoef and Tammo H.A. Bijmolt},
  keywords = {Churn prediction, Scoring models, Customer relationship management},
  abstract = {In this paper, we study the staying power of various churn prediction models. Staying power is defined as the predictive performance of a model in a number of periods after the estimation period. We examine two methods, logit models and classification trees, both with and without applying a bagging procedure. Bagging consists of averaging the results of multiple models that have each been estimated on a bootstrap sample from the original sample. We test the models using customer data of two firms from different industries, namely the internet service provider and insurance markets. The results show that the classification tree in combination with a bagging procedure outperforms the other three methods. It is shown that the ability to identify high risk customers of this model is similar for the in-period and one-period-ahead forecasts. However, for all methods the staying power is rather low, as the predictive performance deteriorates considerably within a few periods after the estimation period. This is due to the fact that both the parameter estimates change over time and the fact that the variables that are significant differ between periods. Our findings indicate that churn models should be adapted regularly. We provide a framework for database analysts to reconsider their methods used for churn modeling and to assess for how long they can use an estimated model.},
}

@Article{PHILLIPS2019100295,
  title = {Configuring the new digital landscape in western Canadian agriculture},
  journal = {NJAS - Wageningen Journal of Life Sciences},
  volume = {90-91},
  pages = {100295},
  year = {2019},
  issn = {1573-5214},
  doi = {https://doi.org/10.1016/j.njas.2019.04.001},
  url = {http://www.sciencedirect.com/science/article/pii/S1573521418302264},
  author = {Peter W.B. Phillips and Jo-Anne Relf-Eckstein and Graeme Jobe and Brian Wixted},
  keywords = {Economic landscape, Agricultural systems, Digital agriculture, Firm strategy, Interoperability, Data},
  abstract = {Digital technologies are working to transform the global agricultural system. Farmers and firms are creating, adapting and adopting a range of new hardware, software, mobile apps, sensor technologies and big data applications, which is working to disrupt established structures within the farm machinery and associated data sectors. Focusing just on the extension of precision technologies to agriculture, this paper maps the competitive landscape using a 2 × 2 typology that situates entities operating in Canada based on their strategies, distinguishing between top-down and bottom-up networks of competitors and collaborators and the degree of interoperability of their digital applications. We examine the emergence of four specific cases in western Canadian agriculture. The typology and the cases suggest global agri-food firms, industry collectives and a host of entrepreneurial start-ups and small and medium-sized enterprises are competing to both organize and disrupt the global agri-food value chain. It is not yet clear which strategy, if any, will prevail and provide the model for broad acre agriculture in Canada and around the world.},
}

@InCollection{NISBET2009335,
  title = {Chapter 16 - Customer Response Modeling},
  editor = {Robert Nisbet and John Elder and Gary Miner},
  booktitle = {Handbook of Statistical Analysis and Data Mining Applications},
  publisher = {Academic Press},
  address = {Boston},
  pages = {335 - 346},
  year = {2009},
  isbn = {978-0-12-374765-5},
  doi = {https://doi.org/10.1016/B978-0-12-374765-5.00016-4},
  url = {http://www.sciencedirect.com/science/article/pii/B9780123747655000164},
  author = {Robert Nisbet and John Elder and Gary Miner},
  abstract = {Customer Relationship Management (CRM) systems are built to manage call centers, how a business relates to its customers, and inform marketing and sales efforts. Companies try to build Customer Relationship Management programs that aim to create the same kinds of relationships with their customers. The key principle in this approach is that the most powerful predictors of customer behavior in the future are customer behavior patterns in the past. Other customer characteristics are important also in defining patterns of customer behavior, i.e., demographic and firmographic information. The key to successful customer response modeling is to associate with each customer a historical time-series of fields that in some way reflect motives and attitudes that cause the customer decision. Modeling customer behavior with temporal abstractions involves rearranging all the modeling variables to more clearly reflect patterns of change in the customer response variable. Customers are biological entities that respond in a biological manner. It seems reasonable to expect that the only way to create highly predictive models of customer behavior is to express some degree of this complex interaction in the design of the modeling methodology. Modeling customer behavior with temporal abstractions involves rearranging all the modeling variables to more clearly reflect patterns of change in the customer response variable.},
}

@Article{COUSSEMENT201727,
  title = {A comparative analysis of data preparation algorithms for customer churn prediction: A case study in the telecommunication industry},
  journal = {Decision Support Systems},
  volume = {95},
  pages = {27 - 36},
  year = {2017},
  issn = {0167-9236},
  doi = {https://doi.org/10.1016/j.dss.2016.11.007},
  url = {http://www.sciencedirect.com/science/article/pii/S0167923616302020},
  author = {Kristof Coussement and Stefan Lessmann and Geert Verstraeten},
  keywords = {Predictive analytics, Data preparation techniques, Churn prediction},
  abstract = {Data preparation is a process that aims to convert independent (categorical and continuous) variables into a form appropriate for further analysis. We examine data-preparation alternatives to enhance the prediction performance for the commonly-used logit model. This study, conducted in a churn prediction modeling context, benchmarks an optimized logit model against eight state-of-the-art data mining techniques that use standard input data, including real-world cross-sectional data from a large European telecommunication provider. The results lead to following conclusions. (i) Analysts better acknowledge that the data-preparation technique they choose actually affects churn prediction performance; we find improvements of up to 14.5% in the area under the receiving operating characteristics curve and 34% in the top decile lift. (ii) The enhanced logistic regression also is competitive with more advanced single and ensemble data mining algorithms. This article concludes with some managerial implications and suggestions for further research, including evidence of the generalizability of the results for other business settings.},
}

@Article{NAKABI2019100212,
  title = {An ANN-based model for learning individual customer behavior in response to electricity prices},
  journal = {Sustainable Energy, Grids and Networks},
  volume = {18},
  pages = {100212},
  year = {2019},
  issn = {2352-4677},
  doi = {https://doi.org/10.1016/j.segan.2019.100212},
  url = {http://www.sciencedirect.com/science/article/pii/S2352467718304570},
  author = {Taha Abdelhalim Nakabi and Pekka Toivanen},
  keywords = {, Artificial neural networks, Customer behavior learning, Demand response programs, LSTM, Price elasticity of demand, Smart grid},
  abstract = {In this paper, we consider the problem of learning the electricity consumption patterns of an individual residential electricity customer, in response to electricity price signals in a demand response program. Two new methods are presented for predicting the hourly loads using the outdoor temperatures, electricity prices and previous loads. The proposed models are based respectively on a fully connected neural network and a Long–Short-term memory network. Both models deal with the uncertainty of household devices and its indoor temperature. Numerical results show the high performance of the proposed methods in terms of accuracy of the predictions. Both models can learn the consumption patterns and are able to give a good approximation of the load profile given a set of prices and temperatures. The proposed architecture can be used to investigate the price elasticity of demand, which can be used in several applications such as optimal pricing, demand flexibility or carbon emission reduction.},
}

@Article{AHMAD2020112851,
  title = {Borrow from rich cousin: transfer learning for emotion detection using cross lingual embedding},
  journal = {Expert Systems with Applications},
  volume = {139},
  pages = {112851},
  year = {2020},
  issn = {0957-4174},
  doi = {https://doi.org/10.1016/j.eswa.2019.112851},
  url = {http://www.sciencedirect.com/science/article/pii/S0957417419305536},
  author = {Zishan Ahmad and Raghav Jindal and Asif Ekbal and Pushpak Bhattachharyya},
  keywords = {Deep learning, Cross lingual transfer learning, Emotion classification},
  abstract = {Performance of any natural language processing (NLP) system greatly depends on the amount of resources and tools available in a particular language or domain. Therefore, while solving any problem in low-resource setting, it is important to investigate techniques to leverage the resources and tools available in resource-rich languages. In this paper we propose an efficient technique to mitigate the problem of resource scarcity for emotion detection in Hindi by leveraging information from a resource-rich language like English. Our method follows a deep transfer learning framework which efficiently captures relevant information through the shared space of two languages, showing significantly better performance compared to the monolingual scenario that learns in the vector space of only one language. As base learning models, we use Convolution Neural Network (CNN) and Bi-Directional Long Short Term Memory (Bi-LSTM). As there are no available emotion labeled dataset for Hindi, we create a new dataset for emotion detection in disaster domain by annotating sentences of news documents with nine different classes based on Plutchikâ;;s wheel of emotions. To improve the performance of emotion classification in Hindi, we employ transfer learning to exploit the resources available in the related domains. The core of our approach lies in generating a cross-lingual word embedding representation of words in the shared embedding space. The neural networks are trained on the existing datasets, and then weights are fine-tuned following the four different transfer learning strategies for emotion classification in Hindi. We obtain a significant performance gain in our our proposed transfer learning techniques, achieving an F1-score of 0.53 (compared to 0.47)-thereby implying that knowledge from a resource-rich language can be transferred across language and domains.11codes and data available at https://github.com/zishanahmad1821cs18/crosslingual_transfer},
}

@Article{THOMASSEY2010470,
  title = {Sales forecasts in clothing industry: The key success factor of the supply chain management},
  journal = {International Journal of Production Economics},
  volume = {128},
  number = {2},
  pages = {470 - 483},
  year = {2010},
  note = {Supply Chain Forecasting Systems},
  issn = {0925-5273},
  doi = {https://doi.org/10.1016/j.ijpe.2010.07.018},
  url = {http://www.sciencedirect.com/science/article/pii/S0925527310002598},
  author = {Sébastien Thomassey},
  keywords = {Sales forecasts, Textile–apparel supply chain, Clothing industry, Sourcing simulation},
  abstract = {Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems. However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust. After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.},
}

@Article{CHEN2015422,
  title = {Behavior-aware user response modeling in social media: Learning from diverse heterogeneous data},
  journal = {European Journal of Operational Research},
  volume = {241},
  number = {2},
  pages = {422 - 434},
  year = {2015},
  issn = {0377-2217},
  doi = {https://doi.org/10.1016/j.ejor.2014.09.008},
  url = {http://www.sciencedirect.com/science/article/pii/S0377221714007255},
  author = {Zhen-Yu Chen and Zhi-Ping Fan and Minghe Sun},
  keywords = {Data mining, Direct marketing, Response modeling, Social media, Engagement behavior},
  abstract = {With the rapid development of Web 2.0 applications, social media have increasingly become a major factor influencing the purchase decisions of customers. Longitudinal individual and engagement behavioral data generated on social media sites post challenges to integrate diverse heterogeneous data to improve prediction performance in customer response modeling. In this study, a hierarchical ensemble learning framework is proposed for behavior-aware user response modeling using diverse heterogeneous data. In the framework, a general-purpose data transformation and feature extraction strategy is developed to transform the heterogeneous high-dimensional multi-relational datasets into customer-centered high-order tensors and to extract attributes. An improved hierarchical multiple kernel support vector machine (H-MK-SVM) is developed to integrate the external, tag and keyword, individual behavioral and engagement behavioral data for feature selection from multiple correlated attributes and for ensemble learning in user response modeling. The subagging strategy is adopted to deal with large-scale imbalanced datasets. Computational experiments using a real-world microblog database were conducted to investigate the benefits of integrating diverse heterogeneous data. Computational results show that the improved H-MK-SVM using longitudinal individual behavioral data exhibits superior performance over some commonly used methods using aggregated behavioral data and the improved H-MK-SVM using engagement behavioral data performs better than using only the external and individual behavioral data.},
}

@Article{CHIANG2018177,
  title = {Does country-of-origin brand personality generate retail customer lifetime value? A Big Data analytics approach},
  journal = {Technological Forecasting and Social Change},
  volume = {130},
  pages = {177 - 187},
  year = {2018},
  issn = {0040-1625},
  doi = {https://doi.org/10.1016/j.techfore.2017.06.034},
  url = {http://www.sciencedirect.com/science/article/pii/S0040162517309150},
  author = {Lan-Lung (Luke) Chiang and Chin-Sheng Yang},
  keywords = {Big Data analytics, Customer-driven marketing strategy, Country-of-origin, Brand personality, Customer lifetime value, Retail industry},
  abstract = {Many retail firms have witnessed the erosion of customer loyalty with the rise of e-commerce and its resulting benefits to consumers, including increased choices, lower prices, and ease of brand switching. Retailers have long collected data to learn about customer purchasing habits; however, many currently do not use data-mining analytics to increase marketing effectiveness by predicting future buying patterns and potential customer lifetime value, particularly to important segments such as loyal and potential repeat customers. Data mining can efficiently analyze large amounts of business data (“Big Data”) in an effort to forecast consumer needs and increase the lifetime value of customers (CLV). Previous studies on these topics primarily focus on conceptual assumptions and generally do not present empirically valid models. The present study sought to fill the research gap by using Big Data analytics to analyze approximately 44,000 point-of-sale transaction records for 26,000 customers of a Taiwanese retail store to understand how consumer personality traits relate to the country-of-origin (COO) traits (brand personality) of beer brands, and to predict potential customer lifetime value (CLV). The findings revealed that consumers tend to purchase and co-purchase brands with traits similar to their own personality traits (i.e., Japan—peacefulness, Belgium—openness, Ireland—excitement, etc.). Significantly, customers with the group of personality traits associated with “peacefulness” and “openness” were the most profitable customers among the five analyzed clusters (CLV value=0.3149, 0.2635). The study provides valuable new insights into COO brand personality and consumer personality traits with co-purchase behaviors via data mining techniques, and highlights the value of extending CLV in developing useful marketing strategies.},
}

@Article{LAHA20154687,
  title = {Modeling of steelmaking process with effective machine learning techniques},
  journal = {Expert Systems with Applications},
  volume = {42},
  number = {10},
  pages = {4687 - 4696},
  year = {2015},
  issn = {0957-4174},
  doi = {https://doi.org/10.1016/j.eswa.2015.01.030},
  url = {http://www.sciencedirect.com/science/article/pii/S0957417415000457},
  author = {Dipak Laha and Ye Ren and P.N. Suganthan},
  keywords = {Steelmaking process, Modeling, Prediction, Random forests, Support vector regression, Artificial neural networks, Dynamic evolving neural-fuzzy inference system, 5-Fold cross validation},
  abstract = {Monitoring and control of the output yield of steel in a steelmaking shop plays a critical role in steel industry. The yield of steel determines how much percentage of hot metal, scrap, and iron ore are being converted into steel ingots. It represents the operational efficiency of the steelmaking shop and is considered as an important performance measure for producing a specific quantity of steel. Due to complexity of the steelmaking process and nonlinear relationship between the process parameters, modeling the input–output process parameters and accurately predicting the output yield in the steelmaking shop is very difficult and has been a major research issue. Statistical models and artificial neural networks (ANN) have been extensively studied by researchers and practitioners to model a variety of complex processes. In the present study, we consider random forests (RF), ANN, dynamic evolving neuro-fuzzy inference system (DENFIS) and support vector regression (SVR) as competitive learning tools to verify the suitability of applications of these approaches and investigate their comparative predictive ability. In the present investigation, 0.00001 of MSE is set as a goal of learning during modeling. Based on real-life data, the computational results depict that the training and testing MSE values of SVR and DENFIS are close to 0.00001 indicating that they have higher prediction ability than ANN and RF. Also, mean absolute percentage prediction errors of the proposed models confirm that the predicted yield based on each method is in good agreement with the testing datasets. Overall, SVR performs best and DENFIS the next best followed by ANN and RF methods respectively. The results suggest that the prediction precision given by SVR can meet the requirement for the actual production of steel.},
}

@InCollection{KHARE202049,
  title = {Chapter 2 - Big data principles and paradigm},
  editor = {Vikas Khare and Savita Nema and Prashant Baredar},
  booktitle = {Ocean Energy Modeling and Simulation with Big Data},
  publisher = {Butterworth-Heinemann},
  pages = {49 - 81},
  year = {2020},
  isbn = {978-0-12-818904-7},
  doi = {https://doi.org/10.1016/B978-0-12-818904-7.00002-2},
  url = {http://www.sciencedirect.com/science/article/pii/B9780128189047000022},
  author = {Vikas Khare and Savita Nema and Prashant Baredar},
  keywords = {Big data, ocean energy system, mapreduce, Hadoop, NoSQL, data stream management},
  abstract = {Big data refers to the massive datasets that are collected from a variety of data sources for business’ needs to reveal new insights for optimized decision making. The ocean energy system is the modernization of electrical energy generation systems due to the pollution-free nature and the continuous advancement of ocean system technologies. In the ocean energy surroundings, the application of big data analysis-based decision-making and control are mainly in the following three aspects: data stream side management, storage side management, and load side management. The objective of this chapter is to present a technological framework for the management of large volumes and the variety and velocity of related information through big data tools such as NoSQL and Hadoop in order to support the assessment of ocean energy systems. This paper also includes the Market Basket model, an application of the MapReduce algorithm, and also provides light on data stream management and landscape ocean data systems.},
}

@Article{VELEZ2020105762,
  title = {Churn and Net Promoter Score forecasting for business decision-making through a new stepwise regression methodology},
  journal = {Knowledge-Based Systems},
  volume = {196},
  pages = {105762},
  year = {2020},
  issn = {0950-7051},
  doi = {https://doi.org/10.1016/j.knosys.2020.105762},
  url = {http://www.sciencedirect.com/science/article/pii/S0950705120301684},
  author = {D. Vélez and A. Ayuso and C. Perales-González and J. Tinguaro Rodríguez},
  keywords = {Churn prediction, Net Promoter Score, Stepwise regression, WOE variables},
  abstract = {Companies typically have to make relevant decisions regarding their clients’ fidelity and retention on the basis of analytical models developed to predict both their churn probability and Net Promoter Score (NPS). Although the predictive capability of these models is important, interpretability is a crucial factor to look for as well, because the decisions to be made from their results have to be properly justified. In this paper, a novel methodology to develop analytical models balancing predictive performance and interpretability is proposed, with the aim of enabling a better decision-making. It proceeds by fitting logistic regression models through a modified stepwise variable selection procedure, which automatically selects input variables while keeping their business logic, previously validated by an expert. In synergy with this procedure, a new method for transforming independent variables in order to better deal with ordinal targets and avoiding some logistic regression issues with outliers and missing data is also proposed. The combination of these two proposals with some competitive machine-learning methods earned the leading position in the NPS forecasting task of an international university talent challenge posed by a well-known global bank. The application of the proposed methodology and the results it obtained at this challenge are described as a case-study.},
}

@Article{ANTONOPOULOS2020109899,
  title = {Artificial intelligence and machine learning approaches to energy demand-side response: A systematic review},
  journal = {Renewable and Sustainable Energy Reviews},
  volume = {130},
  pages = {109899},
  year = {2020},
  issn = {1364-0321},
  doi = {https://doi.org/10.1016/j.rser.2020.109899},
  url = {http://www.sciencedirect.com/science/article/pii/S136403212030191X},
  author = {Ioannis Antonopoulos and Valentin Robu and Benoit Couraud and Desen Kirli and Sonam Norbu and Aristides Kiprakis and David Flynn and Sergio Elizondo-Gonzalez and Steve Wattam},
  keywords = {Artificial intelligence, Machine learning, Artificial neural networks, Nature-inspired intelligence, Multi-agent systems, Demand response, Power systems},
  abstract = {Recent years have seen an increasing interest in Demand Response (DR) as a means to provide flexibility, and hence improve the reliability of energy systems in a cost-effective way. Yet, the high complexity of the tasks associated with DR, combined with their use of large-scale data and the frequent need for near real-time decisions, means that Artificial Intelligence (AI) and Machine Learning (ML) — a branch of AI — have recently emerged as key technologies for enabling demand-side response. AI methods can be used to tackle various challenges, ranging from selecting the optimal set of consumers to respond, learning their attributes and preferences, dynamic pricing, scheduling and control of devices, learning how to incentivise participants in the DR schemes and how to reward them in a fair and economically efficient way. This work provides an overview of AI methods utilised for DR applications, based on a systematic review of over 160 papers, 40 companies and commercial initiatives, and 21 large-scale projects. The papers are classified with regards to both the AI/ML algorithm(s) used and the application area in energy DR. Next, commercial initiatives are presented (including both start-ups and established companies) and large-scale innovation projects, where AI methods have been used for energy DR. The paper concludes with a discussion of advantages and potential limitations of reviewed AI techniques for different DR tasks, and outlines directions for future research in this fast-growing area.},
}

@Article{YANG2018202,
  title = {An evidential reasoning-based decision support system for handling customer complaints in mobile telecommunications},
  journal = {Knowledge-Based Systems},
  volume = {162},
  pages = {202 - 210},
  year = {2018},
  note = {Special Issue on intelligent decision-making and consensus under uncertainty in inconsistent and dynamic environments},
  issn = {0950-7051},
  doi = {https://doi.org/10.1016/j.knosys.2018.09.029},
  url = {http://www.sciencedirect.com/science/article/pii/S0950705118304805},
  author = {Ying Yang and Dong-Ling Xu and Jian-Bo Yang and Yu-Wang Chen},
  keywords = {Decision support system, Customer complaint handling, Evidential reasoning rule, Classification, Mobile telecommunications},
  abstract = {Handling customer complaints is a decision-making process that inherently involves a classification problem where each complaint should be classified exclusively to one of the complaint categories before a resolution is communicated to customers. Previous studies focus extensively on decision support systems (DSSs) to automate complaint handling, while few addresses the issue of classification imprecision when inaccurate or inconsistent information exists in customer complaint narratives. This research presents a novel DSS for handling customer complaints and develops an evidential reasoning (ER) rule-based classifier as the core component of the system to classify customer complaints with uncertain information. More specifically, textual and numeric features are firstly combined to generate evidence for formulating the relationship between customer complaint features and classification results. The ER rule is then applied to combine multiple pieces of evidence and classify customer complaints into different categories with probabilities. An empirical study is conducted in a telecommunication company. Results show that the proposed ER rule-based classification model provides high performance in comparison with other machine learning algorithms. The developed system offers telecommunication companies an informative and data-driven method for handling customer complaints in a systematic and automatic manner.},
}

@Article{DIEZPASTOR201596,
  title = {Random Balance: Ensembles of variable priors classifiers for imbalanced data},
  journal = {Knowledge-Based Systems},
  volume = {85},
  pages = {96 - 111},
  year = {2015},
  issn = {0950-7051},
  doi = {https://doi.org/10.1016/j.knosys.2015.04.022},
  url = {http://www.sciencedirect.com/science/article/pii/S0950705115001720},
  author = {José F. Díez-Pastor and Juan J. Rodríguez and César García-Osorio and Ludmila I. Kuncheva},
  keywords = {Classifier ensembles, Imbalanced data sets, Bagging, AdaBoost, SMOTE, Undersampling},
  abstract = {In Machine Learning, a data set is imbalanced when the class proportions are highly skewed. Imbalanced data sets arise routinely in many application domains and pose a challenge to traditional classifiers. We propose a new approach to building ensembles of classifiers for two-class imbalanced data sets, called Random Balance. Each member of the Random Balance ensemble is trained with data sampled from the training set and augmented by artificial instances obtained using SMOTE. The novelty in the approach is that the proportions of the classes for each ensemble member are chosen randomly. The intuition behind the method is that the proposed diversity heuristic will ensure that the ensemble contains classifiers that are specialized for different operating points on the ROC space, thereby leading to larger AUC compared to other ensembles of classifiers. Experiments have been carried out to test the Random Balance approach by itself, and also in combination with standard ensemble methods. As a result, we propose a new ensemble creation method called RB-Boost which combines Random Balance with AdaBoost.M2. This combination involves enforcing random class proportions in addition to instance re-weighting. Experiments with 86 imbalanced data sets from two well known repositories demonstrate the advantage of the Random Balance approach.},
}

@Article{QIAO2020100,
  title = {Deep learning based software defect prediction},
  journal = {Neurocomputing},
  volume = {385},
  pages = {100 - 110},
  year = {2020},
  issn = {0925-2312},
  doi = {https://doi.org/10.1016/j.neucom.2019.11.067},
  url = {http://www.sciencedirect.com/science/article/pii/S0925231219316698},
  author = {Lei Qiao and Xuesong Li and Qasim Umer and Ping Guo},
  keywords = {Software defect prediction, Deep learning, Software quality, Software metrics, Robustness evaluation},
  abstract = {Software systems have become larger and more complex than ever. Such characteristics make it very challengeable to prevent software defects. Therefore, automatically predicting the number of defects in software modules is necessary and may help developers efficiently to allocate limited resources. Various approaches have been proposed to identify and fix such defects at minimal cost. However, the performance of these approaches require significant improvement. Therefore, in this paper, we propose a novel approach that leverages deep learning techniques to predict the number of defects in software systems. First, we preprocess a publicly available dataset, including log transformation and data normalization. Second, we perform data modeling to prepare the data input for the deep learning model. Third, we pass the modeled data to a specially designed deep neural network-based model to predict the number of defects. We also evaluate the proposed approach on two well-known datasets. The evaluation results illustrate that the proposed approach is accurate and can improve upon the state-of-the-art approaches. On average, the proposed method significantly reduces the mean square error by more than 14% and increases the squared correlation coefficient by more than 8%.},
}

@Article{FRANCE2019456,
  title = {Marketing analytics: Methods, practice, implementation, and links to other fields},
  journal = {Expert Systems with Applications},
  volume = {119},
  pages = {456 - 475},
  year = {2019},
  issn = {0957-4174},
  doi = {https://doi.org/10.1016/j.eswa.2018.11.002},
  url = {http://www.sciencedirect.com/science/article/pii/S095741741830722X},
  author = {Stephen L. France and Sanjoy Ghose},
  keywords = {Analytics, Prediction, Marketing, Visualization, Segmentation, Data mining},
  abstract = {Marketing analytics is a diverse field, with both academic researchers and practitioners coming from a range of backgrounds including marketing, expert systems, statistics, and operations research. This paper provides an integrative review at the boundary of these areas. The aim is to give researchers in the intelligent and expert systems community the opportunity to gain a broad view of the marketing analytics area and provide a starting point for future interdisciplinary collaboration. The topics of visualization, segmentation, and class prediction are featured. Links between the disciplines are emphasized. For each of these topics, a historical overview is given, starting with initial work in the 1960s and carrying through to the present day. Recent innovations for modern, large, and complex “big data” sets are described. Practical implementation advice is given, along with a directory of open source R routines for implementing marketing analytics techniques.},
}

@Article{PACHECO2020107213,
  title = {A framework to classify heterogeneous Internet traffic with Machine Learning and Deep Learning techniques for satellite communications},
  journal = {Computer Networks},
  volume = {173},
  pages = {107213},
  year = {2020},
  issn = {1389-1286},
  doi = {https://doi.org/10.1016/j.comnet.2020.107213},
  url = {http://www.sciencedirect.com/science/article/pii/S1389128619313544},
  author = {Fannia Pacheco and Ernesto Exposito and Mathieu Gineste},
  keywords = {Internet traffic classification, Machine learning, Satellite communications, QoS Management, Encrypted traffic},
  abstract = {Nowadays, the Internet network system serves as a platform for communication, transaction, and entertainment, among others. This communication system is characterized by terrestrial and Satellite components that interact between themselves to provide transmission paths of information between endpoints. Particularly, Satellite Communication providers’ interest is to improve customer satisfaction by optimally exploiting on demand available resources and offering Quality of Service (QoS). Improving the QoS implies to reduce errors linked to information loss and delays of Internet packets in Satellite Communications. In this sense, according to Internet traffic (Streaming, VoIP, Browsing, etc.) and those error conditions, the Internet flows can be classified into different sensitive and non-sensitive classes. Following this idea, this work aims at finding new Internet traffic classification approaches to improving the QoS. Machine Learning (ML) and Deep Learning (DL) techniques will be studied and deployed to classify Internet traffic. All the necessary elements to couple an ML or DL solution over a well-known Satellite Communication and QoS management architecture will be evaluated. To develop this solution, a rich and complete set of Internet traffic is required. In this context, an emulated Satellite Communication platform will serve as a data generation environment in which different Internet communications will be launched and captured. The proposed classification system will deal with different Internet communications (encrypted, unencrypted, and tunneled). This system will process the incoming traffic hierarchically to achieve a high classification performance. Finally, some experiments on a cloud emulated platform validates our proposal and set guidelines for its deployment over a Satellite architecture.},
}

@Article{HAIXIANG2017220,
  title = {Learning from class-imbalanced data: Review of methods and applications},
  journal = {Expert Systems with Applications},
  volume = {73},
  pages = {220 - 239},
  year = {2017},
  issn = {0957-4174},
  doi = {https://doi.org/10.1016/j.eswa.2016.12.035},
  url = {http://www.sciencedirect.com/science/article/pii/S0957417416307175},
  author = {Guo Haixiang and Li Yijing and Jennifer Shang and Gu Mingyun and Huang Yuanyue and Gong Bing},
  keywords = {Rare events, Imbalanced data, Machine learning, Data mining},
  abstract = {Rare events, especially those that could potentially negatively impact society, often require humans’ decision-making responses. Detecting rare events can be viewed as a prediction task in data mining and machine learning communities. As these events are rarely observed in daily life, the prediction task suffers from a lack of balanced data. In this paper, we provide an in depth review of rare event detection from an imbalanced learning perspective. Five hundred and seventeen related papers that have been published in the past decade were collected for the study. The initial statistics suggested that rare events detection and imbalanced learning are concerned across a wide range of research areas from management science to engineering. We reviewed all collected papers from both a technical and a practical point of view. Modeling methods discussed include techniques such as data preprocessing, classification algorithms and model evaluation. For applications, we first provide a comprehensive taxonomy of the existing application domains of imbalanced learning, and then we detail the applications for each category. Finally, some suggestions from the reviewed papers are incorporated with our experiences and judgments to offer further research directions for the imbalanced learning and rare event detection fields.},
}

@Article{CHEN20117451,
  title = {Optimal selection of potential customer range through the union sequential pattern by using a response model},
  journal = {Expert Systems with Applications},
  volume = {38},
  number = {6},
  pages = {7451 - 7461},
  year = {2011},
  issn = {0957-4174},
  doi = {https://doi.org/10.1016/j.eswa.2010.12.078},
  url = {http://www.sciencedirect.com/science/article/pii/S0957417410014417},
  author = {Wen-Chin Chen and Chiun-Chieh Hsu and Jing-Ning Hsu},
  keywords = {Response model, Union sequential pattern, Classification algorithm, Support vector machine, Logistic regression},
  abstract = {Direct marketing is a common data mining application. Previous studies largely adopt the approach that, as the subjects for response model predictions, the entire customer population or filtered through certain attribute values is based on recommendations made from sales professionals. However, such methods may reduce response rates due to an oversized potential customer population, thus diminishing the accuracy of the prediction model. In resolve this problem, this work presents proposes a novel forecasting method that integrates the union sequential pattern with classification algorithms to facilitate the construction of customer response models. Based on use of a union sequential pattern, the potential customer size is established by identifying attributes with a high level of association. The prediction model is then constructed using classification algorithms such as support vector machines and logistic regression. Consequently, the problem involving the setting of range for potential customers can be solved, as well as the time spent on processing extended lists of customers during prediction. Finally, predicted potential Internet-phone customers and churning mobile-phone customers of a telecommunication company in Taiwan as are taken as an illustrative example, based on the proposed prediction model. The proposed method more accurately predicts potential customers than those of previous studies.},
}

@Article{WANG201887,
  title = {Leveraging deep learning with LDA-based text analytics to detect automobile insurance fraud},
  journal = {Decision Support Systems},
  volume = {105},
  pages = {87 - 95},
  year = {2018},
  issn = {0167-9236},
  doi = {https://doi.org/10.1016/j.dss.2017.11.001},
  url = {http://www.sciencedirect.com/science/article/pii/S0167923617302130},
  author = {Yibo Wang and Wei Xu},
  keywords = {Insurance fraud, Fraud detection, Text analytics, Topic modeling, Deep learning},
  abstract = {Automobile insurance fraud represents a pivotal percentage of property insurance companies' costs and affects the companies' pricing strategies and social economic benefits in the long term. Automobile insurance fraud detection has become critically important for reducing the costs of insurance companies. Previous studies on automobile insurance fraud detection examined various numeric factors, such as the time of the claim and the brand of the insured car. However, the textual information in the claims has rarely been studied to analyze insurance fraud. This paper proposes a novel deep learning model for automobile insurance fraud detection that uses Latent Dirichlet Allocation (LDA)-based text analytics. In our proposed method, LDA is first used to extract the text features hiding in the text descriptions of the accidents appearing in the claims, and deep neural networks then are trained on the data, which include the text features and traditional numeric features for detecting fraudulent claims. Based on the real-world insurance fraud dataset, our experimental results reveal that the proposed text analytics-based framework outperforms a traditional one. Furthermore, the experimental results show that the deep neural networks outperform widely used machine learning models, such as random forests and support vector machine. Therefore, our proposed framework that combines deep neural networks and LDA is a suitable potential tool for automobile insurance fraud detection.},
}

@Article{ESTEVEZ2006337,
  title = {Subscription fraud prevention in telecommunications using fuzzy rules and neural networks},
  journal = {Expert Systems with Applications},
  volume = {31},
  number = {2},
  pages = {337 - 344},
  year = {2006},
  issn = {0957-4174},
  doi = {https://doi.org/10.1016/j.eswa.2005.09.028},
  url = {http://www.sciencedirect.com/science/article/pii/S0957417405002204},
  author = {Pablo A. Estévez and Claudio M. Held and Claudio A. Perez},
  keywords = {Fraud prevention, Fraud detection, Subscription fraud, Neural networks, Fuzzy rules},
  abstract = {A system to prevent subscription fraud in fixed telecommunications with high impact on long-distance carriers is proposed. The system consists of a classification module and a prediction module. The classification module classifies subscribers according to their previous historical behavior into four different categories: subscription fraudulent, otherwise fraudulent, insolvent and normal. The prediction module allows us to identify potential fraudulent customers at the time of subscription. The classification module was implemented using fuzzy rules. It was applied to a database containing information of over 10,000 real subscribers of a major telecom company in Chile. In this database, a subscription fraud prevalence of 2.2% was found. The prediction module was implemented as a multilayer perceptron neural network. It was able to identify 56.2% of the true fraudsters, screening only 3.5% of all the subscribers in the test set. This study shows the feasibility of significantly preventing subscription fraud in telecommunications by analyzing the application information and the customer antecedents at the time of application.},
}

@InCollection{NISBET2018169,
  title = {Chapter 9 - Classification},
  editor = {Robert Nisbet and Gary Miner and Ken Yale},
  booktitle = {Handbook of Statistical Analysis and Data Mining Applications (Second Edition)},
  publisher = {Academic Press},
  edition = {Second Edition},
  address = {Boston},
  pages = {169 - 186},
  year = {2018},
  isbn = {978-0-12-416632-5},
  doi = {https://doi.org/10.1016/B978-0-12-416632-5.00009-8},
  url = {http://www.sciencedirect.com/science/article/pii/B9780124166325000098},
  author = {Robert Nisbet and Gary Miner and Ken Yale},
  keywords = {What is classification?, Initial operations in classification, Major issues with classification, Assumptions of classification procedures, Analyzing imbalanced data sets with machine-learning programs, Phases in the operation of classification algorithms, Advantages and disadvantages of common classification algorithms, What is the best algorithm for classification?, Automated analytics—Is it the wave of the future?},
  abstract = {The first general set of data mining applications predicts to which category of the target variable each case belongs. This grouping activity is called classification. In this chapter, we will focus on the use of classification algorithms, rather than their descriptions.},
}

@Article{DEBOCK201112293,
  title = {An empirical evaluation of rotation-based ensemble classifiers for customer churn prediction},
  journal = {Expert Systems with Applications},
  volume = {38},
  number = {10},
  pages = {12293 - 12301},
  year = {2011},
  issn = {0957-4174},
  doi = {https://doi.org/10.1016/j.eswa.2011.04.007},
  url = {http://www.sciencedirect.com/science/article/pii/S0957417411005239},
  author = {Koen W. {De Bock} and Dirk Van {den Poel}},
  keywords = {CRM, Database marketing, Customer churn prediction, Ensemble classification, Rotation-based ensemble classifiers, RotBoost, Rotation Forest, ICA, AUC, Lift},
  abstract = {Several studies have demonstrated the superior performance of ensemble classification algorithms, whereby multiple member classifiers are combined into one aggregated and powerful classification model, over single models. In this paper, two rotation-based ensemble classifiers are proposed as modeling techniques for customer churn prediction. In Rotation Forests, feature extraction is applied to feature subsets in order to rotate the input data for training base classifiers, while RotBoost combines Rotation Forest with AdaBoost. In an experimental validation based on data sets from four real-life customer churn prediction projects, Rotation Forest and RotBoost are compared to a set of well-known benchmark classifiers. Moreover, variations of Rotation Forest and RotBoost are compared, implementing three alternative feature extraction algorithms: principal component analysis (PCA), independent component analysis (ICA) and sparse random projections (SRP). The performance of rotation-based ensemble classifier is found to depend upon: (i) the performance criterion used to measure classification performance, and (ii) the implemented feature extraction algorithm. In terms of accuracy, RotBoost outperforms Rotation Forest, but none of the considered variations offers a clear advantage over the benchmark algorithms. However, in terms of AUC and top-decile lift, results clearly demonstrate the competitive performance of Rotation Forests compared to the benchmark algorithms. Moreover, ICA-based Rotation Forests outperform all other considered classifiers and are therefore recommended as a well-suited alternative classification technique for the prediction of customer churn that allows for improved marketing decision making.},
}

@Article{ESHGHI2019382,
  title = {Introducing a new method for the fusion of fraud evidence in banking transactions with regards to uncertainty},
  journal = {Expert Systems with Applications},
  volume = {121},
  pages = {382 - 392},
  year = {2019},
  issn = {0957-4174},
  doi = {https://doi.org/10.1016/j.eswa.2018.11.039},
  url = {http://www.sciencedirect.com/science/article/pii/S0957417418307590},
  author = {Abdollah Eshghi and Mehrdad Kargari},
  keywords = {Fraud detection, IFS, DST, Uncertainty, Evidential reasoning},
  abstract = {Detection of fraudulent transactions is a vital factor for financial institutions, and finding more effective and accurate methods is of tremendous importance. The use of supervised data mining techniques is not feasible in many cases due to the lack of access to labeled data. Fraud detection is a complex task, and unsupervised methods like clustering and outlier detection techniques employed alone do not yield satisfactory results. Another issue is epistemic uncertainty due to the absence of sufficient information on the behavioral aspects of different customers, which also leads to poorer results for fraud detection and makes the fraud detection system inapplicable in real world environment. In this paper, using multi criteria decision method, intuitionistic fuzzy set, and evidential reasoning, a new method for detection of fraud was introduced, which infuses several behavioral evidence of a transaction concerning the effect of uncertainty for them. Transactional behavior was modeled by considering the trends of different main and aggregated variables at different periods and the extent of deviation of the new arrived transaction from each of these trends were considered as behavioral evidence. The final belief, which is the result of the combination of much evidence using the proposed method, will determine the originality of a newly arrived transaction. Finally, using a real world dataset, the results of the new method were compared with the results of Dempster–Shafer method in terms of the number of frauds discovered and the number of erroneous alerts they issued. The findings showed that the method introduced in this study has higher accuracy and lower false alarms compared to Dempster–Shafer method while the computational complexity of this method makes its implementation time longer.},
}

@Article{LESSMANN2019,
  title = {Targeting customers for profit: An ensemble learning framework to support marketing decision-making},
  journal = {Information Sciences},
  year = {2019},
  issn = {0020-0255},
  doi = {https://doi.org/10.1016/j.ins.2019.05.027},
  url = {http://www.sciencedirect.com/science/article/pii/S0020025519304268},
  author = {Stefan Lessmann and Johannes Haupt and Kristof Coussement and Koen W. {De Bock}},
  keywords = {Profit analytics, Marketing decision support, Machine learning, Business value},
  abstract = {Marketing messages are most effective if they reach the right customers. Deciding which customers to contact is an important task in campaign planning. The paper focuses on empirical targeting models. We argue that common practices to develop such models do not account sufficiently for business goals. To remedy this, we propose profit-conscious ensemble selection, a modeling framework that integrates statistical learning principles and business objectives in the form of campaign profit maximization. Studying the interplay between data-driven learning methods and their business value in real-world application contexts, the paper contributes to the emerging field of profit analytics and provides original insights how to implement profit analytics in marketing. The paper also estimates the degree to which profit-concious modeling adds to the bottom line. The results of a comprehensive empirical study confirm the business value of the proposed ensemble learning framework in that it recommends substantially more profitable target groups than several benchmarks.},
}

@Article{GILL2019100118,
  title = {Transformative effects of IoT, Blockchain and Artificial Intelligence on cloud computing: Evolution, vision, trends and open challenges},
  journal = {Internet of Things},
  volume = {8},
  pages = {100118},
  year = {2019},
  issn = {2542-6605},
  doi = {https://doi.org/10.1016/j.iot.2019.100118},
  url = {http://www.sciencedirect.com/science/article/pii/S2542660519302331},
  author = {Sukhpal Singh Gill and Shreshth Tuli and Minxian Xu and Inderpreet Singh and Karan Vijay Singh and Dominic Lindsay and Shikhar Tuli and Daria Smirnova and Manmeet Singh and Udit Jain and Haris Pervaiz and Bhanu Sehgal and Sukhwinder Singh Kaila and Sanjay Misra and Mohammad Sadegh Aslanpour and Harshit Mehta and Vlado Stankovski and Peter Garraghan},
  keywords = {Cloud computing, Quality of Service, Cloud applications, Cloud paradigms and technologies, IoT, Blockchain, Artificial Intelligence},
  abstract = {Cloud computing plays a critical role in modern society and enables a range of applications from infrastructure to social media. Such system must cope with varying load and evolving usage reflecting societies’ interaction and dependency on automated computing systems whilst satisfying Quality of Service (QoS) guarantees. Enabling these systems are a cohort of conceptual technologies, synthesized to meet demand of evolving computing applications. In order to understand current and future challenges of such system, there is a need to identify key technologies enabling future applications. In this study, we aim to explore how three emerging paradigms (Blockchain, IoT and Artificial Intelligence) will influence future cloud computing systems. Further, we identify several technologies driving these paradigms and invite international experts to discuss the current status and future directions of cloud computing. Finally, we proposed a conceptual model for cloud futurology to explore the influence of emerging paradigms and technologies on evolution of cloud computing.},
}

@Book{2017601,
  title = {Index},
  author = {Ian H. Witten and Eibe Frank and Mark A. Hall and Christopher J. Pal},
  booktitle = {Data Mining (Fourth Edition)},
  publisher = {Morgan Kaufmann},
  edition = {Fourth Edition},
  pages = {601 - 621},
  year = {2017},
  isbn = {978-0-12-804291-5},
  doi = {https://doi.org/10.1016/B978-0-12-804291-5.00027-1},
  url = {http://www.sciencedirect.com/science/article/pii/B9780128042915000271},
}

@Article{SHUKLA2020103625,
  title = {A bibliometric analysis and cutting-edge overview on fuzzy techniques in Big Data},
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {92},
  pages = {103625},
  year = {2020},
  issn = {0952-1976},
  doi = {https://doi.org/10.1016/j.engappai.2020.103625},
  url = {http://www.sciencedirect.com/science/article/pii/S0952197620300877},
  author = {Amit K. Shukla and Pranab K. Muhuri and Ajith Abraham},
  keywords = {Big data, Fuzzy sets, Type-2 fuzzy sets, Bibliometric study, Web of science, Scopus},
  abstract = {Over the last few years, Big Data has gained a tremendous attention from the research community. The data being generated in huge quantity from almost every field is unstructured and unprocessed. Extracting knowledge base and useful information from the big raw data is one of the major challenges, present today. Various computational intelligence and soft computing techniques have been proposed for efficient big data analytics. Fuzzy techniques are one of the soft computing approaches which can play a very crucial role in current big data challenges by pre-processing and reconstructing data. There is a wide spread application domains where traditional fuzzy sets (type-1 fuzzy sets) and higher order fuzzy sets (type-2 fuzzy sets) have shown remarkable outcomes. Although, this research domain of “fuzzy techniques in Big Data” is gaining some attention, there is a strong need for a motivation to encourage researchers to explore more in this area. In this paper, we have conducted bibliometric study on recent development in the field of “fuzzy techniques in big data”. In bibliometric study, various performance metrics including total papers, total citations, and citation per paper are calculated. Further, top 10 of most productive and highly cited authors, discipline, source journals, countries, institutions, and highly influential papers are also evaluated. Later, a comparative analysis is performed on the fuzzy techniques in big data after analysing the most influential works in this field.},
}

@Article{LESSMANN2009520,
  title = {A reference model for customer-centric data mining with support vector machines},
  journal = {European Journal of Operational Research},
  volume = {199},
  number = {2},
  pages = {520 - 530},
  year = {2009},
  issn = {0377-2217},
  doi = {https://doi.org/10.1016/j.ejor.2008.12.017},
  url = {http://www.sciencedirect.com/science/article/pii/S0377221708010515},
  author = {Stefan Lessmann and Stefan Voß},
  keywords = {Marketing, Data mining, Customer relationship management, Support vector machines},
  abstract = {Supervised classification is an important part of corporate data mining to support decision making in customer-centric planning tasks. The paper proposes a hierarchical reference model for support vector machine based classification within this discipline. The approach balances the conflicting goals of transparent yet accurate models and compares favourably to alternative classifiers in a large-scale empirical evaluation in real-world customer relationship management applications. Recent advances in support vector machine oriented research are incorporated to approach feature, instance and model selection in a unified framework.},
}

@Article{KIM2019214,
  title = {Champion-challenger analysis for credit card fraud detection: Hybrid ensemble and deep learning},
  journal = {Expert Systems with Applications},
  volume = {128},
  pages = {214 - 224},
  year = {2019},
  issn = {0957-4174},
  doi = {https://doi.org/10.1016/j.eswa.2019.03.042},
  url = {http://www.sciencedirect.com/science/article/pii/S0957417419302167},
  author = {Eunji Kim and Jehyuk Lee and Hunsik Shin and Hoseong Yang and Sungzoon Cho and Seung-kwan Nam and Youngmi Song and Jeong-a Yoon and Jong-il Kim},
  keywords = {Credit card fraud detection, Deep learning, Hybrid ensemble, Model evaluation, Class imbalance},
  abstract = {Credit card fraud detection is an essential part of screening fraudulent transactions in advance of their authorization by card issuers. Although credit card frauds occur extremely infrequently, they result in huge losses as most fraudulent transactions have large values. An adequate detection of fraud allows investigators to take timely actions that can potentially prevent additional fraud or financial losses. In practice, however, investigators can only check a few alerts per day since the investigation process can be long and tedious. Thus, the primary goal of the fraud detection model is to return accurate alerts with fewer false alarms and missed frauds. Conventional fraud detection is mainly based on the hybrid ensemble of diverse machine learning models. Recently, several studies have compared deep learning and traditional machine learning models including ensemble. However, these studies used evaluation methods without considering that the real-world fraud detection system operated with the constraints: (i) the number of investigators who check the high-risk transactions from the data-driven scoring models are limited and (ii) the two types of misclassification, false alarms and missed frauds, have different costs. In this study, we conducted an in-depth comparison between the hybrid ensemble and deep learning method to determine whether or not to adopt the latter in our partner’s system that currently operates with the hybrid ensemble model. To compare the two, we introduced the champion-challenger framework and the development process of the two models. After developing the two models, we evaluated them on large transaction data sets taken from our partner, a major card issuing company in South Korea. We used various practical evaluation metrics appropriate for this domain that has severe class and cost imbalances. Moreover, we deployed these models in a real-world fraud detection system to check the post-launch performance for one month. The challenger outperformed the champion on both in off-line and post-launch tests.},
}

@Article{PARK201715,
  title = {A deep learning-based sports player evaluation model based on game statistics and news articles},
  journal = {Knowledge-Based Systems},
  volume = {138},
  pages = {15 - 26},
  year = {2017},
  issn = {0950-7051},
  doi = {https://doi.org/10.1016/j.knosys.2017.09.028},
  url = {http://www.sciencedirect.com/science/article/pii/S095070511730446X},
  author = {Young Joon Park and Hyung Seok Kim and Donghwa Kim and Hankyu Lee and Seoung Bum Kim and Pilsung Kang},
  keywords = {Sports player evaluation, Deep neural network, Sentence polarity, Baseball},
  abstract = {Player evaluation is a key component of the question-answering (QA) system in sports. Since existing player evaluation methods heavily rely on game statistics, they cannot capture the qualitative impact of each player during a game, which can be exploited using news articles after the game. In this paper, we propose a deep learning-based player evaluation model by combining both quantitative game statistics and the qualitative analyses provided by news articles. Players are classified as positive or negative based on their performance during certain periods, and news articles in the same period are annotated using the player's class. Then, the relationship between news articles and the annotated polarity is investigated by a deep neural network, which can deal with the high dimensionality of the text data. Since there is no explicit polarity label for news articles, we use the change in game statistics in target periods to annotate related sentences. The proposed system is applied to a Korean professional baseball league (KBO) and it is shown to be capable of understanding the sentence polarity of news articles on player performances.},
}

@Article{KERAMATI2014994,
  title = {Improved churn prediction in telecommunication industry using data mining techniques},
  journal = {Applied Soft Computing},
  volume = {24},
  pages = {994 - 1012},
  year = {2014},
  issn = {1568-4946},
  doi = {https://doi.org/10.1016/j.asoc.2014.08.041},
  url = {http://www.sciencedirect.com/science/article/pii/S1568494614004062},
  author = {A. Keramati and R. Jafari-Marandi and M. Aliannejadi and I. Ahmadian and M. Mozaffari and U. Abbasi},
  keywords = {Telecommunication, Churn prediction, ANN, KNN, SVM, Decision tree},
  abstract = {To survive in today's telecommunication business it is imperative to distinguish customers who are not reluctant to move toward a competitor. Therefore, customer churn prediction has become an essential issue in telecommunication business. In such competitive business a reliable customer predictor will be regarded priceless. This paper has employed data mining classification techniques including Decision Tree, Artificial Neural Networks, K-Nearest Neighbors, and Support Vector Machine so as to compare their performances. Using the data of an Iranian mobile company, not only were these techniques experienced and compared to one another, but also we have drawn a parallel between some different prominent data mining software. Analyzing the techniques’ behavior and coming to know their specialties, we proposed a hybrid methodology which made considerable improvements to the value of some of the evaluations metrics. The proposed methodology results showed that above 95% accuracy for Recall and Precision is easily achievable. Apart from that a new methodology for extracting influential features in dataset was introduced and experienced.},
}

@Article{DELEN2020575,
  title = {Development of a Bayesian Belief Network-based DSS for predicting and understanding freshmen student attrition},
  journal = {European Journal of Operational Research},
  volume = {281},
  number = {3},
  pages = {575 - 587},
  year = {2020},
  note = {Featured Cluster: Business Analytics: Defining the field and identifying a research agenda},
  issn = {0377-2217},
  doi = {https://doi.org/10.1016/j.ejor.2019.03.037},
  url = {http://www.sciencedirect.com/science/article/pii/S0377221719302954},
  author = {Dursun Delen and Kazim Topuz and Enes Eryarsoy},
  keywords = {Student retention, Prediction, Elastic net, Bayesian Belief Network (BBN), Imbalance data},
  abstract = {Student attrition – the departure from an institution of higher learning prior to the achievement of a degree or earning due educational credentials – is an administratively important, scientifically interesting and yet practically challenging problem for decision makers and researchers. This study aims to find the prominent variables and their conditional dependencies/interrelations that affect student attrition in college settings. Specifically, using a large and feature-rich dataset, proposed methodology successfully captures the probabilistic interactions between attrition (the dependent variable) and related factors (the independent variables) to reveal the underlying, potentially complex/non-linear relationships. The proposed methodology successfully predicts the individual students' attrition risk through a Bayesian Belief Network-driven probabilistic model. The findings suggest that the proposed probabilistic graphical/network method is capable of predicting student attrition with 84% in AUC – Area Under the Receiver Operating Characteristics Curve. Using a 2-by-2 investigational design framework, this body of research also compares the impact and contribution of data balancing and feature selection to the resultant prediction models. The results show that (1) the imbalanced dataset produces similar predictive results in detecting the at-risk students, and (2) the feature selection, which is the process of identifying and eliminating unnecessary/unimportant predictors, results in simpler, more understandable, interpretable, and actionable results without compromising on the accuracy of the prediction task.},
}

@InCollection{NISBET2018279,
  title = {Chapter 14 - Customer Response Modeling},
  editor = {Robert Nisbet and Gary Miner and Ken Yale},
  booktitle = {Handbook of Statistical Analysis and Data Mining Applications (Second Edition)},
  publisher = {Academic Press},
  edition = {Second Edition},
  address = {Boston},
  pages = {279 - 288},
  year = {2018},
  isbn = {978-0-12-416632-5},
  doi = {https://doi.org/10.1016/B978-0-12-416632-5.00014-1},
  url = {http://www.sciencedirect.com/science/article/pii/B9780124166325000141},
  author = {Robert Nisbet and Gary Miner and Ken Yale},
  keywords = {Early CRM issues in business, Knowing how customers behaved before they acted, CRM in business ecosystems, Retention, Up-sell, Cross sell, LTV=life time value modeling, Ecosystems, Business ecosystems, Industrial revolution, Static measures, Evolutionary measures, Qualitative abstraction, Generalization abstraction, Temporal abstractions, Lag variable},
  abstract = {Most organizations, whether for profit or nonprofit purposes, exist to develop and promote some things or ideas related to their organization. One of the major activities of these organizations is to appeal to people outside their organizations to join them, support them, or purchase their goods or services. Traditional means of doing this included offering goods and services in storefronts, by advertisements in appropriate venues and by contacting a broad spectrum of people by phone or mail. These methods are rather passive. The philosophy was to build it, show it, and advertise or promote it, and customers would come. Since the early 1990s, many businesses have taken a more active approach by using various technological approaches to identify specific prospective customers and going after their business, rather than waiting for them to respond to the passive appeals. The key issue in this process is identifying which prospects are most likely to respond to the appeals. The activity of identifying prospects and quantifying their likelihood to respond is one of earliest applications of data mining technology to business.},
}

@Article{VERBEKE2017858,
  title = {RULEM: A novel heuristic rule learning approach for ordinal classification with monotonicity constraints},
  journal = {Applied Soft Computing},
  volume = {60},
  pages = {858 - 873},
  year = {2017},
  issn = {1568-4946},
  doi = {https://doi.org/10.1016/j.asoc.2017.01.042},
  url = {http://www.sciencedirect.com/science/article/pii/S1568494617300571},
  author = {Wouter Verbeke and David Martens and Bart Baesens},
  keywords = {Ordinal classification, Rule learning, Monotonicity constraints, Justifiability, Heuristic, Post-processing},
  abstract = {In many real world applications classification models are required to be in line with domain knowledge and to respect monotone relations between predictor variables and the target class, in order to be acceptable for implementation. This paper presents a novel heuristic approach, called RULEM, to induce monotone ordinal rule based classification models. The proposed approach can be applied in combination with any rule- or tree-based classification technique, since monotonicity is guaranteed in a post-processing step. RULEM checks whether a rule set or decision tree violates the imposed monotonicity constraints and existing violations are resolved by inducing a set of additional rules which enforce monotone classification. The approach is able to handle non-monotonic noise, and can be applied to both partially and totally monotone problems with an ordinal target variable. Two novel justifiability measures are introduced which are based on RULEM and allow to calculate the extent to which a classification model is in line with domain knowledge expressed in the form of monotonicity constraints. An extensive benchmarking experiment and subsequent statistical analysis of the results on 14 public data sets indicates that RULEM preserves the predictive power of a rule induction technique while guaranteeing monotone classification. On the other hand, the post-processed rule sets are found to be significantly larger which is due to the induction of additional rules. E.g., when combined with Ripper a median performance difference was observed in terms of PCC equal to zero and an average difference equal to −0.66%, with on average 5 rules added to the rule sets. The average and minimum justifiability of the original rule sets equal respectively 92.66% and 34.44% in terms of the RULEMF justifiability index, and 91.28% and 40.1% in terms of RULEMS, indicating the effective need for monotonizing the rule sets.},
}

@InCollection{PYLE2003275,
  title = {Chapter 9 - Getting Started},
  editor = {Dorian Pyle},
  booktitle = {Business Modeling and Data Mining},
  publisher = {Morgan Kaufmann},
  address = {San Francisco},
  pages = {275 - 311},
  year = {2003},
  series = {The Morgan Kaufmann Series in Data Management Systems},
  isbn = {978-1-55860-653-1},
  doi = {https://doi.org/10.1016/B978-155860653-1.50012-9},
  url = {http://www.sciencedirect.com/science/article/pii/B9781558606531500129},
  author = {Dorian Pyle},
  abstract = {This chapter is an approach to get raw data into a state that is appropriate for mining purposes. Any miner uses these basic techniques for addressing almost any business problem. Miners need to note preparation techniques when mining data in other domains, such as biomedical data, industrial automation data, telemetry data, geophysical data, time domain data, and so on. Developing mined models requires a hard work. It involves working with data, including making changes and taking actions that depend on the needs of the business problem and the miner's data discovery. Mining data is not magic, and it is not something that computer software will do for any one. Basically, data mining is an organized way of working with data, digging out useful information, and application of that useful information in solving the business problems. Most of the tools that are currently in use were developed from three main areas including statistics, artificial intelligence, and machine learning. Despite apparently different roots, these tools essentially do only one thing that is related to the discovery of a relationship, which more or less maps measurements in one part of a data set to measurements in another linked part of the data set.},
}

@Article{AMIN2017242,
  title = {Customer churn prediction in the telecommunication sector using a rough set approach},
  journal = {Neurocomputing},
  volume = {237},
  pages = {242 - 254},
  year = {2017},
  issn = {0925-2312},
  doi = {https://doi.org/10.1016/j.neucom.2016.12.009},
  url = {http://www.sciencedirect.com/science/article/pii/S0925231216314849},
  author = {Adnan Amin and Sajid Anwar and Awais Adnan and Muhammad Nawaz and Khalid Alawfi and Amir Hussain and Kaizhu Huang},
  keywords = {Classification, Churn prediction, Data mining, Feature selection, Rough Set theory},
  abstract = {Customer churn is a critical and challenging problem affecting business and industry, in particular, the rapidly growing, highly competitive telecommunication sector. It is of substantial interest to both academic researchers and industrial practitioners, interested in forecasting the behavior of customers in order to differentiate the churn from non-churn customers. The primary motivation is the dire need of businesses to retain existing customers, coupled with the high cost associated with acquiring new ones. A review of the field has revealed a lack of efficient, rule-based Customer Churn Prediction (CCP) approaches in the telecommunication sector. This study proposes an intelligent rule-based decision-making technique, based on rough set theory (RST), to extract important decision rules related to customer churn and non-churn. The proposed approach effectively performs classification of churn from non-churn customers, along with prediction of those customers who will churn or may possibly churn in the near future. Extensive simulation experiments are carried out to evaluate the performance of our proposed RST based CCP approach using four rule-generation mechanisms, namely, the Exhaustive Algorithm (EA), Genetic Algorithm (GA), Covering Algorithm (CA) and the LEM2 algorithm (LA). Empirical results show that RST based on GA is the most efficient technique for extracting implicit knowledge in the form of decision rules from the publicly available, benchmark telecom dataset. Further, comparative results demonstrate that our proposed approach offers a globally optimal solution for CCP in the telecom sector, when benchmarked against several state-of-the-art methods. Finally, we show how attribute-level analysis can pave the way for developing a successful customer retention policy that could form an indispensable part of strategic decision making and planning process in the telecom sector.},
}

@Article{MALDONADO2017113,
  title = {Integrated framework for profit-based feature selection and SVM classification in credit scoring},
  journal = {Decision Support Systems},
  volume = {104},
  pages = {113 - 121},
  year = {2017},
  issn = {0167-9236},
  doi = {https://doi.org/10.1016/j.dss.2017.10.007},
  url = {http://www.sciencedirect.com/science/article/pii/S0167923617301951},
  author = {Sebastián Maldonado and Cristián Bravo and Julio López and Juan Pérez},
  keywords = {Profit measure, Group penalty, Credit scoring, Support Vector Machines, Analytics},
  abstract = {In this paper, we propose a profit-driven approach for classifier construction and simultaneous variable selection based on linear Support Vector Machines. The main goal is to incorporate business-related information such as the variable acquisition costs, the Types I and II error costs, and the profit generated by correctly classified instances, into the modeling process. Our proposal incorporates a group penalty function in the SVM formulation in order to penalize the variables simultaneously that belong to the same group, assuming that companies often acquire groups of related variables for a given cost rather than acquiring them individually. The proposed framework was studied in a credit scoring problem for a Chilean bank, and led to superior performance with respect to business-related goals.},
}

@Article{BINDU2016213,
  title = {Mining social networks for anomalies: Methods and challenges},
  journal = {Journal of Network and Computer Applications},
  volume = {68},
  pages = {213 - 229},
  year = {2016},
  issn = {1084-8045},
  doi = {https://doi.org/10.1016/j.jnca.2016.02.021},
  url = {http://www.sciencedirect.com/science/article/pii/S1084804516300029},
  author = {P.V. Bindu and P. Santhi Thilagam},
  keywords = {Anomaly detection, Online social networks, Graph mining, Graph anomaly detection, Outlier detection, Social Network Analysis},
  abstract = {Online social networks have received a dramatic increase of interest in the last decade due to the growth of Internet and Web 2.0. They are among the most popular sites on the Internet that are being used in almost all areas of life including education, medical, entertainment, business, and telemarketing. Unfortunately, they have become primary targets for malicious users who attempt to perform illegal activities and cause harm to other users. The unusual behavior of such users can be identified by using anomaly detection techniques. Anomaly detection in social networks refers to the problem of identifying the strange and unexpected behavior of users by exploring the patterns hidden in the networks, as the patterns of interaction of such users deviate significantly from the normal users of the networks. Even though a multitude of anomaly detection methods have been developed for different problem settings, this field is still relatively young and rapidly growing. Hence, there is a growing need for an organized study of the work done in the area of anomaly detection in social networks. In this paper, we provide a comprehensive review of a large set of methods for mining social networks for anomalies by providing a multi-level taxonomy to categorize the existing techniques based on the nature of input network, the type of anomalies they detect, and the underlying anomaly detection approach. In addition, this paper highlights the various application scenarios where these methods have been used, and explores the research challenges and open issues in this field.},
}

@Book{ville_microsoft_2001,
  address = {Boston},
  edition = {1st Edition},
  title = {Microsoft {Data} {Mining}: {Integrated} {Business} {Intelligence} for e-{Commerce} and {Knowledge} {Management}},
  isbn = {978-1-55558-242-5},
  shorttitle = {Microsoft {Data} {Mining}},
  abstract = {Microsoft Data Mining approaches data mining from the particular perspective of IT professionals using Microsoft data management technologies. The author explains the new data mining capabilities in Microsoft's SQL Server 2000 database, Commerce Server, and other products, details the Microsoft OLE DB for Data Mining standard, and gives readers best practices for using all of them. The book bridges the previously specialized field of data mining with the new technologies and methods that are quickly making it an important mainstream tool for companies of all sizes.Data mining refers to a set of technologies and techniques by which IT professionals search large databases of information (such as those contained by SQL Server) for patterns and trends. Traditionally important in finance, telecommunication, and other information-intensive fields, data mining increasingly helps companies better understand and serve their customers by revealing buying patterns and related interests. It is becoming a foundation for e-commerce and knowledge management.},
  language = {English},
  publisher = {Digital Press},
  author = {Barry de Ville},
  month = {may},
  year = {2001},
  doi = {https://doi.org/10.1016/B978-155558242-5/50016-4},
  url = {http://www.sciencedirect.com/science/article/pii/B9781555582425500164},
}

@Article{VANWEZEL2007436,
  title = {Improved customer choice predictions using ensemble methods},
  journal = {European Journal of Operational Research},
  volume = {181},
  number = {1},
  pages = {436 - 452},
  year = {2007},
  issn = {0377-2217},
  doi = {https://doi.org/10.1016/j.ejor.2006.05.029},
  url = {http://www.sciencedirect.com/science/article/pii/S0377221706003900},
  author = {Michiel {van Wezel} and Rob Potharst},
  keywords = {Marketing, Bagging, Bias/variance decomposition, Boosting, Brand choice, CART, Choice models, Data mining, Ensembles},
  abstract = {In this paper various ensemble learning methods from machine learning and statistics are considered and applied to the customer choice modeling problem. The application of ensemble learning usually improves the prediction quality of flexible models like decision trees and thus leads to improved predictions. We give experimental results for two real-life marketing datasets using decision trees, ensemble versions of decision trees and the logistic regression model, which is a standard approach for this problem. The ensemble models are found to improve upon individual decision trees and outperform logistic regression. Next, an additive decomposition of the prediction error of a model is considered known as the bias/variance decomposition. A model with a high bias lacks the flexibility to fit the data well. A high variance indicates that a model is instable with respect to different datasets. Decision trees have a high variance component and a low bias component in the prediction error, whereas logistic regression has a high bias component and a low variance component. It is shown that ensemble methods aim at minimizing the variance component in the prediction error while leaving the bias component unaltered. Bias/variance decompositions for all models for both customer choice datasets are given to illustrate these concepts.},
}

@Article{LUGHOFER2020425,
  title = {On-line anomaly detection with advanced independent component analysis of multi-variate residual signals from causal relation networks},
  journal = {Information Sciences},
  volume = {537},
  pages = {425 - 451},
  year = {2020},
  issn = {0020-0255},
  doi = {https://doi.org/10.1016/j.ins.2020.06.034},
  url = {http://www.sciencedirect.com/science/article/pii/S0020025520306204},
  author = {Edwin Lughofer and Alexandru-Ciprian Zavoianu and Robert Pollak and Mahardhika Pratama and Pauline Meyer-Heye and Helmut Zörrer and Christian Eitzinger and Thomas Radauer},
  keywords = {On-line anomaly detection, Causal relation networks, Advanced multi-variate residual analysis, Dominant parts of independent component analysis, Automated control limits, On-line production systems},
  abstract = {Anomaly detection in todays industrial environments is an ambitious challenge to detect possible faults/problems which may turn into severe waste during production, defects, or systems components damage, at an early stage. Data-driven anomaly detection in multi-sensor networks rely on models which are extracted from multi-sensor measurements and which characterize the anomaly-free reference situation. Therefore, significant deviations to these models indicate potential anomalies. In this paper, we propose a new approach which is based on causal relation networks (CRNs) that represent the inner causes and effects between sensor channels (or sensor nodes) in form of partial sub-relations, and evaluate its functionality and performance on two distinct production phases within a micro-fluidic chip manufacturing scenario. The partial relations are modeled by non-linear (fuzzy) regression models for characterizing the (local) degree of influences of the single causes on the effects. An advanced analysis of the multi-variate residual signals, obtained from the partial relations in the CRNs, is conducted. It employs independent component analysis (ICA) to characterize hidden structures in the fused residuals through independent components (latent variables) as obtained through the demixing matrix. A significant change in the energy content of latent variables, detected through automated control limits, indicates an anomaly. Suppression of possible noise content in residuals—to decrease the likelihood of false alarms—is achieved by performing the residual analysis solely on the dominant parts of the demixing matrix. Our approach could detect anomalies in the process which caused bad quality chips (with the occurrence of malfunctions) with negligible delay based on the process data recorded by multiple sensors in two production phases: injection molding and bonding, which are independently carried out with completely different process parameter settings and on different machines (hence, can be seen as two distinct use cases). Our approach furthermore i.) produced lower false alarm rates than several related and well-known state-of-the-art methods for (unsupervised) anomaly detection, and ii.) also caused much lower parametrization efforts (in fact, none at all). Both aspects are essential for the useability of an anomaly detection approach.},
}

@Article{PENG2020337,
  title = {Do the pieces fit? Assessing the configuration effects of promotion attributes},
  journal = {Journal of Business Research},
  volume = {109},
  pages = {337 - 349},
  year = {2020},
  issn = {0148-2963},
  doi = {https://doi.org/10.1016/j.jbusres.2019.11.081},
  url = {http://www.sciencedirect.com/science/article/pii/S0148296319307581},
  author = {Ling Peng and Geng Cui and Yuho Chung},
  keywords = {Sales promotion, Configuration theory, Boosted tree approach, Marketing strategies},
  abstract = {Despite the extensive resources allocated to sales promotions, managers are still often unsure which combinations of promotion attributes, in what circumstances, will be most effective in achieving their marketing objectives. This problem has intensified as more firms move online and engage in frequent promotions. We conduct two empirical studies using field data from hundreds of online campaigns to identify the significant interactions that lead to an immediate sales effect. We adopt a boosted tree (BT) approach to investigate how promotional attributes can be aligned with one another and with other contextual variables to achieve synergy, and propose a parametric test to assess the statistical significance of these interactions. Our findings provide valuable insights into effective promotional designs. We also compare the proposed approach with various machine learning methods to demonstrate the merit of the BT approach and its potential applications for strategic configurations in business and marketing.},
}

@Article{MALEKMOHAMADI2011487,
  title = {Evaluating the efficacy of SVMs, BNs, ANNs and ANFIS in wave height prediction},
  journal = {Ocean Engineering},
  volume = {38},
  number = {2},
  pages = {487 - 497},
  year = {2011},
  issn = {0029-8018},
  doi = {https://doi.org/10.1016/j.oceaneng.2010.11.020},
  url = {http://www.sciencedirect.com/science/article/pii/S0029801810002659},
  author = {Iman Malekmohamadi and Mohammad Reza Bazargan-Lari and Reza Kerachian and Mohammad Reza Nikoo and Mahsa Fallahnia},
  keywords = {Wave height forecasting, Lake Superior, Support Vector Machines (SVMs), Bayesian Networks (BNs), Adaptive Neuro-Fuzzy Inference System(ANFIS), Artificial Neural Networks (ANNs)},
  abstract = {Wave Height (WH) is one of the most important factors in design and operation of maritime projects. Different methods such as semi-empirical, numerical and soft computing-based approaches have been developed for WH forecasting. The soft computing-based methods have the ability to approximate nonlinear wind–wave and wave–wave interactions without a prior knowledge about them. In the present study, several soft computing-based models, namely Support Vector Machines (SVMs), Bayesian Networks (BNs), Artificial Neural Networks (ANNs) and Adaptive Neuro-Fuzzy Inference System (ANFIS) are used for mapping wind data to wave height. The data set used for training and testing the simulation models comprises the WH and wind data gathered by National Data Buoy Center (NDBC) in Lake Superior, USA. Several statistical indices are used to evaluate the efficacy of the aforementioned methods. The results show that the ANN, ANFIS and SVM can provide acceptable predictions for wave heights, while the BNs results are unreliable.},
}

@Article{DELEN2018186,
  title = {The analytics paradigm in business research},
  journal = {Journal of Business Research},
  volume = {90},
  pages = {186 - 195},
  year = {2018},
  issn = {0148-2963},
  doi = {https://doi.org/10.1016/j.jbusres.2018.05.013},
  url = {http://www.sciencedirect.com/science/article/pii/S0148296318302480},
  author = {Dursun Delen and Hamed M. Zolbanin},
  keywords = {Business analytics, Causal-explanatory modeling, Predictive modeling, Big data, Business disciplines, Business research},
  abstract = {The availability of data in massive collections in recent past not only has enabled data-driven decision-making, but also has created new questions that cannot be addressed effectively with the traditional statistical analysis methods. The traditional scientific research not only has prevented business scholars from working on emerging problems with big and rich data-sets, but also has resulted in irrelevant theory and questionable conclusions; mostly because the traditional method has mainly focused on modeling and analysis/explanation than on the real/practical problem and the data. We believe the lack of due attention to the analytics paradigm can to some extent be attributed to the business scholars' unfamiliarity with the analytics methods/methodologies and the type of questions it can answer. Therefore, our purpose in this paper is to illustrate how analytics, as a complement, rather than a successor, to the traditional research paradigm, can be used to address interesting emerging business research questions.},
}

@InCollection{ANITHA202029,
  title = {Chapter 3 - Social media data analytics using feature engineering},
  editor = {J. Dinesh Peter and Steven L. Fernandes},
  booktitle = {Systems Simulation and Modeling for Cloud Computing and Big Data Applications},
  publisher = {Academic Press},
  pages = {29 - 59},
  year = {2020},
  series = {Advances in ubiquitous sensing applications for healthcare},
  issn = {25891014},
  doi = {https://doi.org/10.1016/B978-0-12-819779-0.00003-4},
  url = {http://www.sciencedirect.com/science/article/pii/B9780128197790000034},
  author = {J. Anitha and I-Hsien Ting and S. Akila Agnes and S. Immanuel Alex Pandian and R.V. Belfin},
  keywords = {Feature engineering, Representation learning, Big data analytics, Clustering, Social media data},
  abstract = {In this fast-growing digital world, social media analytics is gaining attention in the field of big data. Big data is the collection of huge amounts of raw digital data that is difficult to analyze with conventional analysis methods. Due to the popularity of social media sites such as Twitter and Facebook, a vast amount of public unstructured data is generated by millions of users every day. This raw bulk data cannot be directly used in decision making and prediction tasks. Therefore many researchers have been working on converting this huge unstructured information into meaningful information through big data analytics. Developing an efficient data analytics tool is essential to understand the multimodal social media data that improves the performance of decision-making and prediction systems. This chapter overviews the strengths of the current state-of-the-art feature engineering approaches that have been developed to represent the characteristics of social media data. In addition, this chapter outlines the proposed framework for analyzing the social media data including the shared images, tags, associated comments, and its social relationship.},
}

@Article{DEVRIENDT2019,
  title = {Why you should stop predicting customer churn and start using uplift models},
  journal = {Information Sciences},
  year = {2019},
  issn = {0020-0255},
  doi = {https://doi.org/10.1016/j.ins.2019.12.075},
  url = {http://www.sciencedirect.com/science/article/pii/S0020025519312022},
  author = {Floris Devriendt and Jeroen Berrevoets and Wouter Verbeke},
  keywords = {Prescriptive analytics, Uplift modeling, Customer churn prediction, Customer retention, Maximum profit},
  abstract = {Uplift modeling has received increasing interest in both the business analytics research community and the industry as an improved paradigm for predictive analytics for data-driven operational decision-making. The literature, however, does not provide conclusive empirical evidence that uplift modeling outperforms predictive modeling. Case studies that directly compare both approaches are lacking, and the performance of predictive models and uplift models as reported in various experimental studies cannot be compared indirectly since different evaluation measures are used to assess their performance. Therefore, in this paper, we introduce a novel evaluation metric called the maximum profit uplift (MPU) measure that allows assessing the performance in terms of the maximum potential profit that can be achieved by adopting an uplift model. This measure, developed for evaluating customer churn uplift models, extends the maximum profit measure for evaluating customer churn prediction models. While introducing the MPU measure, we describe the generally applicable liftup curve and liftup measure for evaluating uplift models as counterparts of the lift curve and lift measure that are broadly used to evaluate predictive models. These measures are subsequently applied to assess and compare the performance of customer churn prediction and uplift models in a case study that applies uplift modeling to customer retention in the financial industry. We observe that uplift models outperform predictive models and lead to improved profitability of retention campaigns.},
}

@Article{YAP201113274,
  title = {Using data mining to improve assessment of credit worthiness via credit scoring models},
  journal = {Expert Systems with Applications},
  volume = {38},
  number = {10},
  pages = {13274 - 13283},
  year = {2011},
  issn = {0957-4174},
  doi = {https://doi.org/10.1016/j.eswa.2011.04.147},
  url = {http://www.sciencedirect.com/science/article/pii/S0957417411006749},
  author = {Bee Wah Yap and Seng Huat Ong and Nor Huselina Mohamed Husain},
  keywords = {Data mining, Credit scoring, Logistic regression, Decision tree, Classification, Predictive modeling},
  abstract = {Credit scoring model have been developed by banks and researchers to improve the process of assessing credit worthiness during the credit evaluation process. The objective of credit scoring models is to assign credit risk to either a “good risk” group that is likely to repay financial obligation or a “bad risk” group who has high possibility of defaulting on the financial obligation. Construction of credit scoring models requires data mining techniques. Using historical data on payments, demographic characteristics and statistical techniques, credit scoring models can help identify the important demographic characteristics related to credit risk and provide a score for each customer. This paper illustrates using data mining to improve assessment of credit worthiness using credit scoring models. Due to privacy concerns and unavailability of real financial data from banks this study applies the credit scoring techniques using data of payment history of members from a recreational club. The club has been facing a problem of rising number in defaulters in their monthly club subscription payments. The management would like to have a model which they can deploy to identify potential defaulters. The classification performance of credit scorecard model, logistic regression model and decision tree model were compared. The classification error rates for credit scorecard model, logistic regression and decision tree were 27.9%, 28.8% and 28.1%, respectively. Although no model outperforms the other, scorecards are relatively much easier to deploy in practical applications.},
}

@Article{RUZ20161,
  title = {Improving the performance of inductive learning classifiers through the presentation order of the training patterns},
  journal = {Expert Systems with Applications},
  volume = {58},
  pages = {1 - 9},
  year = {2016},
  issn = {0957-4174},
  doi = {https://doi.org/10.1016/j.eswa.2016.04.003},
  url = {http://www.sciencedirect.com/science/article/pii/S0957417416301610},
  author = {Gonzalo A. Ruz},
  keywords = {Inductive learning, Rules family, Clustering, Classification},
  abstract = {Although the development of new supervised learning algorithms for machine learning techniques are mostly oriented to improve the predictive power or classification accuracy, the capacity to understand how the classification process is carried out is of great interest for many applications in business and industry. Inductive learning algorithms, like the Rules family, induce semantically interpretable classification rules in the form of if-then rules. Although the effectiveness of the Rules family has been studied thoroughly and new and improved versions are constantly been developed, one important drawback is the effect of the presentation order of the training patterns which has not been studied in depth previously. In this paper this issue is addressed, first by studying empirically the effect of random presentation orders in the number of rules and the generalization power of the resulting classifier. Then a presentation order method for the training examples is proposed which combines a clustering stage with a new density measure developed specifically for this problem. The results using benchmark datasets and a real application of wood defect classification show the effectiveness of the proposed method. Also, since the presentation order method is employed as a preprocessing stage, the simplicity of the Rules family is not affected but instead it enables the generation of fewer and more accurate rules, which can have a direct impact in the performance and usefulness of the Rules family in an expert system context.},
}

@Article{LIU2020102363,
  title = {Edge sensing data-imaging conversion scheme of load forecasting in smart grid},
  journal = {Sustainable Cities and Society},
  volume = {62},
  pages = {102363},
  year = {2020},
  issn = {2210-6707},
  doi = {https://doi.org/10.1016/j.scs.2020.102363},
  url = {http://www.sciencedirect.com/science/article/pii/S2210670720305849},
  author = {Xiaozhu Liu and Zhiyang Xiao and Rongbo Zhu and Jun Wang and Lu Liu and Maode Ma},
  keywords = {Empirical mode decomposition, Data-image conversion, Load forecasting, Smart grid, Edge intelligence},
  abstract = {Edge sensing data in smart grid provides vast valuable information, which promotes further innovated smart power applications in Internet of things (IoT) oriented smart cities and society. While in power load prediction, the potential relationships between the time series of power load data and the characteristics of temperature, weather and date, have not been explored comprehensively, which degrades the accuracy of load prediction in smart grid. In order to extract the generalized features and latent relationships in power load related edge sensing data, a power load prediction scheme based on edge sensing data-imaging conversion (DIC) is proposed to improve the forecasting accuracy in smart cites and society. DIC employs empirical mode decomposition (EMD) for power load time series data and combines it with characteristic time series including temperature, weather and date to form an image-like structure. And a DIC-based convolutional neural network (DI-CNN) is presented to implement convolution. Experimental results show that, compared with long short-term memory (LSTM), support vector machines (SVM), and CNN, the proposed DIC scheme improves the training speed by 61.7 %, reduces root mean square error (RMSE) by 32.9 % at least, and enhances the prediction accuracy by 1.4 %.},
}

@Article{BERNABEMORENO2019865,
  title = {A fuzzy linguistic supported framework to increase Artificial Intelligence intelligibility for subject matter experts},
  journal = {Procedia Computer Science},
  volume = {162},
  pages = {865 - 872},
  year = {2019},
  note = {7th International Conference on Information Technology and Quantitative Management (ITQM 2019): Information technology and quantitative management based on Artificial Intelligence},
  issn = {1877-0509},
  doi = {https://doi.org/10.1016/j.procs.2019.12.061},
  url = {http://www.sciencedirect.com/science/article/pii/S1877050919320721},
  author = {Juan Bernabé-Moreno and Karsten Wildberger},
  keywords = {fuzzy linguistic modeling, expert knowledge modelling, decision making, intelligible AI},
  abstract = {The application of artificial intelligence (AI) techniques in the decision making processes is more widespread in the industry than ever before. Yet, one of the most critical show-stoppers is the communication gap between the machine learning (ML) models and the experts community. On one hand, the output of ML is often not intelligible for experts, in spite of the latest advances in explainable AI. On the other hand, the expert knowledge, rarely completely present in the available data, but rather in the heads of the experts, needs to be connected to the data-driven insights created by the ML model. In this paper we first identify the most critical situations with a manifest intelligibility gap and then propose a framework supported by fuzzy linguistic modelling techniques to close this gap. In addition, we present its integration into the end-to-end decision making flow, from data gathering to the execution and evaluation and we show the output of our approach with practical examples.},
}

@Article{KAUFFMAN2017115,
  title = {Combining machine-based and econometrics methods for policy analytics insights},
  journal = {Electronic Commerce Research and Applications},
  volume = {25},
  pages = {115 - 140},
  year = {2017},
  issn = {1567-4223},
  doi = {https://doi.org/10.1016/j.elerap.2017.04.004},
  url = {http://www.sciencedirect.com/science/article/pii/S1567422317300145},
  author = {Robert J. Kauffman and Kwansoo Kim and Sang-Yong Tom Lee and Ai-Phuong Hoang and Jing Ren},
  keywords = {Causality, Computational Social Science, Data analytics, Econometrics, E-commerce, Empirical research, Fintech, Fusion analytics, Music popularity, Stock trading, Policy analytics, TV viewing, Video-on-demand (VoD)},
  abstract = {Computational Social Science (CSS) has become a mainstream approach in the empirical study of policy analytics issues in various domains of e-commerce research. This article is intended to represent recent advances that have been made for the discovery of new policy-related insights in business, consumer and social settings. The approach discussed is fusion analytics, which combines machine-based methods from Computer Science (CS) and explanatory empiricism involving advanced Econometrics and Statistics. It explores several efforts to conduct research inquiry in different functional areas of Electronic Commerce and Information Systems (IS), with applications that represent different functional areas of business, as well as individual consumer, social and public issues. Recent developments and shifts in the scientific study of technology-related phenomena and Social Science issues in the presence of historically-large datasets prompt new forms of research inquiry. They include blended approaches to research methodology, and more interest in the production of research results that have direct application to industry contexts. This article showcases the methods shifts and several contemporary applications. They discuss: (1) feedback effects in mobile phone-based stock trading; (2) sustainability of top-rank chart popularity of music tracks; (3) household TV viewing patterns; and (4) household sampling and purchases of video-on-demand (VoD) services. The range of applicability of the ideas goes beyond the scope of these illustrations, to include issues in public services, healthcare, product and service deployment, public opinion and elections, electronic auctions, and travel and tourism services. In fact, the coverage is as broad as for-profit and for-non-profit, private and public, and governmental and non-governmental institutions.},
}

@Article{CASABAYO20151637,
  title = {Improved market segmentation by fuzzifying crisp clusters: A case study of the energy market in Spain},
  journal = {Expert Systems with Applications},
  volume = {42},
  number = {3},
  pages = {1637 - 1643},
  year = {2015},
  issn = {0957-4174},
  doi = {https://doi.org/10.1016/j.eswa.2014.09.044},
  url = {http://www.sciencedirect.com/science/article/pii/S0957417414005971},
  author = {Mònica Casabayó and Núria Agell and Germán Sánchez-Hernández},
  keywords = {Customer segmentation, Fuzzy operators, Learning systems, Business case, Fuzzy segmentation},
  abstract = {This paper provides an innovative segmentation approach stemming from the combination of cluster analyses and fuzzy learning techniques. Our research provides a real case solution in the Spanish energy market to respond to the increasing number of requests from industry managers to be able to interpret ambiguous market information as realistically as possible. The learning stage is based on the segments created from a non-hierarchical cluster analysis. This results in fuzzy segmentation which permits patterns to be assigned to more than one segment. This in turn reveals that “fuzzifying” an excluding attitudinal segmentation offers more interpretable and acceptable results for managers. Our results demonstrate that 30% of the individuals show plural patterns of behaviour because they have a significant degree of adequacy to more than one segment. In such a rational market, this fact enables sales forces to develop more precise approaches to capture new customers and/or retain existing ones.},
}

@Article{SAHA20191,
  title = {Integrated Rough Fuzzy Clustering for Categorical data Analysis},
  journal = {Fuzzy Sets and Systems},
  volume = {361},
  pages = {1 - 32},
  year = {2019},
  note = {Theme: Clustering and Rule-based Models},
  issn = {0165-0114},
  doi = {https://doi.org/10.1016/j.fss.2018.02.007},
  url = {http://www.sciencedirect.com/science/article/pii/S0165011418300629},
  author = {Indrajit Saha and Jnanendra Prasad Sarkar and Ujjwal Maulik},
  keywords = {Categorical data, Cluster validity indices, Rough Fuzzy Clustering, Simulated Annealing, Genetic Algorithm, Random Forest, Sensitivity analysis, Statistical test},
  abstract = {In recent times, advanced data mining research has been mostly focusing on clustering of categorical data, where a natural ordering in attribute values is missing. To address this fact the Rough Fuzzy K-Modes clustering technique has been recently developed in order to handle imperfect information, i.e. indiscernibility (coarseness) and vagueness within the dataset. However, it has been observed that the said technique suffers from the problem of local optima due to the random choice of initial cluster modes. Hence, in this paper, we have proposed an integrated clustering technique using multi-phase learning. In this regard, first, Simulated Annealing based Rough Fuzzy K-Modes and Genetic Algorithm based Rough Fuzzy K-Modes are proposed in order to perform the clustering better by considering clustering as an underlying optimization problem. These clustering methods individually produce clusters having set of central and peripheral points. Thereafter, for each case, final improved clustering results are obtained by assigning peripheral points to a particular crisp cluster using Random Forest, where central points are used as training set. Second, the varying cardinality of the training and testing sets produced by each clustering method further motivated us to propose a generalized technique called Integrated Rough Fuzzy Clustering using Random Forest, where, results of three aforementioned clustering techniques are used to compute the roughness measure. Based on this measure, three different sets namely best central points, semi-best central points and pure peripheral points are determined. Thereafter, using multi-phase learning, best central points are used to classify the semi-best central points and then using both of them, pure peripheral points are classified by Random Forest. Experimental results are reported quantitatively and visually to demonstrate the effectiveness of the proposed methods in comparison with well-known state-of-the-art methods for six synthetic and five real-life datasets. Finally, statistical significance tests are conducted to establish the superiority of the results produced by the proposed methods.},
}

@Article{SCHAEFFER2020101918,
  title = {Forecasting client retention — A machine-learning approach},
  journal = {Journal of Retailing and Consumer Services},
  volume = {52},
  pages = {101918},
  year = {2020},
  issn = {0969-6989},
  doi = {https://doi.org/10.1016/j.jretconser.2019.101918},
  url = {http://www.sciencedirect.com/science/article/pii/S0969698919302668},
  author = {Satu Elisa Schaeffer and Sara Veronica {Rodriguez Sanchez}},
  keywords = {Client retention, Sales forecasting, Machine learning, Prepaid unitary services},
  abstract = {In the age of big data, companies store practically all data on any client transaction. Making use of this data is commonly done with machine-learning techniques so as to turn it into information that can be used to drive business decisions. Our interest lies in using data on prepaid unitary services in a business-to-business setting to forecast client retention: whether a particular client is at risk of being lost before they cease being clients. The purpose of such a forecast is to provide the company with an opportunity to reach out to such clients as an effort to ensure their retention. We work with monthly records of client transactions: each client is represented as a series of purchases and consumptions. We vary (1) the length of the time period used to make the forecast, (2) the length of a period of inactivity after which a client is assumed to be lost, and (3) how far in advance the forecast is made. Our experimental work finds that current machine-learning techniques able to adequately predict, well in advance, which clients will be lost. This knowledge permits a company to focus marketing efforts on such clients as early as three months in advance.},
}

@Article{ALSHAER2019792,
  title = {IBRIDIA: A hybrid solution for processing big logistics data},
  journal = {Future Generation Computer Systems},
  volume = {97},
  pages = {792 - 804},
  year = {2019},
  issn = {0167-739X},
  doi = {https://doi.org/10.1016/j.future.2019.02.044},
  url = {http://www.sciencedirect.com/science/article/pii/S0167739X1830606X},
  author = {Mohammed AlShaer and Yehia Taher and Rafiqul Haque and Mohand-Saïd Hacid and Mohamed Dbouk},
  keywords = {Realtime processing, Clustering, Big data, Internet of Things, Logistics, Hierarchical clustering algorithm},
  abstract = {Internet of Things (IoT) is leading to a paradigm shift within the logistics industry. Logistics services providers use sensor technologies such as GPS or telemetry to track and manage their shipment processes. Additionally, they use external data that contain critical information about events such as traffic, accidents, and natural disasters. Correlating data from different sensors and social media and performing analysis in real-time provide opportunities to predict events and prevent unexpected delivery delay at run-time. However, collecting and processing data from heterogeneous sources foster problems due to the variety and velocity of data. In addition, processing data in real-time is heavily challenging that it cannot be dealt with using conventional logistics information systems. In this paper, we present a hybrid framework for processing massive volume of data in batch style and real-time. Our framework is built upon Johnson’s hierarchical clustering (HCL) algorithm which produces a dendrogram that represents different clusters of data objects.},
}

@Article{GAO2020,
  title = {Big data analytics for smart factories of the future},
  journal = {CIRP Annals},
  year = {2020},
  issn = {0007-8506},
  doi = {https://doi.org/10.1016/j.cirp.2020.05.002},
  url = {http://www.sciencedirect.com/science/article/pii/S0007850620301359},
  author = {Robert X. Gao and Lihui Wang and Moneer Helu and Roberto Teti},
  keywords = {Digital manufacturing system, Information, Learning},
  abstract = {Continued advancement of sensors has led to an ever-increasing amount of data of various physical nature to be acquired from production lines. As rich information relevant to the machines and processes are embedded within these “big data”, how to effectively and efficiently discover patterns in the big data to enhance productivity and economy has become both a challenge and an opportunity. This paper discusses essential elements of and promising solutions enabled by data science that are critical to processing data of high volume, velocity, variety, and low veracity, towards the creation of added-value in smart factories of the future.},
}

@Article{CHEN2013123,
  title = {Dynamic customer lifetime value prediction using longitudinal data: An improved multiple kernel SVR approach},
  journal = {Knowledge-Based Systems},
  volume = {43},
  pages = {123 - 134},
  year = {2013},
  issn = {0950-7051},
  doi = {https://doi.org/10.1016/j.knosys.2013.01.022},
  url = {http://www.sciencedirect.com/science/article/pii/S0950705113000348},
  author = {Zhen-Yu Chen and Zhi-Ping Fan},
  keywords = {Data mining, Customer relationship management, Customer lifetime value, Dynamic multi-step-ahead prediction, Support vector machine, Multiple kernel learning},
  abstract = {Customer lifetime value (CLV), as an important metric in customer relationship management (CRM), has attracted widespread attention over the last decade. Most CLV prediction models do not take into consideration the dynamics of the customer purchase behavior and changes of the marketing environment such as the adoption of different promotion policies. In this study, a framework for the dynamic CLV prediction using longitudinal data is presented. In the framework, both the dynamic customer purchase behavior and customized promotions are considered. An improved multiple kernel support vector regression (MK-SVR) approach is developed to predict the future CLV and select the best promotion using both the customer behavioral variables and controlled variable about multiple promotions. Computational experiments using two databases show that the MK-SVR exhibits good prediction performance and the usage of longitudinal data in the MK-SVR facilitate the dynamic prediction and promotion optimization.},
}

@Article{TANAKA2017956,
  title = {Classifying and Understanding Prospective Customers via Heterogeneity of Supermarket Stores},
  journal = {Procedia Computer Science},
  volume = {112},
  pages = {956 - 964},
  year = {2017},
  note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 21st International Conference, KES-20176-8 September 2017, Marseille, France},
  issn = {1877-0509},
  doi = {https://doi.org/10.1016/j.procs.2017.08.133},
  url = {http://www.sciencedirect.com/science/article/pii/S1877050917314904},
  author = {Takamasa Tanaka and Tomohiro Hamaguchi and Takumi Saigo and Kazuhiko Tsuda},
  keywords = {RFM, Customer Relationship Management, Logistic Regression},
  abstract = {In recent years, the supermarket industry in Japan is in a state of declining sales over the long term, and market contraction is expected to continue due to environmental changes such as demographic changes. In this research, as a support for supermarket managers placed under such circumstances we suggest a new method to classify good customers who should be kept top priority. We define good customers on the basis of not only current good customers but also customers who generate most of sales in the future and classify good customers in advance from the current information. Additionally We provide goods information for outstanding purchase by good customers to supermarkets. It can be expected to contribute to the improvement of management efficiency by utilizing it for sales promotion activities.},
}

@Article{HUNG2006515,
  title = {Applying data mining to telecom churn management},
  journal = {Expert Systems with Applications},
  volume = {31},
  number = {3},
  pages = {515 - 524},
  year = {2006},
  issn = {0957-4174},
  doi = {https://doi.org/10.1016/j.eswa.2005.09.080},
  url = {http://www.sciencedirect.com/science/article/pii/S0957417405002654},
  author = {Shin-Yuan Hung and David C. Yen and Hsiu-Yu Wang},
  keywords = {Churn management, Wireless telecommunication, Data mining, Decision tree, Neural network},
  abstract = {Taiwan deregulated its wireless telecommunication services in 1997. Fierce competition followed, and churn management becomes a major focus of mobile operators to retain subscribers via satisfying their needs under resource constraints. One of the challenges is churner prediction. Through empirical evaluation, this study compares various data mining techniques that can assign a ‘propensity-to-churn’ score periodically to each subscriber of a mobile operator. The results indicate that both decision tree and neural network techniques can deliver accurate churn prediction models by using customer demographics, billing information, contract/service status, call detail records, and service change log.},
}

@Article{FARVARESH2011182,
  title = {A data mining framework for detecting subscription fraud in telecommunication},
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {24},
  number = {1},
  pages = {182 - 194},
  year = {2011},
  issn = {0952-1976},
  doi = {https://doi.org/10.1016/j.engappai.2010.05.009},
  url = {http://www.sciencedirect.com/science/article/pii/S0952197610001144},
  author = {Hamid Farvaresh and Mohammad Mehdi Sepehri},
  keywords = {Fraud detection, Data mining, Neural networks, Decision tree, Support vector machines, Ensembles, Telecommunication.},
  abstract = {Service providing companies including telecommunication companies often receive substantial damage from customers’ fraudulent behaviors. One of the common types of fraud is subscription fraud in which usage type is in contradiction with subscription type. This study aimed at identifying customers’ subscription fraud by employing data mining techniques and adopting knowledge discovery process. To this end, a hybrid approach consisting of preprocessing, clustering, and classification phases was applied, and appropriate tools were employed commensurate to each phase. Specifically, in the clustering phase SOM and K-means were combined, and in the classification phase decision tree (C4.5), neural networks, and support vector machines as single classifiers and bagging, boosting, stacking, majority and consensus voting as ensembles were examined. In addition to using clustering to identify outlier cases, it was also possible – by defining new features – to maintain the results of clustering phase for the classification phase. This, in turn, contributed to better classification results. A real dataset provided by Telecommunication Company of Tehran was applied to demonstrate the effectiveness of the proposed method. The efficient use of synergy among these techniques significantly increased prediction accuracy. The performance of all single and ensemble classifiers is evaluated based on various metrics and compared by statistical tests. The results showed that support vector machines among single classifiers and boosted trees among all classifiers have the best performance in terms of various metrics. The research findings show that the proposed model has a high accuracy, and the resulting outcomes are significant both theoretically and practically.},
}

@Article{KLEANTHOUS2020105048,
  title = {Gated Mixture Variational Autoencoders for Value Added Tax audit case selection},
  journal = {Knowledge-Based Systems},
  volume = {188},
  pages = {105048},
  year = {2020},
  issn = {0950-7051},
  doi = {https://doi.org/10.1016/j.knosys.2019.105048},
  url = {http://www.sciencedirect.com/science/article/pii/S0950705119304435},
  author = {Christos Kleanthous and Sotirios Chatzis},
  keywords = {Value Added Tax, Audit selection, Variational autoencoder, Finite mixture model},
  abstract = {In this work, we address the problem of targeted Value Added Tax (VAT) audit case selection by means of machine learning. This is a challenging problem that has remained rather elusive for EU-based Tax Departments, due to the inadequate quantity of tax audits that can be used for conventional supervised model training. To this end, we devise a novel Gated Mixture Variational Autoencoder deep network, that can be effectively trained with data from a limited number of audited taxpayers, combined with a large corpus of filed VAT returns. This gives rise to a semi-supervised learning framework that leverages the latest advances in deep learning and robust regularization using variational inference. We developed our approach in collaboration with the Cyprus Tax Department and experimentally deployed it to facilitate its audit selection process; to this end, we used actual VAT data from Cyprus-based taxpayers. This way, we obtained strong empirical evidence that our approach can greatly facilitate the VAT audit case selection process. Specifically, we obtained up to 76% out-of-sample accuracy in detecting whether a significant tax yield will be generated from a specific prospective VAT audit.},
}

@Article{CHEN2015150,
  title = {Predicting microblog users’ lifetime activities – A user-based analysis},
  journal = {Electronic Commerce Research and Applications},
  volume = {14},
  number = {3},
  pages = {150 - 168},
  year = {2015},
  note = {New Research on E-Commerce from the Asia Region},
  issn = {1567-4223},
  doi = {https://doi.org/10.1016/j.elerap.2014.06.001},
  url = {http://www.sciencedirect.com/science/article/pii/S1567422314000295},
  author = {Xi Chen and Ruibin Geng and Shun Cai},
  keywords = {Social network, Microblogging, User lifetime vitality, Pareto/NBD model, BG/NBD model},
  abstract = {With the rapid development of online social media, social networking services have become an important research area in recent years. In particular, microblogging as a new social media platform draws much attention from both researchers and practitioners. Although most current studies focus on the effect of social networks on the diffusion of services or information, most are descriptions or explanations of what has already happened. This study focuses on future activity by employing probability models such as the Pareto/NBD and BG/NBD models to predict user lifetime vitality. Three experiments were implemented to test the two models. Our results showed that both the Pareto/NBD model and the BG/NBD model were effective in predicting SNS user usage behavior on microblogging websites. It was found that tweeting behavior is more suitable for such probability models than retweeting behavior and user segmentation can improve prediction accuracy by distinguishing between currently active and inactive users.},
}

@Article{KHAJVAND201157,
  title = {Estimating customer lifetime value based on RFM analysis of customer purchase behavior: Case study},
  journal = {Procedia Computer Science},
  volume = {3},
  pages = {57 - 63},
  year = {2011},
  note = {World Conference on Information Technology},
  issn = {1877-0509},
  doi = {https://doi.org/10.1016/j.procs.2010.12.011},
  url = {http://www.sciencedirect.com/science/article/pii/S1877050910003868},
  author = {Mahboubeh Khajvand and Kiyana Zolfaghar and Sarah Ashoori and Somayeh Alizadeh},
  keywords = {Customer relationship management, Customer lifetime value, Data mining, RFM analysis, Customer segmentation},
  abstract = {Since the increased importance is placed on customer equity in today’s business environment, many firms are focusing on the notion of customer loyalty and profitability to increasing market share. Building successful customer relationship management (CRM), a firm starts from identifying customers’ true value and loyalty since customer value can provide basic information to deploy more targeted and personalized marketing. In this paper, customer lifetime value (CLV) is used to customer segmentation of a health and beauty company. Two approaches are used: in the first approach, RFM (Recency, Frequency, and Monetary) marketing analysis method is used in order to segmentation of customers and in the second approach, the proposed extended RFM analysis method with one additional parameter—called Count Item—is used. Comparing results of these approaches, shows that adding count Item as a new parameter to RFM method makes no difference to clustering result, so CLV is calculated based on weighted RFM method for each segment. The results of calculated CLV for different segments can be used to explain marketing and sales strategies by the company.},
}

@Article{RAVI201514,
  title = {A survey on opinion mining and sentiment analysis: Tasks, approaches and applications},
  journal = {Knowledge-Based Systems},
  volume = {89},
  pages = {14 - 46},
  year = {2015},
  issn = {0950-7051},
  doi = {https://doi.org/10.1016/j.knosys.2015.06.015},
  url = {http://www.sciencedirect.com/science/article/pii/S0950705115002336},
  author = {Kumar Ravi and Vadlamani Ravi},
  keywords = {Opinion mining, Sentiment analysis, Social media, Micro blog, Lexica creation, Machine learning, Ontology},
  abstract = {With the advent of Web 2.0, people became more eager to express and share their opinions on web regarding day-to-day activities and global issues as well. Evolution of social media has also contributed immensely to these activities, thereby providing us a transparent platform to share views across the world. These electronic Word of Mouth (eWOM) statements expressed on the web are much prevalent in business and service industry to enable customer to share his/her point of view. In the last one and half decades, research communities, academia, public and service industries are working rigorously on sentiment analysis, also known as, opinion mining, to extract and analyze public mood and views. In this regard, this paper presents a rigorous survey on sentiment analysis, which portrays views presented by over one hundred articles published in the last decade regarding necessary tasks, approaches, and applications of sentiment analysis. Several sub-tasks need to be performed for sentiment analysis which in turn can be accomplished using various approaches and techniques. This survey covering published literature during 2002–2015, is organized on the basis of sub-tasks to be performed, machine learning and natural language processing techniques used and applications of sentiment analysis. The paper also presents open issues and along with a summary table of a hundred and sixty-one articles.},
}

@Article{BERTSCH2020105822,
  title = {Bank misconduct and online lending},
  journal = {Journal of Banking & Finance},
  volume = {116},
  pages = {105822},
  year = {2020},
  issn = {0378-4266},
  doi = {https://doi.org/10.1016/j.jbankfin.2020.105822},
  url = {http://www.sciencedirect.com/science/article/pii/S0378426620300893},
  author = {Christoph Bertsch and Isaiah Hull and Yingjie Qi and Xin Zhang},
  keywords = {Financial development, Consumer loans, Bank misconduct, FinTech},
  abstract = {We introduce a high quality proxy for bank misconduct that is constructed from Consumer Financial Protection Bureau (CFPB) complaint data. We employ this proxy to measure the impact of bank misconduct on the expansion of online lending in the United States. Using nearly complete loan and application data from the online lending market, we demonstrate that bank misconduct is associated with a statistically and economically significant increase in online lending demand at the state and county levels. This result is robust to the inclusion of bank credit supply shocks and holds for both broader and more narrowly-defined bank misconduct‘ measures. Furthermore, we show that this effect is strongest for lower rated borrowers and weakest in states with high levels of generalized trust.},
}

@Article{DIERKES2011361,
  title = {Estimating the effect of word of mouth on churn and cross-buying in the mobile phone market with Markov logic networks},
  journal = {Decision Support Systems},
  volume = {51},
  number = {3},
  pages = {361 - 371},
  year = {2011},
  issn = {0167-9236},
  doi = {https://doi.org/10.1016/j.dss.2011.01.002},
  url = {http://www.sciencedirect.com/science/article/pii/S0167923611000273},
  author = {Torsten Dierkes and Martin Bichler and Ramayya Krishnan},
  keywords = {Data mining, Marketing, Telecommunications, Social network analysis},
  abstract = {Much has been written about word of mouth and customer behavior. Telephone call detail records provide a novel way to understand the strength of the relationship between individuals. In this paper, we predict using call detail records the impact that the behavior of one customer has on another customer's decisions. We study this in the context of churn (a decision to leave a communication service provider) and cross-buying decisions based on an anonymized data set from a telecommunications provider. Call detail records are represented as a weighted graph and a novel statistical learning technique, Markov logic networks, is used in conjunction with logit models based on lagged neighborhood variables to develop the predictive model. In addition, we propose an approach to propositionalization tailored to predictive modeling with social network data. The results show that information on the churn of network neighbors has a significant positive impact on the predictive accuracy and in particular the sensitivity of churn models. The results provide evidence that word of mouth has a considerable impact on customers' churn decisions and also on the purchase decisions, leading to a 19.5% and 8.4% increase in sensitivity of predictive models.},
}

@Article{BUAH2020116210,
  title = {Emotional responses to energy projects: A new method for modeling and prediction beyond self-reported emotion measure},
  journal = {Energy},
  volume = {190},
  pages = {116210},
  year = {2020},
  issn = {0360-5442},
  doi = {https://doi.org/10.1016/j.energy.2019.116210},
  url = {http://www.sciencedirect.com/science/article/pii/S036054421931905X},
  author = {Eric Buah and Lassi Linnanen and Huapeng Wu},
  keywords = {Artificial intelligence, CO2 capture and stoarge, Deep neural network algorithm, Environmental social science, Fuzzy logic, Fuzzy deep learning},
  abstract = {A considerable amount of studies report that negative emotions evoked by Wind Energy, Nuclear Energy and CO2 Capture and Storage (CCS) can lead to cancellation of the energy project or a delay in policy decisions for its implementation if not adequately addressed. Earlier studies have attempted to study this problem using self-reported emotion measurements to identify the emotions the participants felt. As an alternative, we propose the use of an emotional artificial intelligence (AI) algorithm for improved modeling and prediction of the participants’ emotional behaviour to guide decision-making. We have validated the system using emotional responses to a hypothetical CCS project as a case study. Running our simulation on the experimental dataset (thus 40% of the 72,105), we obtained an average validation accuracy of 98.81%. We challenged the algorithm further with 84 test samples (unseen cases), and it predicted 75 feelings correctly when the stakeholders took a definite position on how they felt. Although there are few limitations to this study, we did find, in a sensitivity experiment, that it was challenging for the algorithm to predict indecisive feelings. The method is adaptable to study emotional responses to other projects, including Wind Energy, Nuclear Energy and Hydrogen Technology.},
}
