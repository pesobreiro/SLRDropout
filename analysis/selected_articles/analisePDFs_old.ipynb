{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDFs analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho='C:\\nuvem\\Dropbox\\doutoramento\\tese\\SLRDropout\\analysis\\selected_articles\\final_dataset\\files\\11391\\Hung et al_2006_Applying data mining to telecom churn management.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf=pdfplumber.open(\"final_dataset/files/11391/Hung et al_2006_Applying data mining to telecom churn management.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf.pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' To segment customers by loyalty, contribution, and usage, we selected Bill Amount, Tenure, MOU, MTU, and Payment Rate as variables . We used K-Means to model the customers into 5 clusters . To generate roughly the same number of subscribers in each of the 5 clusters, we divided the customers equally into three segments .'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarization(pdf.pages[8].extract_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Page:1>  This material is brought to you by the Pacific Asia Conference on Information Systems (PACIS) at AIS Electronic Library (AISeL) It has been accepted for inclusion in PACIS 2004 Proceedings by an authorized administrator of AIS . For more information, please contact contact contactelibrary@aisnet.org.au .\n",
      "<Page:2>  Taiwan deregulated its wireless telecommunication services in 1997 . Churn management becomes a major focus of mobile operators to retain customers . Figure 1 suggests that Asian telecom providers face a more challenging challenge than those in the other parts of the world . Study compares various data mining techniques that can assign a “propensity-to-churn” score periodically to each and every  subscriber of a mobile operator .\n",
      "<Page:3>  This paper shares the result of the research. Section 2 defines some basic concepts (and rationale) that we use in the research . Section 3 describes our research methodology, and Section 4 presents the findings. Section 5 concludes our presentation. Section 1 defines some of the basic concepts and the findings .\n",
      "<Page:4>  Data mining techniques most commonly used include clustering, associations, rule induction, genetic algorithm, decision tree, and neural network . Table 1 summarizes some data mining functionalities, techniques, and applications in CRM space . Churn Prediction Data Mining Assessment Methodology assesses performance of various data mining techniques .\n",
      "<Page:5>  Research selected Decision Tree, Neural Network and K-means cluster as data mining techniques to build predictive models or segment customers . Churner is defined as a subscriber  who is voluntary to leave; non-churner  is the subscriber who is still using this operator’s service . We used latest six months’ transactions of each subscriber to  predict customers’ churn probability of the following month .\n",
      "<Page:6>  Models were built by Decision Tree (C5.0) and Back .Propagation Neural Network (BPN) techniques . Data Preprocessing, Variable Analysis and Selection, and Data Extraction, is a data quality integration process to ensure data quality and code optimization . We took two approaches to assess how models built using Decision Tree and Back.\n",
      "<Page:7>  In Approach 1, we used K-means clustering methods to segment customers into 5 clusters . Then we create a Decision Tree model in each cluster (see Approach 1 in Figure 3) This is to assess if the churn behaviors are different in different “Value-Loyalty’s” segments . LIFT is a measure of productivity with modeling .\n",
      "<Page:8>  A wireless telecom company in Taiwan provides their customer related data . The data source includes data of about 160,000 subscribers, including 14,000 churners, from July 2001 to June 2002 . We got possible variables from other researches and telecom experts’ interviews . We then analyzed these variables with z-test from four dimensions and listed significant variables .\n",
      "<Page:9>  To segment customers by loyalty, contribution, and usage, we selected Bill Amount, Tenure, MOU, MTU, and Payment Rate as variables . We used K-Means to model the customers into 5 clusters . To generate roughly the same number of subscribers in each of the 5 clusters, we divided the customers equally into three segments .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1530 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Page:10> erro\n",
      "<Page:11>  We use the same training set for BPN as for Decision Tree . Table 6 shows the results, in which model  N18-R6, for example, uses 18 neurons in the hidden layer with 0.6 learning rate . Figure 5 shows that all the models demonstrate stable accuracy in the first 6 months . But there a significant degradation occurs in the month of February 2002 .\n",
      "<Page:12>  Table 7 lists T-test results: The performance of decision tree model without segmentation is better than that with segmentation . Table 7 shows that the performance of BPN is better on both the hit ratio and capture rate of the neural network model and the capture rate .\n",
      "<Page:13>  The mobile service provider only budgeted this study at the population of about 160,000 customers, and the associated monthly churn rate was only 0.71% . The data size was not sufficient to build a good predictive model by each customer segment because we could not explore real significant information from only few churners .\n",
      "<Page:14>  Churn prediction and management is critical in liberalized mobile telecom markets . Mobile service providers must be able to predict possible churners and take proactive actions to retain valuable customers . Data mining techniques can be applied in many fields in CRM space, such as credit card fraud detection and credit score .\n",
      "<Page:15>  Thearling, Kurt “A Introduction of Data Mining”, Direct Marketing Magazine, Feb 1999 . Setiono, Rudy, Liu, Huan “Neural-Network feature selector’, IEEE transaction on neural . network, Vol. 8(3), 1997, pp654-661 .\n"
     ]
    }
   ],
   "source": [
    "all_text = ''\n",
    "for page in pdf.pages:\n",
    "    page_text = page.extract_text()\n",
    "    try:\n",
    "        resumo=summarization(page_text)[0]['summary_text']\n",
    "        print(page,resumo)\n",
    "    except:\n",
    "        print(page,'erro')\n",
    "    #print(page_text)\n",
    "    all_text = all_text + '\\n' + resumo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf=pdfplumber.open(\"final_dataset/files/11393/Dierkes et al_2011_Estimating the effect of word of mouth on churn and cross-buying in the mobile.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf.pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Page:1>  The results provide evidence that word of mouth has a considerable impact on customers’ churn decisions and also on the purchase decisions, leading to a 19.5% and 8.4% increase in sensitivity of pre-dictive models . The results show that information on the churn of network neigh-bors has a significant positive impact on the predictive accuracy and in particular the sensitivity of churn models .\n",
      "<Page:2>  The churn rate refers to the proportion of contractual customers or subscribers who leave a service pro-uablyvider during a given time period . It is a possible indicator of customer dissatisfaction, cheaper and/or bet-ter offers from the competition, or reasons related to the customer life cycle . The accuracy of churn prediction models matters and that just using one method rather than another  can easily amount to changes in profit in the hundreds of thousands of dollars .\n",
      "<Page:3>  We use customers’ anonymized call detail records as a way of modeling WOM . In contrast to traditional classification methods, we take into account the infor-heticalmation about who a customer calls – i.e., a customer’s neighbors in the communication graph derived  from the call detail record data . We interpret these graphs as social networks, which can be stored in a relational data model .\n",
      "<Page:4>  Statistical relational learning is a relatively young field and there is still only limited empirical evidence on the performance of respective learners . There are two fundamentally different approaches to analyzing multi-relational data . Markov logic networks (MLNs) have recently been suggested as a significant step forward in this field .\n",
      "<Page:5>  The prediction of customers’ churn or buying decisions is important to marketers . In this paper, we analyze whether WOM has an impact on customer behavior or not . The analysis is based on anonymized calling data from a telecom provider . We found the churn behavior of a customer’s neighbor has a significant positive impact on predictive accuracy of churn models .\n",
      "<Page:6>  Churn management is to determine the reasons for churn and to predict the potential churners . In the following section we discuss churn and the related literature on churn and WOM literature . There are different strands of literature that are relevant to this paper . There is also a relation between customer tenure and the tendency of customers to engage in word of mouth .\n",
      "<Page:7>  A huge body of literature has emerged on the analysis of social networks . We focus on post-paid customers, which allows us to leverage information about potential churners . The authors study the evolution of churners in an operator’s network of pre-paid subscribers and the pro-verselypensity of a subscriber to churn out of a service provider's network depending on the number of ties .\n",
      "<Page:8>  Statistical Relational Learning (SRL) has been an emerging research topic in the data mining community in recent years . We understand propositionalization as a transformation of multi-relational learning prob-lems into attribute-value representations . We discuss this approach in more detail in the next section .\n",
      "<Page:9>  ILP has traditionally dealt with multi-relational data . ILP tools can be applied directly to multi-referred data to find first-order rules from relational data . Markov logic networks (MLNs) have become very popular in statistical relational learning recently . In our analysis, we use Alchemy (http://alchemy.cs.washington.edu/), an open source software tool for learning MLNs from data .\n",
      "<Page:10>  There are two main approaches to propositionalization in the literature, logic-oriented and database-oriented literature . Business databases present different challenges than those found in the classical showcase areas of ILP and logic-based propositionalizations . Relational databases are usually structurally simpler .\n",
      "<Page:11>  A churner is defined as a customer who gives notice about their intent to cancel the contract and does not re-evaluate his decision by extending his contract at some point afterwards . A non-churner (or negative) is a  customer who does not give notice at any time . In our data, 6,800 customers told the phone provider that they wanted to cancel their contract (notification) Roughly 1,000 revoked their decision afterwards by extending their contracts .\n",
      "<Page:12>  Figure 1 (a) and (b) shows a visualization of connections be-tween positives and customers with a game download in the test data set . The graphs could suggest that game downloading is contagious because there are many connections between persons downloading                 games, while this is less so in the case of churn customers .\n",
      "<Page:13>  We split the set of 2,645 customers in training and test datasets such that it was stratified with re-naissancespect to the number of positives and edge counts . Training and test data contained 1325 and 1320 custom-ishlyers, respectively . Positives were assigned according to their notification date; customers who notified before July 1, 2008 were assigned to the training data .\n",
      "<Page:14>  Among the selected 3,000 customers there were 7,950 edges – 1,865 among positives, 2,127 among negatives, and 3,950 between both groups . From the 70 available customer attributes we selected the 32 best ones based on information gain with respect to the target variable .\n",
      "<Page:15>  MLN, classification is the problem of inferring the truth value of C(x,v) for all x and v of interest . In relational learning problems, dependencies between objects can be represented by relational predicates . In our example, churn of customer x would be considered independent of the churn of other custom-crafteders . We model an influence relationship between connected customers’ urchurn behavior saying that customer x is likely to churn if customer x did already .\n",
      "<Page:16>  A fundamental problem with database-oriented propositionalization has been referred to as degree disparity [25]. It describes the systematic variation in the distribution of the degree                 with respect to the target variable . A customer with a large number of neighbors would also have more churn neighbors than a customer with just a few neighbors . The difference among MLN settings is again the way in which connec-propriitionalization approaches are different .\n",
      "<Page:17>  The resulting models for the logistic regression (benchmark) as well as for all propositionalization settings showed that three groups of customer attributes were especially important . In the logit model, all but one variable about a customer’s utilized products were highly significant (<0.001) In contrast, for propositionalized settings T2.1-to-T2.4 we found only five significant customer attributes in the respective logit models .\n",
      "<Page:18>  Propositionalization settings with churn aggregates (T2.1 to T2.4) dominate all other settings,  apologetic and MLN settings . Sensitivity measures the proportion of true positives that are cor-rectly recognized as true positives . Specificity measures proportion of false positives correctly recognized as false positives . Table 1 presents the results for churn prediction .\n",
      "<Page:19>  The best propositionalization approach outperforms the best MLN model . Figure 2 shows the ROC curve of the three MLN settings and the logistic regression . The ROC is based on the proportion of true positives (TPR) vs. false positives (FPR) for every possible cutoff .\n",
      "<Page:20>  Propositionalization with churn rate aggregates (T2.1 to T2.4) has the highest overall sensitivity and accuracy, but MLNs yield comparable results for smaller samples . Both propositionalization and MLN clearly outperform the baseline model .\n",
      "<Page:21>  We were interested in ways how information about neighbors can help predict alternative target variables . We looked at game download, since people with game downloads appear to be well connected . Game download showed exceptionally many connections among the positives, suggesting that  there might also be an influence of customers on each other .\n",
      "<Page:22>  The overall accuracy and precision were again highest for the propositionalization settings T2.1 to T.2.4 and the three MLN settings T3.1 . All nine settings performed  better than the benchmark logistic regression, except for one attribute about the duration of the present contract .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 142, but you input_length is only 140. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Page:23>  Both propositionalization and MLN outperform the baseline model . The number of calls and voice minutes weighted did not have a strong effect . The impact of relational attributes was highly significant for predicting game download as well, but had a lower effect on sensitivity and the ROC curve compared to churn prediction .\n",
      "<Page:24>  Figure 5: Visualization of connections a) between negatives (without churn), b) from positives with game (squares) to negatives (triangles) and c (without game download) Figure 5 shows the connections between negatives and negatives (with churn) and negatives without churn . Figure 4: Churn: positives to negatives, negatives to negatives; game download: negatives only .\n",
      "<Page:25>  Churn positives are less common than customers with a game download in the sample . Overall, positives are much less connected in the churn sample than in the game download sample . If there was churn in the neighborhood of a cus-culartomer, this event was a powerful predictor for churn of this customer, as compared to game download .\n",
      "<Page:26>  Traditional discrete choice models do not allow the influence of peers through a social network to be modeled . We developed an MLN for churn prediction based on the anonymized data set of a mobile phone provider . We found this approach to provide even better results than MLNs in terms of increasing sensitivity of the benchmark logit model .\n",
      "<Page:27>  The book was written by W.-H. Au, K.C. Chan, X. Yao and S. Dzeroski . We would like to thank Florian Wangenheim, the anonymous associate editor and the reviewers for their                 valuable comments .\n",
      "<Page:28>  L. Getoor, B. Taskar, D. Mayzlin and D. Godes have written a number of articles on the topic of customer churning . Churn data mining techniques have been used to model churning patterns in the past . The study has been published in the European Symposium on Artificial Neural Networks in Bruges .\n",
      "<Page:29>  Decision Support Systems, 49(4) (2010) 474-485. [24] O. Hinz, M. Spann, Managing information diffusion in Name-Your-Own-Price auctions . [25] D. Jensen, J.-U. Kietz, J. Neville and M. Hay, Avoiding Bias when Aggregating Relational Data with Degree Disparity, in: Twentieth International Conference on Machine Learning (ICML-2003), (Washington, DC, USA, 2003).\n",
      "<Page:30>  Customers Churn: Stop it Before it Starts, Mercer Management Journal, 17(2004). [36] M. Kon, M.-A. Krogel, S. Rawles, F. Zelezny, P. Flach, N. Lavrac and S. Dzeroski .\n",
      "<Page:31>  P.A. Schweidel, P.S. Fader, E.T. Bradlow and J.R. Quinlan have written a number of articles on machine learning and customer retention models . P.J. Shaw, C. Subramaniam, G.W. Tan, M.E. Welge and M.C. Mozer have written numerous articles on the topic of machine learning .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 142, but you input_length is only 120. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Page:32>  P. Getoor, B. Taskar Eds. Introduction to statistical relational learning, (MIT Press, Cambridge, Mass., ,) 2007), pp. 93-129 . P. Sutton, A. McCallum: An Introduction to Conditional Random Fields for Relational Learning .\n",
      "<Page:33>  Figure 7: ROC curves of three MNL settings and the logistic regression for game download . Figure 8: Roc curves of propositionalization settings . Figure 9:  ROC curve of MLNs, propositionalized settings and logistic regressions . Figure 10:    The logistic reparation for game download is based on a logistic model .\n"
     ]
    }
   ],
   "source": [
    "all_text = ''\n",
    "for page in pdf.pages:\n",
    "    page_text = page.extract_text()\n",
    "    try:\n",
    "        resumo=summarization(page_text)[0]['summary_text']\n",
    "        print(page,resumo)\n",
    "    except:\n",
    "        print(page,'erro')\n",
    "    #print(page_text)\n",
    "    all_text = all_text + '\\n' + resumo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
